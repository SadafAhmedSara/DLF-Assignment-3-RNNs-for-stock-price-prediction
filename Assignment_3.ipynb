{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc058e7c",
      "metadata": {
        "scrolled": true,
        "id": "cc058e7c"
      },
      "outputs": [],
      "source": [
        "# Importing the necaessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, GRU, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9576e684",
      "metadata": {
        "id": "9576e684"
      },
      "source": [
        "This cell imports the necessary libraries for data processing (pandas, numpy), scaling (MinMaxScaler), splitting the data (train_test_split), and building the neural network model (tensorflow, Sequential, SimpleRNN, Dropout)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da16c7bd",
      "metadata": {
        "id": "da16c7bd"
      },
      "outputs": [],
      "source": [
        "# Loading the dataset\n",
        "data = pd.read_csv(\"DIS.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a08d2442",
      "metadata": {
        "id": "a08d2442"
      },
      "source": [
        "## Data Inspection and Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72380b1b",
      "metadata": {
        "id": "72380b1b",
        "outputId": "1afef0da-c2d3-4d2d-8421-0e17e5c760e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values in each column:\n",
            " Date              0\n",
            "Low               0\n",
            "Open              0\n",
            "Volume            0\n",
            "High              0\n",
            "Close             0\n",
            "Adjusted Close    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Checking for missing values\n",
        "missing_values = data.isnull().sum()\n",
        "print(\"Missing values in each column:\\n\", missing_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9822c5ff",
      "metadata": {
        "id": "9822c5ff"
      },
      "source": [
        "No missing values in the dataset so we do not need to handle missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f5b19b4",
      "metadata": {
        "id": "2f5b19b4",
        "outputId": "1bf1f2f0-5bd9-482c-dd19-93cb51a730cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 13356 entries, 0 to 13355\n",
            "Data columns (total 7 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   Date            13356 non-null  object \n",
            " 1   Low             13356 non-null  float64\n",
            " 2   Open            13356 non-null  float64\n",
            " 3   Volume          13356 non-null  int64  \n",
            " 4   High            13356 non-null  float64\n",
            " 5   Close           13356 non-null  float64\n",
            " 6   Adjusted Close  13356 non-null  float64\n",
            "dtypes: float64(5), int64(1), object(1)\n",
            "memory usage: 730.5+ KB\n"
          ]
        }
      ],
      "source": [
        "# Inspecting the dataset\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62f62917",
      "metadata": {
        "id": "62f62917",
        "outputId": "f8c7451b-49a8-4e0a-b968-009daed11fb0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Low</th>\n",
              "      <th>Open</th>\n",
              "      <th>Volume</th>\n",
              "      <th>High</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adjusted Close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>02-01-1970</td>\n",
              "      <td>0.683357</td>\n",
              "      <td>0.688495</td>\n",
              "      <td>1109377</td>\n",
              "      <td>0.689779</td>\n",
              "      <td>0.683357</td>\n",
              "      <td>0.454785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>05-01-1970</td>\n",
              "      <td>0.662805</td>\n",
              "      <td>0.683357</td>\n",
              "      <td>1440243</td>\n",
              "      <td>0.688495</td>\n",
              "      <td>0.662805</td>\n",
              "      <td>0.441107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>06-01-1970</td>\n",
              "      <td>0.646106</td>\n",
              "      <td>0.655098</td>\n",
              "      <td>3503294</td>\n",
              "      <td>0.655098</td>\n",
              "      <td>0.649960</td>\n",
              "      <td>0.432559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>07-01-1970</td>\n",
              "      <td>0.652529</td>\n",
              "      <td>0.652529</td>\n",
              "      <td>5741510</td>\n",
              "      <td>0.670512</td>\n",
              "      <td>0.666658</td>\n",
              "      <td>0.443671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>08-01-1970</td>\n",
              "      <td>0.673081</td>\n",
              "      <td>0.673081</td>\n",
              "      <td>2316067</td>\n",
              "      <td>0.692348</td>\n",
              "      <td>0.683357</td>\n",
              "      <td>0.454785</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date       Low      Open   Volume      High     Close  Adjusted Close\n",
              "0  02-01-1970  0.683357  0.688495  1109377  0.689779  0.683357        0.454785\n",
              "1  05-01-1970  0.662805  0.683357  1440243  0.688495  0.662805        0.441107\n",
              "2  06-01-1970  0.646106  0.655098  3503294  0.655098  0.649960        0.432559\n",
              "3  07-01-1970  0.652529  0.652529  5741510  0.670512  0.666658        0.443671\n",
              "4  08-01-1970  0.673081  0.673081  2316067  0.692348  0.683357        0.454785"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7d3f5bb",
      "metadata": {
        "id": "e7d3f5bb",
        "outputId": "8cb02943-e04f-48fe-deb5-ae5498619f3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13356, 7)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39dfbbc7",
      "metadata": {
        "id": "39dfbbc7"
      },
      "source": [
        "The dataset has 7 columns: Date, Low, Open, Volume, High, Close, and Adjusted Close and 13356 rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44099cf4",
      "metadata": {
        "id": "44099cf4"
      },
      "outputs": [],
      "source": [
        "# Converting the 'Date' to datetime and sorting it\n",
        "data['Date'] = pd.to_datetime(data['Date'], dayfirst=True, errors='coerce')\n",
        "data = data.sort_values(by='Date')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "839f2398",
      "metadata": {
        "id": "839f2398"
      },
      "source": [
        "The Date column is converted to a datetime object for better handling and the data is sorted chronologically by date to maintain the time-series structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67eb3712",
      "metadata": {
        "scrolled": false,
        "id": "67eb3712",
        "outputId": "fa5cce9d-ab50-4e35-e342-5077d5d0f12c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualizing the 'Close' prices to identify trends and anomalies\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(data['Date'], data['Close'], label='Close Price', color='blue')\n",
        "plt.title('Close Price Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5101b60d",
      "metadata": {
        "id": "5101b60d"
      },
      "source": [
        "This line plot shows the 'Close' prices of the stock over time, highlighting a significant long-term upward trend, particularly from the 1990s to 2020, followed by a sharp decline post-2020. Noticeable spikes and drops indicate high volatility during certain periods, especially after 2010. These insights can guide preprocessing by informing sequence length selection to capture historical trends.\n",
        "\n",
        "In real-life stock market scenarios, a 30-day sequence length is appropriate because it captures one month's worth of trading data, which is a common financial reporting period and trading cycle. Analyzing the graph, we see significant short-term fluctuations in the 'Close' price, particularly during periods of high market volatility like 2008 and post-2020. These fluctuations often represent trends or anomalies that span a few weeks to a month. By using a 30-day sequence, the model can leverage these recent trends to make more informed predictions about the next day's price while staying relevant to short-term market dynamics. Additionally, this approach aligns with real-world decision-making cycles, where investors often assess market performance over monthly periods for buy-sell decisions, making the model's predictions actionable and practical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b36b170",
      "metadata": {
        "id": "7b36b170",
        "outputId": "05de0f06-caa3-4634-afda-64a0fe266454"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Boxplot for 'Close' prices to detect outliers\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x=data['Close'], color='skyblue')\n",
        "plt.title('Boxplot of Close Prices')\n",
        "plt.xlabel('Close Price')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "779ec1b8",
      "metadata": {
        "id": "779ec1b8"
      },
      "source": [
        "The boxplot of Close prices reveals a significant number of outliers above the upper whisker, representing extreme stock price values. These outliers likely correspond to periods of heightened market activity, reflecting rapid growth, peaks, or corrections, particularly visible in recent years as seen in the trend analysis. For the stock price prediction task, these outliers indicate the need for careful handling during modeling, such as robust scaling or applying techniques like clipping or capping to prevent the model from being overly influenced by these extreme values. However, since outliers are part of real-world market conditions, they must be carefully considered rather than entirely removed, as they may contain valuable predictive information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38c9e761",
      "metadata": {
        "id": "38c9e761",
        "outputId": "7d4927f0-e011-464d-b535-87c5506f9a67"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualizing the 'Volume' to identify unusual patterns\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(data['Date'], data['Volume'], label='Volume', color='green')\n",
        "plt.title('Volume Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Volume')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a41c00db",
      "metadata": {
        "id": "a41c00db"
      },
      "source": [
        "The graph displays the trading volume of the stock over time, revealing key patterns and anomalies. Notable spikes in volume, particularly during the late 1980s and early 2000s, indicate periods of heightened market activity likely driven by major events, economic shifts, or company-specific news. The increasing trend in volume over the years, with periodic peaks, suggests evolving market dynamics and growing investor participation. For preprocessing, these volume spikes are important indicators of market volatility, which may correlate with significant price movements. This insight highlights the need to include volume as a feature in predictive models and to normalize or scale the data appropriately to ensure these spikes do not disproportionately influence the model while retaining their predictive significance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a39d9367",
      "metadata": {
        "id": "a39d9367",
        "outputId": "d383159f-69af-405a-e0fe-abf3e98976d5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Boxplot for 'Volume' to detect outliers\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x=data['Volume'], color='lightgreen')\n",
        "plt.title('Boxplot of Volume')\n",
        "plt.xlabel('Volume')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e353615",
      "metadata": {
        "id": "9e353615"
      },
      "source": [
        "The boxplot of Volume reveals a significant number of outliers far above the upper whisker, indicating periods of unusually high trading activity, often linked to market events or heightened investor interest. These outliers are important as they reflect potential price volatility and market anomalies. Min-Max scaling will normalize the data within a range, ensuring that these extreme values are accounted for without overwhelming the model during training. This scaling technique helps balance the influence of outliers while retaining their importance for predictive modeling, enabling the model to effectively learn from both typical and anomalous market conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2cb63bc",
      "metadata": {
        "id": "c2cb63bc"
      },
      "source": [
        "###  Handling Non-Consecutive Dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e87658e",
      "metadata": {
        "id": "0e87658e",
        "outputId": "abe2cc1e-8b2a-4ca4-b6ab-d167d1b12f03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing dates:\n",
            "DatetimeIndex(['1970-01-03', '1970-01-04', '1970-01-10', '1970-01-11',\n",
            "               '1970-01-17', '1970-01-18', '1970-01-24', '1970-01-25',\n",
            "               '1970-01-31', '1970-02-01',\n",
            "               ...\n",
            "               '2022-11-13', '2022-11-19', '2022-11-20', '2022-11-24',\n",
            "               '2022-11-26', '2022-11-27', '2022-12-03', '2022-12-04',\n",
            "               '2022-12-10', '2022-12-11'],\n",
            "              dtype='datetime64[ns]', length=5982, freq=None)\n",
            "Number of missing days: 5982\n"
          ]
        }
      ],
      "source": [
        "# Generating a full range of dates from the minimum to the maximum date\n",
        "full_date_range = pd.date_range(start=data['Date'].min(), end=data['Date'].max(), freq='D')\n",
        "\n",
        "# Identifying the missing dates\n",
        "missing_dates = full_date_range.difference(data['Date'])\n",
        "\n",
        "print(\"Missing dates:\")\n",
        "print(missing_dates)\n",
        "print(f\"Number of missing days: {len(missing_dates)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "903a7d57",
      "metadata": {
        "scrolled": true,
        "id": "903a7d57",
        "outputId": "715efdcf-be8c-4f51-cb77-57d525297930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weekend missing dates:\n",
            "[Timestamp('1970-01-03 00:00:00'), Timestamp('1970-01-04 00:00:00'), Timestamp('1970-01-10 00:00:00'), Timestamp('1970-01-11 00:00:00'), Timestamp('1970-01-17 00:00:00'), Timestamp('1970-01-18 00:00:00'), Timestamp('1970-01-24 00:00:00'), Timestamp('1970-01-25 00:00:00'), Timestamp('1970-01-31 00:00:00'), Timestamp('1970-02-01 00:00:00'), Timestamp('1970-02-07 00:00:00'), Timestamp('1970-02-08 00:00:00'), Timestamp('1970-02-14 00:00:00'), Timestamp('1970-02-15 00:00:00'), Timestamp('1970-02-21 00:00:00'), Timestamp('1970-02-22 00:00:00'), Timestamp('1970-02-28 00:00:00'), Timestamp('1970-03-01 00:00:00'), Timestamp('1970-03-07 00:00:00'), Timestamp('1970-03-08 00:00:00'), Timestamp('1970-03-14 00:00:00'), Timestamp('1970-03-15 00:00:00'), Timestamp('1970-03-21 00:00:00'), Timestamp('1970-03-22 00:00:00'), Timestamp('1970-03-28 00:00:00'), Timestamp('1970-03-29 00:00:00'), Timestamp('1970-04-04 00:00:00'), Timestamp('1970-04-05 00:00:00'), Timestamp('1970-04-11 00:00:00'), Timestamp('1970-04-12 00:00:00'), Timestamp('1970-04-18 00:00:00'), Timestamp('1970-04-19 00:00:00'), Timestamp('1970-04-25 00:00:00'), Timestamp('1970-04-26 00:00:00'), Timestamp('1970-05-02 00:00:00'), Timestamp('1970-05-03 00:00:00'), Timestamp('1970-05-09 00:00:00'), Timestamp('1970-05-10 00:00:00'), Timestamp('1970-05-16 00:00:00'), Timestamp('1970-05-17 00:00:00'), Timestamp('1970-05-23 00:00:00'), Timestamp('1970-05-24 00:00:00'), Timestamp('1970-05-30 00:00:00'), Timestamp('1970-05-31 00:00:00'), Timestamp('1970-06-06 00:00:00'), Timestamp('1970-06-07 00:00:00'), Timestamp('1970-06-13 00:00:00'), Timestamp('1970-06-14 00:00:00'), Timestamp('1970-06-20 00:00:00'), Timestamp('1970-06-21 00:00:00'), Timestamp('1970-06-27 00:00:00'), Timestamp('1970-06-28 00:00:00'), Timestamp('1970-07-04 00:00:00'), Timestamp('1970-07-05 00:00:00'), Timestamp('1970-07-11 00:00:00'), Timestamp('1970-07-12 00:00:00'), Timestamp('1970-07-18 00:00:00'), Timestamp('1970-07-19 00:00:00'), Timestamp('1970-07-25 00:00:00'), Timestamp('1970-07-26 00:00:00'), Timestamp('1970-08-01 00:00:00'), Timestamp('1970-08-02 00:00:00'), Timestamp('1970-08-08 00:00:00'), Timestamp('1970-08-09 00:00:00'), Timestamp('1970-08-15 00:00:00'), Timestamp('1970-08-16 00:00:00'), Timestamp('1970-08-22 00:00:00'), Timestamp('1970-08-23 00:00:00'), Timestamp('1970-08-29 00:00:00'), Timestamp('1970-08-30 00:00:00'), Timestamp('1970-09-05 00:00:00'), Timestamp('1970-09-06 00:00:00'), Timestamp('1970-09-12 00:00:00'), Timestamp('1970-09-13 00:00:00'), Timestamp('1970-09-19 00:00:00'), Timestamp('1970-09-20 00:00:00'), Timestamp('1970-09-26 00:00:00'), Timestamp('1970-09-27 00:00:00'), Timestamp('1970-10-03 00:00:00'), Timestamp('1970-10-04 00:00:00'), Timestamp('1970-10-10 00:00:00'), Timestamp('1970-10-11 00:00:00'), Timestamp('1970-10-17 00:00:00'), Timestamp('1970-10-18 00:00:00'), Timestamp('1970-10-24 00:00:00'), Timestamp('1970-10-25 00:00:00'), Timestamp('1970-10-31 00:00:00'), Timestamp('1970-11-01 00:00:00'), Timestamp('1970-11-07 00:00:00'), Timestamp('1970-11-08 00:00:00'), Timestamp('1970-11-14 00:00:00'), Timestamp('1970-11-15 00:00:00'), Timestamp('1970-11-21 00:00:00'), Timestamp('1970-11-22 00:00:00'), Timestamp('1970-11-28 00:00:00'), Timestamp('1970-11-29 00:00:00'), Timestamp('1970-12-05 00:00:00'), Timestamp('1970-12-06 00:00:00'), Timestamp('1970-12-12 00:00:00'), Timestamp('1970-12-13 00:00:00'), Timestamp('1970-12-19 00:00:00'), Timestamp('1970-12-20 00:00:00'), Timestamp('1970-12-26 00:00:00'), Timestamp('1970-12-27 00:00:00'), Timestamp('1971-01-02 00:00:00'), Timestamp('1971-01-03 00:00:00'), Timestamp('1971-01-09 00:00:00'), Timestamp('1971-01-10 00:00:00'), Timestamp('1971-01-16 00:00:00'), Timestamp('1971-01-17 00:00:00'), Timestamp('1971-01-23 00:00:00'), Timestamp('1971-01-24 00:00:00'), Timestamp('1971-01-30 00:00:00'), Timestamp('1971-01-31 00:00:00'), Timestamp('1971-02-06 00:00:00'), Timestamp('1971-02-07 00:00:00'), Timestamp('1971-02-13 00:00:00'), Timestamp('1971-02-14 00:00:00'), Timestamp('1971-02-20 00:00:00'), Timestamp('1971-02-21 00:00:00'), Timestamp('1971-02-27 00:00:00'), Timestamp('1971-02-28 00:00:00'), Timestamp('1971-03-06 00:00:00'), Timestamp('1971-03-07 00:00:00'), Timestamp('1971-03-13 00:00:00'), Timestamp('1971-03-14 00:00:00'), Timestamp('1971-03-20 00:00:00'), Timestamp('1971-03-21 00:00:00'), Timestamp('1971-03-27 00:00:00'), Timestamp('1971-03-28 00:00:00'), Timestamp('1971-04-03 00:00:00'), Timestamp('1971-04-04 00:00:00'), Timestamp('1971-04-10 00:00:00'), Timestamp('1971-04-11 00:00:00'), Timestamp('1971-04-17 00:00:00'), Timestamp('1971-04-18 00:00:00'), Timestamp('1971-04-24 00:00:00'), Timestamp('1971-04-25 00:00:00'), Timestamp('1971-05-01 00:00:00'), Timestamp('1971-05-02 00:00:00'), Timestamp('1971-05-08 00:00:00'), Timestamp('1971-05-09 00:00:00'), Timestamp('1971-05-15 00:00:00'), Timestamp('1971-05-16 00:00:00'), Timestamp('1971-05-22 00:00:00'), Timestamp('1971-05-23 00:00:00'), Timestamp('1971-05-29 00:00:00'), Timestamp('1971-05-30 00:00:00'), Timestamp('1971-06-05 00:00:00'), Timestamp('1971-06-06 00:00:00'), Timestamp('1971-06-12 00:00:00'), Timestamp('1971-06-13 00:00:00'), Timestamp('1971-06-19 00:00:00'), Timestamp('1971-06-20 00:00:00'), Timestamp('1971-06-26 00:00:00'), Timestamp('1971-06-27 00:00:00'), Timestamp('1971-07-03 00:00:00'), Timestamp('1971-07-04 00:00:00'), Timestamp('1971-07-10 00:00:00'), Timestamp('1971-07-11 00:00:00'), Timestamp('1971-07-17 00:00:00'), Timestamp('1971-07-18 00:00:00'), Timestamp('1971-07-24 00:00:00'), Timestamp('1971-07-25 00:00:00'), Timestamp('1971-07-31 00:00:00'), Timestamp('1971-08-01 00:00:00'), Timestamp('1971-08-07 00:00:00'), Timestamp('1971-08-08 00:00:00'), Timestamp('1971-08-14 00:00:00'), Timestamp('1971-08-15 00:00:00'), Timestamp('1971-08-21 00:00:00'), Timestamp('1971-08-22 00:00:00'), Timestamp('1971-08-28 00:00:00'), Timestamp('1971-08-29 00:00:00'), Timestamp('1971-09-04 00:00:00'), Timestamp('1971-09-05 00:00:00'), Timestamp('1971-09-11 00:00:00'), Timestamp('1971-09-12 00:00:00'), Timestamp('1971-09-18 00:00:00'), Timestamp('1971-09-19 00:00:00'), Timestamp('1971-09-25 00:00:00'), Timestamp('1971-09-26 00:00:00'), Timestamp('1971-10-02 00:00:00'), Timestamp('1971-10-03 00:00:00'), Timestamp('1971-10-09 00:00:00'), Timestamp('1971-10-10 00:00:00'), Timestamp('1971-10-16 00:00:00'), Timestamp('1971-10-17 00:00:00'), Timestamp('1971-10-23 00:00:00'), Timestamp('1971-10-24 00:00:00'), Timestamp('1971-10-30 00:00:00'), Timestamp('1971-10-31 00:00:00'), Timestamp('1971-11-06 00:00:00'), Timestamp('1971-11-07 00:00:00'), Timestamp('1971-11-13 00:00:00'), Timestamp('1971-11-14 00:00:00'), Timestamp('1971-11-20 00:00:00'), Timestamp('1971-11-21 00:00:00'), Timestamp('1971-11-27 00:00:00'), Timestamp('1971-11-28 00:00:00'), Timestamp('1971-12-04 00:00:00'), Timestamp('1971-12-05 00:00:00'), Timestamp('1971-12-11 00:00:00'), Timestamp('1971-12-12 00:00:00'), Timestamp('1971-12-18 00:00:00'), Timestamp('1971-12-19 00:00:00'), Timestamp('1971-12-25 00:00:00'), Timestamp('1971-12-26 00:00:00'), Timestamp('1972-01-01 00:00:00'), Timestamp('1972-01-02 00:00:00'), Timestamp('1972-01-08 00:00:00'), Timestamp('1972-01-09 00:00:00'), Timestamp('1972-01-15 00:00:00'), Timestamp('1972-01-16 00:00:00'), Timestamp('1972-01-22 00:00:00'), Timestamp('1972-01-23 00:00:00'), Timestamp('1972-01-29 00:00:00'), Timestamp('1972-01-30 00:00:00'), Timestamp('1972-02-05 00:00:00'), Timestamp('1972-02-06 00:00:00'), Timestamp('1972-02-12 00:00:00'), Timestamp('1972-02-13 00:00:00'), Timestamp('1972-02-19 00:00:00'), Timestamp('1972-02-20 00:00:00'), Timestamp('1972-02-26 00:00:00'), Timestamp('1972-02-27 00:00:00'), Timestamp('1972-03-04 00:00:00'), Timestamp('1972-03-05 00:00:00'), Timestamp('1972-03-11 00:00:00'), Timestamp('1972-03-12 00:00:00'), Timestamp('1972-03-18 00:00:00'), Timestamp('1972-03-19 00:00:00'), Timestamp('1972-03-25 00:00:00'), Timestamp('1972-03-26 00:00:00'), Timestamp('1972-04-01 00:00:00'), Timestamp('1972-04-02 00:00:00'), Timestamp('1972-04-08 00:00:00'), Timestamp('1972-04-09 00:00:00'), Timestamp('1972-04-15 00:00:00'), Timestamp('1972-04-16 00:00:00'), Timestamp('1972-04-22 00:00:00'), Timestamp('1972-04-23 00:00:00'), Timestamp('1972-04-29 00:00:00'), Timestamp('1972-04-30 00:00:00'), Timestamp('1972-05-06 00:00:00'), Timestamp('1972-05-07 00:00:00'), Timestamp('1972-05-13 00:00:00'), Timestamp('1972-05-14 00:00:00'), Timestamp('1972-05-20 00:00:00'), Timestamp('1972-05-21 00:00:00'), Timestamp('1972-05-27 00:00:00'), Timestamp('1972-05-28 00:00:00'), Timestamp('1972-06-03 00:00:00'), Timestamp('1972-06-04 00:00:00'), Timestamp('1972-06-10 00:00:00'), Timestamp('1972-06-11 00:00:00'), Timestamp('1972-06-17 00:00:00'), Timestamp('1972-06-18 00:00:00'), Timestamp('1972-06-24 00:00:00'), Timestamp('1972-06-25 00:00:00'), Timestamp('1972-07-01 00:00:00'), Timestamp('1972-07-02 00:00:00'), Timestamp('1972-07-08 00:00:00'), Timestamp('1972-07-09 00:00:00'), Timestamp('1972-07-15 00:00:00'), Timestamp('1972-07-16 00:00:00'), Timestamp('1972-07-22 00:00:00'), Timestamp('1972-07-23 00:00:00'), Timestamp('1972-07-29 00:00:00'), Timestamp('1972-07-30 00:00:00'), Timestamp('1972-08-05 00:00:00'), Timestamp('1972-08-06 00:00:00'), Timestamp('1972-08-12 00:00:00'), Timestamp('1972-08-13 00:00:00'), Timestamp('1972-08-19 00:00:00'), Timestamp('1972-08-20 00:00:00'), Timestamp('1972-08-26 00:00:00'), Timestamp('1972-08-27 00:00:00'), Timestamp('1972-09-02 00:00:00'), Timestamp('1972-09-03 00:00:00'), Timestamp('1972-09-09 00:00:00'), Timestamp('1972-09-10 00:00:00'), Timestamp('1972-09-16 00:00:00'), Timestamp('1972-09-17 00:00:00'), Timestamp('1972-09-23 00:00:00'), Timestamp('1972-09-24 00:00:00'), Timestamp('1972-09-30 00:00:00'), Timestamp('1972-10-01 00:00:00'), Timestamp('1972-10-07 00:00:00'), Timestamp('1972-10-08 00:00:00'), Timestamp('1972-10-14 00:00:00'), Timestamp('1972-10-15 00:00:00'), Timestamp('1972-10-21 00:00:00'), Timestamp('1972-10-22 00:00:00'), Timestamp('1972-10-28 00:00:00'), Timestamp('1972-10-29 00:00:00'), Timestamp('1972-11-04 00:00:00'), Timestamp('1972-11-05 00:00:00'), Timestamp('1972-11-11 00:00:00'), Timestamp('1972-11-12 00:00:00'), Timestamp('1972-11-18 00:00:00'), Timestamp('1972-11-19 00:00:00'), Timestamp('1972-11-25 00:00:00'), Timestamp('1972-11-26 00:00:00'), Timestamp('1972-12-02 00:00:00'), Timestamp('1972-12-03 00:00:00'), Timestamp('1972-12-09 00:00:00'), Timestamp('1972-12-10 00:00:00'), Timestamp('1972-12-16 00:00:00'), Timestamp('1972-12-17 00:00:00'), Timestamp('1972-12-23 00:00:00'), Timestamp('1972-12-24 00:00:00'), Timestamp('1972-12-30 00:00:00'), Timestamp('1972-12-31 00:00:00'), Timestamp('1973-01-06 00:00:00'), Timestamp('1973-01-07 00:00:00'), Timestamp('1973-01-13 00:00:00'), Timestamp('1973-01-14 00:00:00'), Timestamp('1973-01-20 00:00:00'), Timestamp('1973-01-21 00:00:00'), Timestamp('1973-01-27 00:00:00'), Timestamp('1973-01-28 00:00:00'), Timestamp('1973-02-03 00:00:00'), Timestamp('1973-02-04 00:00:00'), Timestamp('1973-02-10 00:00:00'), Timestamp('1973-02-11 00:00:00'), Timestamp('1973-02-17 00:00:00'), Timestamp('1973-02-18 00:00:00'), Timestamp('1973-02-24 00:00:00'), Timestamp('1973-02-25 00:00:00'), Timestamp('1973-03-03 00:00:00'), Timestamp('1973-03-04 00:00:00'), Timestamp('1973-03-10 00:00:00'), Timestamp('1973-03-11 00:00:00'), Timestamp('1973-03-17 00:00:00'), Timestamp('1973-03-18 00:00:00'), Timestamp('1973-03-24 00:00:00'), Timestamp('1973-03-25 00:00:00'), Timestamp('1973-03-31 00:00:00'), Timestamp('1973-04-01 00:00:00'), Timestamp('1973-04-07 00:00:00'), Timestamp('1973-04-08 00:00:00'), Timestamp('1973-04-14 00:00:00'), Timestamp('1973-04-15 00:00:00'), Timestamp('1973-04-21 00:00:00'), Timestamp('1973-04-22 00:00:00'), Timestamp('1973-04-28 00:00:00'), Timestamp('1973-04-29 00:00:00'), Timestamp('1973-05-05 00:00:00'), Timestamp('1973-05-06 00:00:00'), Timestamp('1973-05-12 00:00:00'), Timestamp('1973-05-13 00:00:00'), Timestamp('1973-05-19 00:00:00'), Timestamp('1973-05-20 00:00:00'), Timestamp('1973-05-26 00:00:00'), Timestamp('1973-05-27 00:00:00'), Timestamp('1973-06-02 00:00:00'), Timestamp('1973-06-03 00:00:00'), Timestamp('1973-06-09 00:00:00'), Timestamp('1973-06-10 00:00:00'), Timestamp('1973-06-16 00:00:00'), Timestamp('1973-06-17 00:00:00'), Timestamp('1973-06-23 00:00:00'), Timestamp('1973-06-24 00:00:00'), Timestamp('1973-06-30 00:00:00'), Timestamp('1973-07-01 00:00:00'), Timestamp('1973-07-07 00:00:00'), Timestamp('1973-07-08 00:00:00'), Timestamp('1973-07-14 00:00:00'), Timestamp('1973-07-15 00:00:00'), Timestamp('1973-07-21 00:00:00'), Timestamp('1973-07-22 00:00:00'), Timestamp('1973-07-28 00:00:00'), Timestamp('1973-07-29 00:00:00'), Timestamp('1973-08-04 00:00:00'), Timestamp('1973-08-05 00:00:00'), Timestamp('1973-08-11 00:00:00'), Timestamp('1973-08-12 00:00:00'), Timestamp('1973-08-18 00:00:00'), Timestamp('1973-08-19 00:00:00'), Timestamp('1973-08-25 00:00:00'), Timestamp('1973-08-26 00:00:00'), Timestamp('1973-09-01 00:00:00'), Timestamp('1973-09-02 00:00:00'), Timestamp('1973-09-08 00:00:00'), Timestamp('1973-09-09 00:00:00'), Timestamp('1973-09-15 00:00:00'), Timestamp('1973-09-16 00:00:00'), Timestamp('1973-09-22 00:00:00'), Timestamp('1973-09-23 00:00:00'), Timestamp('1973-09-29 00:00:00'), Timestamp('1973-09-30 00:00:00'), Timestamp('1973-10-06 00:00:00'), Timestamp('1973-10-07 00:00:00'), Timestamp('1973-10-13 00:00:00'), Timestamp('1973-10-14 00:00:00'), Timestamp('1973-10-20 00:00:00'), Timestamp('1973-10-21 00:00:00'), Timestamp('1973-10-27 00:00:00'), Timestamp('1973-10-28 00:00:00'), Timestamp('1973-11-03 00:00:00'), Timestamp('1973-11-04 00:00:00'), Timestamp('1973-11-10 00:00:00'), Timestamp('1973-11-11 00:00:00'), Timestamp('1973-11-17 00:00:00'), Timestamp('1973-11-18 00:00:00'), Timestamp('1973-11-24 00:00:00'), Timestamp('1973-11-25 00:00:00'), Timestamp('1973-12-01 00:00:00'), Timestamp('1973-12-02 00:00:00'), Timestamp('1973-12-08 00:00:00'), Timestamp('1973-12-09 00:00:00'), Timestamp('1973-12-15 00:00:00'), Timestamp('1973-12-16 00:00:00'), Timestamp('1973-12-22 00:00:00'), Timestamp('1973-12-23 00:00:00'), Timestamp('1973-12-29 00:00:00'), Timestamp('1973-12-30 00:00:00'), Timestamp('1974-01-05 00:00:00'), Timestamp('1974-01-06 00:00:00'), Timestamp('1974-01-12 00:00:00'), Timestamp('1974-01-13 00:00:00'), Timestamp('1974-01-19 00:00:00'), Timestamp('1974-01-20 00:00:00'), Timestamp('1974-01-26 00:00:00'), Timestamp('1974-01-27 00:00:00'), Timestamp('1974-02-02 00:00:00'), Timestamp('1974-02-03 00:00:00'), Timestamp('1974-02-09 00:00:00'), Timestamp('1974-02-10 00:00:00'), Timestamp('1974-02-16 00:00:00'), Timestamp('1974-02-17 00:00:00'), Timestamp('1974-02-23 00:00:00'), Timestamp('1974-02-24 00:00:00'), Timestamp('1974-03-02 00:00:00'), Timestamp('1974-03-03 00:00:00'), Timestamp('1974-03-09 00:00:00'), Timestamp('1974-03-10 00:00:00'), Timestamp('1974-03-16 00:00:00'), Timestamp('1974-03-17 00:00:00'), Timestamp('1974-03-23 00:00:00'), Timestamp('1974-03-24 00:00:00'), Timestamp('1974-03-30 00:00:00'), Timestamp('1974-03-31 00:00:00'), Timestamp('1974-04-06 00:00:00'), Timestamp('1974-04-07 00:00:00'), Timestamp('1974-04-13 00:00:00'), Timestamp('1974-04-14 00:00:00'), Timestamp('1974-04-20 00:00:00'), Timestamp('1974-04-21 00:00:00'), Timestamp('1974-04-27 00:00:00'), Timestamp('1974-04-28 00:00:00'), Timestamp('1974-05-04 00:00:00'), Timestamp('1974-05-05 00:00:00'), Timestamp('1974-05-11 00:00:00'), Timestamp('1974-05-12 00:00:00'), Timestamp('1974-05-18 00:00:00'), Timestamp('1974-05-19 00:00:00'), Timestamp('1974-05-25 00:00:00'), Timestamp('1974-05-26 00:00:00'), Timestamp('1974-06-01 00:00:00'), Timestamp('1974-06-02 00:00:00'), Timestamp('1974-06-08 00:00:00'), Timestamp('1974-06-09 00:00:00'), Timestamp('1974-06-15 00:00:00'), Timestamp('1974-06-16 00:00:00'), Timestamp('1974-06-22 00:00:00'), Timestamp('1974-06-23 00:00:00'), Timestamp('1974-06-29 00:00:00'), Timestamp('1974-06-30 00:00:00'), Timestamp('1974-07-06 00:00:00'), Timestamp('1974-07-07 00:00:00'), Timestamp('1974-07-13 00:00:00'), Timestamp('1974-07-14 00:00:00'), Timestamp('1974-07-20 00:00:00'), Timestamp('1974-07-21 00:00:00'), Timestamp('1974-07-27 00:00:00'), Timestamp('1974-07-28 00:00:00'), Timestamp('1974-08-03 00:00:00'), Timestamp('1974-08-04 00:00:00'), Timestamp('1974-08-10 00:00:00'), Timestamp('1974-08-11 00:00:00'), Timestamp('1974-08-17 00:00:00'), Timestamp('1974-08-18 00:00:00'), Timestamp('1974-08-24 00:00:00'), Timestamp('1974-08-25 00:00:00'), Timestamp('1974-08-31 00:00:00'), Timestamp('1974-09-01 00:00:00'), Timestamp('1974-09-07 00:00:00'), Timestamp('1974-09-08 00:00:00'), Timestamp('1974-09-14 00:00:00'), Timestamp('1974-09-15 00:00:00'), Timestamp('1974-09-21 00:00:00'), Timestamp('1974-09-22 00:00:00'), Timestamp('1974-09-28 00:00:00'), Timestamp('1974-09-29 00:00:00'), Timestamp('1974-10-05 00:00:00'), Timestamp('1974-10-06 00:00:00'), Timestamp('1974-10-12 00:00:00'), Timestamp('1974-10-13 00:00:00'), Timestamp('1974-10-19 00:00:00'), Timestamp('1974-10-20 00:00:00'), Timestamp('1974-10-26 00:00:00'), Timestamp('1974-10-27 00:00:00'), Timestamp('1974-11-02 00:00:00'), Timestamp('1974-11-03 00:00:00'), Timestamp('1974-11-09 00:00:00'), Timestamp('1974-11-10 00:00:00'), Timestamp('1974-11-16 00:00:00'), Timestamp('1974-11-17 00:00:00'), Timestamp('1974-11-23 00:00:00'), Timestamp('1974-11-24 00:00:00'), Timestamp('1974-11-30 00:00:00'), Timestamp('1974-12-01 00:00:00'), Timestamp('1974-12-07 00:00:00'), Timestamp('1974-12-08 00:00:00'), Timestamp('1974-12-14 00:00:00'), Timestamp('1974-12-15 00:00:00'), Timestamp('1974-12-21 00:00:00'), Timestamp('1974-12-22 00:00:00'), Timestamp('1974-12-28 00:00:00'), Timestamp('1974-12-29 00:00:00'), Timestamp('1975-01-04 00:00:00'), Timestamp('1975-01-05 00:00:00'), Timestamp('1975-01-11 00:00:00'), Timestamp('1975-01-12 00:00:00'), Timestamp('1975-01-18 00:00:00'), Timestamp('1975-01-19 00:00:00'), Timestamp('1975-01-25 00:00:00'), Timestamp('1975-01-26 00:00:00'), Timestamp('1975-02-01 00:00:00'), Timestamp('1975-02-02 00:00:00'), Timestamp('1975-02-08 00:00:00'), Timestamp('1975-02-09 00:00:00'), Timestamp('1975-02-15 00:00:00'), Timestamp('1975-02-16 00:00:00'), Timestamp('1975-02-22 00:00:00'), Timestamp('1975-02-23 00:00:00'), Timestamp('1975-03-01 00:00:00'), Timestamp('1975-03-02 00:00:00'), Timestamp('1975-03-08 00:00:00'), Timestamp('1975-03-09 00:00:00'), Timestamp('1975-03-15 00:00:00'), Timestamp('1975-03-16 00:00:00'), Timestamp('1975-03-22 00:00:00'), Timestamp('1975-03-23 00:00:00'), Timestamp('1975-03-29 00:00:00'), Timestamp('1975-03-30 00:00:00'), Timestamp('1975-04-05 00:00:00'), Timestamp('1975-04-06 00:00:00'), Timestamp('1975-04-12 00:00:00'), Timestamp('1975-04-13 00:00:00'), Timestamp('1975-04-19 00:00:00'), Timestamp('1975-04-20 00:00:00'), Timestamp('1975-04-26 00:00:00'), Timestamp('1975-04-27 00:00:00'), Timestamp('1975-05-03 00:00:00'), Timestamp('1975-05-04 00:00:00'), Timestamp('1975-05-10 00:00:00'), Timestamp('1975-05-11 00:00:00'), Timestamp('1975-05-17 00:00:00'), Timestamp('1975-05-18 00:00:00'), Timestamp('1975-05-24 00:00:00'), Timestamp('1975-05-25 00:00:00'), Timestamp('1975-05-31 00:00:00'), Timestamp('1975-06-01 00:00:00'), Timestamp('1975-06-07 00:00:00'), Timestamp('1975-06-08 00:00:00'), Timestamp('1975-06-14 00:00:00'), Timestamp('1975-06-15 00:00:00'), Timestamp('1975-06-21 00:00:00'), Timestamp('1975-06-22 00:00:00'), Timestamp('1975-06-28 00:00:00'), Timestamp('1975-06-29 00:00:00'), Timestamp('1975-07-05 00:00:00'), Timestamp('1975-07-06 00:00:00'), Timestamp('1975-07-12 00:00:00'), Timestamp('1975-07-13 00:00:00'), Timestamp('1975-07-19 00:00:00'), Timestamp('1975-07-20 00:00:00'), Timestamp('1975-07-26 00:00:00'), Timestamp('1975-07-27 00:00:00'), Timestamp('1975-08-02 00:00:00'), Timestamp('1975-08-03 00:00:00'), Timestamp('1975-08-09 00:00:00'), Timestamp('1975-08-10 00:00:00'), Timestamp('1975-08-16 00:00:00'), Timestamp('1975-08-17 00:00:00'), Timestamp('1975-08-23 00:00:00'), Timestamp('1975-08-24 00:00:00'), Timestamp('1975-08-30 00:00:00'), Timestamp('1975-08-31 00:00:00'), Timestamp('1975-09-06 00:00:00'), Timestamp('1975-09-07 00:00:00'), Timestamp('1975-09-13 00:00:00'), Timestamp('1975-09-14 00:00:00'), Timestamp('1975-09-20 00:00:00'), Timestamp('1975-09-21 00:00:00'), Timestamp('1975-09-27 00:00:00'), Timestamp('1975-09-28 00:00:00'), Timestamp('1975-10-04 00:00:00'), Timestamp('1975-10-05 00:00:00'), Timestamp('1975-10-11 00:00:00'), Timestamp('1975-10-12 00:00:00'), Timestamp('1975-10-18 00:00:00'), Timestamp('1975-10-19 00:00:00'), Timestamp('1975-10-25 00:00:00'), Timestamp('1975-10-26 00:00:00'), Timestamp('1975-11-01 00:00:00'), Timestamp('1975-11-02 00:00:00'), Timestamp('1975-11-08 00:00:00'), Timestamp('1975-11-09 00:00:00'), Timestamp('1975-11-15 00:00:00'), Timestamp('1975-11-16 00:00:00'), Timestamp('1975-11-22 00:00:00'), Timestamp('1975-11-23 00:00:00'), Timestamp('1975-11-29 00:00:00'), Timestamp('1975-11-30 00:00:00'), Timestamp('1975-12-06 00:00:00'), Timestamp('1975-12-07 00:00:00'), Timestamp('1975-12-13 00:00:00'), Timestamp('1975-12-14 00:00:00'), Timestamp('1975-12-20 00:00:00'), Timestamp('1975-12-21 00:00:00'), Timestamp('1975-12-27 00:00:00'), Timestamp('1975-12-28 00:00:00'), Timestamp('1976-01-03 00:00:00'), Timestamp('1976-01-04 00:00:00'), Timestamp('1976-01-10 00:00:00'), Timestamp('1976-01-11 00:00:00'), Timestamp('1976-01-17 00:00:00'), Timestamp('1976-01-18 00:00:00'), Timestamp('1976-01-24 00:00:00'), Timestamp('1976-01-25 00:00:00'), Timestamp('1976-01-31 00:00:00'), Timestamp('1976-02-01 00:00:00'), Timestamp('1976-02-07 00:00:00'), Timestamp('1976-02-08 00:00:00'), Timestamp('1976-02-14 00:00:00'), Timestamp('1976-02-15 00:00:00'), Timestamp('1976-02-21 00:00:00'), Timestamp('1976-02-22 00:00:00'), Timestamp('1976-02-28 00:00:00'), Timestamp('1976-02-29 00:00:00'), Timestamp('1976-03-06 00:00:00'), Timestamp('1976-03-07 00:00:00'), Timestamp('1976-03-13 00:00:00'), Timestamp('1976-03-14 00:00:00'), Timestamp('1976-03-20 00:00:00'), Timestamp('1976-03-21 00:00:00'), Timestamp('1976-03-27 00:00:00'), Timestamp('1976-03-28 00:00:00'), Timestamp('1976-04-03 00:00:00'), Timestamp('1976-04-04 00:00:00'), Timestamp('1976-04-10 00:00:00'), Timestamp('1976-04-11 00:00:00'), Timestamp('1976-04-17 00:00:00'), Timestamp('1976-04-18 00:00:00'), Timestamp('1976-04-24 00:00:00'), Timestamp('1976-04-25 00:00:00'), Timestamp('1976-05-01 00:00:00'), Timestamp('1976-05-02 00:00:00'), Timestamp('1976-05-08 00:00:00'), Timestamp('1976-05-09 00:00:00'), Timestamp('1976-05-15 00:00:00'), Timestamp('1976-05-16 00:00:00'), Timestamp('1976-05-22 00:00:00'), Timestamp('1976-05-23 00:00:00'), Timestamp('1976-05-29 00:00:00'), Timestamp('1976-05-30 00:00:00'), Timestamp('1976-06-05 00:00:00'), Timestamp('1976-06-06 00:00:00'), Timestamp('1976-06-12 00:00:00'), Timestamp('1976-06-13 00:00:00'), Timestamp('1976-06-19 00:00:00'), Timestamp('1976-06-20 00:00:00'), Timestamp('1976-06-26 00:00:00'), Timestamp('1976-06-27 00:00:00'), Timestamp('1976-07-03 00:00:00'), Timestamp('1976-07-04 00:00:00'), Timestamp('1976-07-10 00:00:00'), Timestamp('1976-07-11 00:00:00'), Timestamp('1976-07-17 00:00:00'), Timestamp('1976-07-18 00:00:00'), Timestamp('1976-07-24 00:00:00'), Timestamp('1976-07-25 00:00:00'), Timestamp('1976-07-31 00:00:00'), Timestamp('1976-08-01 00:00:00'), Timestamp('1976-08-07 00:00:00'), Timestamp('1976-08-08 00:00:00'), Timestamp('1976-08-14 00:00:00'), Timestamp('1976-08-15 00:00:00'), Timestamp('1976-08-21 00:00:00'), Timestamp('1976-08-22 00:00:00'), Timestamp('1976-08-28 00:00:00'), Timestamp('1976-08-29 00:00:00'), Timestamp('1976-09-04 00:00:00'), Timestamp('1976-09-05 00:00:00'), Timestamp('1976-09-11 00:00:00'), Timestamp('1976-09-12 00:00:00'), Timestamp('1976-09-18 00:00:00'), Timestamp('1976-09-19 00:00:00'), Timestamp('1976-09-25 00:00:00'), Timestamp('1976-09-26 00:00:00'), Timestamp('1976-10-02 00:00:00'), Timestamp('1976-10-03 00:00:00'), Timestamp('1976-10-09 00:00:00'), Timestamp('1976-10-10 00:00:00'), Timestamp('1976-10-16 00:00:00'), Timestamp('1976-10-17 00:00:00'), Timestamp('1976-10-23 00:00:00'), Timestamp('1976-10-24 00:00:00'), Timestamp('1976-10-30 00:00:00'), Timestamp('1976-10-31 00:00:00'), Timestamp('1976-11-06 00:00:00'), Timestamp('1976-11-07 00:00:00'), Timestamp('1976-11-13 00:00:00'), Timestamp('1976-11-14 00:00:00'), Timestamp('1976-11-20 00:00:00'), Timestamp('1976-11-21 00:00:00'), Timestamp('1976-11-27 00:00:00'), Timestamp('1976-11-28 00:00:00'), Timestamp('1976-12-04 00:00:00'), Timestamp('1976-12-05 00:00:00'), Timestamp('1976-12-11 00:00:00'), Timestamp('1976-12-12 00:00:00'), Timestamp('1976-12-18 00:00:00'), Timestamp('1976-12-19 00:00:00'), Timestamp('1976-12-25 00:00:00'), Timestamp('1976-12-26 00:00:00'), Timestamp('1977-01-01 00:00:00'), Timestamp('1977-01-02 00:00:00'), Timestamp('1977-01-08 00:00:00'), Timestamp('1977-01-09 00:00:00'), Timestamp('1977-01-15 00:00:00'), Timestamp('1977-01-16 00:00:00'), Timestamp('1977-01-22 00:00:00'), Timestamp('1977-01-23 00:00:00'), Timestamp('1977-01-29 00:00:00'), Timestamp('1977-01-30 00:00:00'), Timestamp('1977-02-05 00:00:00'), Timestamp('1977-02-06 00:00:00'), Timestamp('1977-02-12 00:00:00'), Timestamp('1977-02-13 00:00:00'), Timestamp('1977-02-19 00:00:00'), Timestamp('1977-02-20 00:00:00'), Timestamp('1977-02-26 00:00:00'), Timestamp('1977-02-27 00:00:00'), Timestamp('1977-03-05 00:00:00'), Timestamp('1977-03-06 00:00:00'), Timestamp('1977-03-12 00:00:00'), Timestamp('1977-03-13 00:00:00'), Timestamp('1977-03-19 00:00:00'), Timestamp('1977-03-20 00:00:00'), Timestamp('1977-03-26 00:00:00'), Timestamp('1977-03-27 00:00:00'), Timestamp('1977-04-02 00:00:00'), Timestamp('1977-04-03 00:00:00'), Timestamp('1977-04-09 00:00:00'), Timestamp('1977-04-10 00:00:00'), Timestamp('1977-04-16 00:00:00'), Timestamp('1977-04-17 00:00:00'), Timestamp('1977-04-23 00:00:00'), Timestamp('1977-04-24 00:00:00'), Timestamp('1977-04-30 00:00:00'), Timestamp('1977-05-01 00:00:00'), Timestamp('1977-05-07 00:00:00'), Timestamp('1977-05-08 00:00:00'), Timestamp('1977-05-14 00:00:00'), Timestamp('1977-05-15 00:00:00'), Timestamp('1977-05-21 00:00:00'), Timestamp('1977-05-22 00:00:00'), Timestamp('1977-05-28 00:00:00'), Timestamp('1977-05-29 00:00:00'), Timestamp('1977-06-04 00:00:00'), Timestamp('1977-06-05 00:00:00'), Timestamp('1977-06-11 00:00:00'), Timestamp('1977-06-12 00:00:00'), Timestamp('1977-06-18 00:00:00'), Timestamp('1977-06-19 00:00:00'), Timestamp('1977-06-25 00:00:00'), Timestamp('1977-06-26 00:00:00'), Timestamp('1977-07-02 00:00:00'), Timestamp('1977-07-03 00:00:00'), Timestamp('1977-07-09 00:00:00'), Timestamp('1977-07-10 00:00:00'), Timestamp('1977-07-16 00:00:00'), Timestamp('1977-07-17 00:00:00'), Timestamp('1977-07-23 00:00:00'), Timestamp('1977-07-24 00:00:00'), Timestamp('1977-07-30 00:00:00'), Timestamp('1977-07-31 00:00:00'), Timestamp('1977-08-06 00:00:00'), Timestamp('1977-08-07 00:00:00'), Timestamp('1977-08-13 00:00:00'), Timestamp('1977-08-14 00:00:00'), Timestamp('1977-08-20 00:00:00'), Timestamp('1977-08-21 00:00:00'), Timestamp('1977-08-27 00:00:00'), Timestamp('1977-08-28 00:00:00'), Timestamp('1977-09-03 00:00:00'), Timestamp('1977-09-04 00:00:00'), Timestamp('1977-09-10 00:00:00'), Timestamp('1977-09-11 00:00:00'), Timestamp('1977-09-17 00:00:00'), Timestamp('1977-09-18 00:00:00'), Timestamp('1977-09-24 00:00:00'), Timestamp('1977-09-25 00:00:00'), Timestamp('1977-10-01 00:00:00'), Timestamp('1977-10-02 00:00:00'), Timestamp('1977-10-08 00:00:00'), Timestamp('1977-10-09 00:00:00'), Timestamp('1977-10-15 00:00:00'), Timestamp('1977-10-16 00:00:00'), Timestamp('1977-10-22 00:00:00'), Timestamp('1977-10-23 00:00:00'), Timestamp('1977-10-29 00:00:00'), Timestamp('1977-10-30 00:00:00'), Timestamp('1977-11-05 00:00:00'), Timestamp('1977-11-06 00:00:00'), Timestamp('1977-11-12 00:00:00'), Timestamp('1977-11-13 00:00:00'), Timestamp('1977-11-19 00:00:00'), Timestamp('1977-11-20 00:00:00'), Timestamp('1977-11-26 00:00:00'), Timestamp('1977-11-27 00:00:00'), Timestamp('1977-12-03 00:00:00'), Timestamp('1977-12-04 00:00:00'), Timestamp('1977-12-10 00:00:00'), Timestamp('1977-12-11 00:00:00'), Timestamp('1977-12-17 00:00:00'), Timestamp('1977-12-18 00:00:00'), Timestamp('1977-12-24 00:00:00'), Timestamp('1977-12-25 00:00:00'), Timestamp('1977-12-31 00:00:00'), Timestamp('1978-01-01 00:00:00'), Timestamp('1978-01-07 00:00:00'), Timestamp('1978-01-08 00:00:00'), Timestamp('1978-01-14 00:00:00'), Timestamp('1978-01-15 00:00:00'), Timestamp('1978-01-21 00:00:00'), Timestamp('1978-01-22 00:00:00'), Timestamp('1978-01-28 00:00:00'), Timestamp('1978-01-29 00:00:00'), Timestamp('1978-02-04 00:00:00'), Timestamp('1978-02-05 00:00:00'), Timestamp('1978-02-11 00:00:00'), Timestamp('1978-02-12 00:00:00'), Timestamp('1978-02-18 00:00:00'), Timestamp('1978-02-19 00:00:00'), Timestamp('1978-02-25 00:00:00'), Timestamp('1978-02-26 00:00:00'), Timestamp('1978-03-04 00:00:00'), Timestamp('1978-03-05 00:00:00'), Timestamp('1978-03-11 00:00:00'), Timestamp('1978-03-12 00:00:00'), Timestamp('1978-03-18 00:00:00'), Timestamp('1978-03-19 00:00:00'), Timestamp('1978-03-25 00:00:00'), Timestamp('1978-03-26 00:00:00'), Timestamp('1978-04-01 00:00:00'), Timestamp('1978-04-02 00:00:00'), Timestamp('1978-04-08 00:00:00'), Timestamp('1978-04-09 00:00:00'), Timestamp('1978-04-15 00:00:00'), Timestamp('1978-04-16 00:00:00'), Timestamp('1978-04-22 00:00:00'), Timestamp('1978-04-23 00:00:00'), Timestamp('1978-04-29 00:00:00'), Timestamp('1978-04-30 00:00:00'), Timestamp('1978-05-06 00:00:00'), Timestamp('1978-05-07 00:00:00'), Timestamp('1978-05-13 00:00:00'), Timestamp('1978-05-14 00:00:00'), Timestamp('1978-05-20 00:00:00'), Timestamp('1978-05-21 00:00:00'), Timestamp('1978-05-27 00:00:00'), Timestamp('1978-05-28 00:00:00'), Timestamp('1978-06-03 00:00:00'), Timestamp('1978-06-04 00:00:00'), Timestamp('1978-06-10 00:00:00'), Timestamp('1978-06-11 00:00:00'), Timestamp('1978-06-17 00:00:00'), Timestamp('1978-06-18 00:00:00'), Timestamp('1978-06-24 00:00:00'), Timestamp('1978-06-25 00:00:00'), Timestamp('1978-07-01 00:00:00'), Timestamp('1978-07-02 00:00:00'), Timestamp('1978-07-08 00:00:00'), Timestamp('1978-07-09 00:00:00'), Timestamp('1978-07-15 00:00:00'), Timestamp('1978-07-16 00:00:00'), Timestamp('1978-07-22 00:00:00'), Timestamp('1978-07-23 00:00:00'), Timestamp('1978-07-29 00:00:00'), Timestamp('1978-07-30 00:00:00'), Timestamp('1978-08-05 00:00:00'), Timestamp('1978-08-06 00:00:00'), Timestamp('1978-08-12 00:00:00'), Timestamp('1978-08-13 00:00:00'), Timestamp('1978-08-19 00:00:00'), Timestamp('1978-08-20 00:00:00'), Timestamp('1978-08-26 00:00:00'), Timestamp('1978-08-27 00:00:00'), Timestamp('1978-09-02 00:00:00'), Timestamp('1978-09-03 00:00:00'), Timestamp('1978-09-09 00:00:00'), Timestamp('1978-09-10 00:00:00'), Timestamp('1978-09-16 00:00:00'), Timestamp('1978-09-17 00:00:00'), Timestamp('1978-09-23 00:00:00'), Timestamp('1978-09-24 00:00:00'), Timestamp('1978-09-30 00:00:00'), Timestamp('1978-10-01 00:00:00'), Timestamp('1978-10-07 00:00:00'), Timestamp('1978-10-08 00:00:00'), Timestamp('1978-10-14 00:00:00'), Timestamp('1978-10-15 00:00:00'), Timestamp('1978-10-21 00:00:00'), Timestamp('1978-10-22 00:00:00'), Timestamp('1978-10-28 00:00:00'), Timestamp('1978-10-29 00:00:00'), Timestamp('1978-11-04 00:00:00'), Timestamp('1978-11-05 00:00:00'), Timestamp('1978-11-11 00:00:00'), Timestamp('1978-11-12 00:00:00'), Timestamp('1978-11-18 00:00:00'), Timestamp('1978-11-19 00:00:00'), Timestamp('1978-11-25 00:00:00'), Timestamp('1978-11-26 00:00:00'), Timestamp('1978-12-02 00:00:00'), Timestamp('1978-12-03 00:00:00'), Timestamp('1978-12-09 00:00:00'), Timestamp('1978-12-10 00:00:00'), Timestamp('1978-12-16 00:00:00'), Timestamp('1978-12-17 00:00:00'), Timestamp('1978-12-23 00:00:00'), Timestamp('1978-12-24 00:00:00'), Timestamp('1978-12-30 00:00:00'), Timestamp('1978-12-31 00:00:00'), Timestamp('1979-01-06 00:00:00'), Timestamp('1979-01-07 00:00:00'), Timestamp('1979-01-13 00:00:00'), Timestamp('1979-01-14 00:00:00'), Timestamp('1979-01-20 00:00:00'), Timestamp('1979-01-21 00:00:00'), Timestamp('1979-01-27 00:00:00'), Timestamp('1979-01-28 00:00:00'), Timestamp('1979-02-03 00:00:00'), Timestamp('1979-02-04 00:00:00'), Timestamp('1979-02-10 00:00:00'), Timestamp('1979-02-11 00:00:00'), Timestamp('1979-02-17 00:00:00'), Timestamp('1979-02-18 00:00:00'), Timestamp('1979-02-24 00:00:00'), Timestamp('1979-02-25 00:00:00'), Timestamp('1979-03-03 00:00:00'), Timestamp('1979-03-04 00:00:00'), Timestamp('1979-03-10 00:00:00'), Timestamp('1979-03-11 00:00:00'), Timestamp('1979-03-17 00:00:00'), Timestamp('1979-03-18 00:00:00'), Timestamp('1979-03-24 00:00:00'), Timestamp('1979-03-25 00:00:00'), Timestamp('1979-03-31 00:00:00'), Timestamp('1979-04-01 00:00:00'), Timestamp('1979-04-07 00:00:00'), Timestamp('1979-04-08 00:00:00'), Timestamp('1979-04-14 00:00:00'), Timestamp('1979-04-15 00:00:00'), Timestamp('1979-04-21 00:00:00'), Timestamp('1979-04-22 00:00:00'), Timestamp('1979-04-28 00:00:00'), Timestamp('1979-04-29 00:00:00'), Timestamp('1979-05-05 00:00:00'), Timestamp('1979-05-06 00:00:00'), Timestamp('1979-05-12 00:00:00'), Timestamp('1979-05-13 00:00:00'), Timestamp('1979-05-19 00:00:00'), Timestamp('1979-05-20 00:00:00'), Timestamp('1979-05-26 00:00:00'), Timestamp('1979-05-27 00:00:00'), Timestamp('1979-06-02 00:00:00'), Timestamp('1979-06-03 00:00:00'), Timestamp('1979-06-09 00:00:00'), Timestamp('1979-06-10 00:00:00'), Timestamp('1979-06-16 00:00:00'), Timestamp('1979-06-17 00:00:00'), Timestamp('1979-06-23 00:00:00'), Timestamp('1979-06-24 00:00:00'), Timestamp('1979-06-30 00:00:00'), Timestamp('1979-07-01 00:00:00'), Timestamp('1979-07-07 00:00:00'), Timestamp('1979-07-08 00:00:00'), Timestamp('1979-07-14 00:00:00'), Timestamp('1979-07-15 00:00:00'), Timestamp('1979-07-21 00:00:00'), Timestamp('1979-07-22 00:00:00'), Timestamp('1979-07-28 00:00:00'), Timestamp('1979-07-29 00:00:00'), Timestamp('1979-08-04 00:00:00'), Timestamp('1979-08-05 00:00:00'), Timestamp('1979-08-11 00:00:00'), Timestamp('1979-08-12 00:00:00'), Timestamp('1979-08-18 00:00:00'), Timestamp('1979-08-19 00:00:00'), Timestamp('1979-08-25 00:00:00'), Timestamp('1979-08-26 00:00:00'), Timestamp('1979-09-01 00:00:00'), Timestamp('1979-09-02 00:00:00'), Timestamp('1979-09-08 00:00:00'), Timestamp('1979-09-09 00:00:00'), Timestamp('1979-09-15 00:00:00'), Timestamp('1979-09-16 00:00:00'), Timestamp('1979-09-22 00:00:00'), Timestamp('1979-09-23 00:00:00'), Timestamp('1979-09-29 00:00:00'), Timestamp('1979-09-30 00:00:00'), Timestamp('1979-10-06 00:00:00'), Timestamp('1979-10-07 00:00:00'), Timestamp('1979-10-13 00:00:00'), Timestamp('1979-10-14 00:00:00'), Timestamp('1979-10-20 00:00:00'), Timestamp('1979-10-21 00:00:00'), Timestamp('1979-10-27 00:00:00'), Timestamp('1979-10-28 00:00:00'), Timestamp('1979-11-03 00:00:00'), Timestamp('1979-11-04 00:00:00'), Timestamp('1979-11-10 00:00:00'), Timestamp('1979-11-11 00:00:00'), Timestamp('1979-11-17 00:00:00'), Timestamp('1979-11-18 00:00:00'), Timestamp('1979-11-24 00:00:00'), Timestamp('1979-11-25 00:00:00'), Timestamp('1979-12-01 00:00:00'), Timestamp('1979-12-02 00:00:00'), Timestamp('1979-12-08 00:00:00'), Timestamp('1979-12-09 00:00:00'), Timestamp('1979-12-15 00:00:00'), Timestamp('1979-12-16 00:00:00'), Timestamp('1979-12-22 00:00:00'), Timestamp('1979-12-23 00:00:00'), Timestamp('1979-12-29 00:00:00'), Timestamp('1979-12-30 00:00:00'), Timestamp('1980-01-05 00:00:00'), Timestamp('1980-01-06 00:00:00'), Timestamp('1980-01-12 00:00:00'), Timestamp('1980-01-13 00:00:00'), Timestamp('1980-01-19 00:00:00'), Timestamp('1980-01-20 00:00:00'), Timestamp('1980-01-26 00:00:00'), Timestamp('1980-01-27 00:00:00'), Timestamp('1980-02-02 00:00:00'), Timestamp('1980-02-03 00:00:00'), Timestamp('1980-02-09 00:00:00'), Timestamp('1980-02-10 00:00:00'), Timestamp('1980-02-16 00:00:00'), Timestamp('1980-02-17 00:00:00'), Timestamp('1980-02-23 00:00:00'), Timestamp('1980-02-24 00:00:00'), Timestamp('1980-03-01 00:00:00'), Timestamp('1980-03-02 00:00:00'), Timestamp('1980-03-08 00:00:00'), Timestamp('1980-03-09 00:00:00'), Timestamp('1980-03-15 00:00:00'), Timestamp('1980-03-16 00:00:00'), Timestamp('1980-03-22 00:00:00'), Timestamp('1980-03-23 00:00:00'), Timestamp('1980-03-29 00:00:00'), Timestamp('1980-03-30 00:00:00'), Timestamp('1980-04-05 00:00:00'), Timestamp('1980-04-06 00:00:00'), Timestamp('1980-04-12 00:00:00'), Timestamp('1980-04-13 00:00:00'), Timestamp('1980-04-19 00:00:00'), Timestamp('1980-04-20 00:00:00'), Timestamp('1980-04-26 00:00:00'), Timestamp('1980-04-27 00:00:00'), Timestamp('1980-05-03 00:00:00'), Timestamp('1980-05-04 00:00:00'), Timestamp('1980-05-10 00:00:00'), Timestamp('1980-05-11 00:00:00'), Timestamp('1980-05-17 00:00:00'), Timestamp('1980-05-18 00:00:00'), Timestamp('1980-05-24 00:00:00'), Timestamp('1980-05-25 00:00:00'), Timestamp('1980-05-31 00:00:00'), Timestamp('1980-06-01 00:00:00'), Timestamp('1980-06-07 00:00:00'), Timestamp('1980-06-08 00:00:00'), Timestamp('1980-06-14 00:00:00'), Timestamp('1980-06-15 00:00:00'), Timestamp('1980-06-21 00:00:00'), Timestamp('1980-06-22 00:00:00'), Timestamp('1980-06-28 00:00:00'), Timestamp('1980-06-29 00:00:00'), Timestamp('1980-07-05 00:00:00'), Timestamp('1980-07-06 00:00:00'), Timestamp('1980-07-12 00:00:00'), Timestamp('1980-07-13 00:00:00'), Timestamp('1980-07-19 00:00:00'), Timestamp('1980-07-20 00:00:00'), Timestamp('1980-07-26 00:00:00'), Timestamp('1980-07-27 00:00:00'), Timestamp('1980-08-02 00:00:00'), Timestamp('1980-08-03 00:00:00'), Timestamp('1980-08-09 00:00:00'), Timestamp('1980-08-10 00:00:00'), Timestamp('1980-08-16 00:00:00'), Timestamp('1980-08-17 00:00:00'), Timestamp('1980-08-23 00:00:00'), Timestamp('1980-08-24 00:00:00'), Timestamp('1980-08-30 00:00:00'), Timestamp('1980-08-31 00:00:00'), Timestamp('1980-09-06 00:00:00'), Timestamp('1980-09-07 00:00:00'), Timestamp('1980-09-13 00:00:00'), Timestamp('1980-09-14 00:00:00'), Timestamp('1980-09-20 00:00:00'), Timestamp('1980-09-21 00:00:00'), Timestamp('1980-09-27 00:00:00'), Timestamp('1980-09-28 00:00:00'), Timestamp('1980-10-04 00:00:00'), Timestamp('1980-10-05 00:00:00'), Timestamp('1980-10-11 00:00:00'), Timestamp('1980-10-12 00:00:00'), Timestamp('1980-10-18 00:00:00'), Timestamp('1980-10-19 00:00:00'), Timestamp('1980-10-25 00:00:00'), Timestamp('1980-10-26 00:00:00'), Timestamp('1980-11-01 00:00:00'), Timestamp('1980-11-02 00:00:00'), Timestamp('1980-11-08 00:00:00'), Timestamp('1980-11-09 00:00:00'), Timestamp('1980-11-15 00:00:00'), Timestamp('1980-11-16 00:00:00'), Timestamp('1980-11-22 00:00:00'), Timestamp('1980-11-23 00:00:00'), Timestamp('1980-11-29 00:00:00'), Timestamp('1980-11-30 00:00:00'), Timestamp('1980-12-06 00:00:00'), Timestamp('1980-12-07 00:00:00'), Timestamp('1980-12-13 00:00:00'), Timestamp('1980-12-14 00:00:00'), Timestamp('1980-12-20 00:00:00'), Timestamp('1980-12-21 00:00:00'), Timestamp('1980-12-27 00:00:00'), Timestamp('1980-12-28 00:00:00'), Timestamp('1981-01-03 00:00:00'), Timestamp('1981-01-04 00:00:00'), Timestamp('1981-01-10 00:00:00'), Timestamp('1981-01-11 00:00:00'), Timestamp('1981-01-17 00:00:00'), Timestamp('1981-01-18 00:00:00'), Timestamp('1981-01-24 00:00:00'), Timestamp('1981-01-25 00:00:00'), Timestamp('1981-01-31 00:00:00'), Timestamp('1981-02-01 00:00:00'), Timestamp('1981-02-07 00:00:00'), Timestamp('1981-02-08 00:00:00'), Timestamp('1981-02-14 00:00:00'), Timestamp('1981-02-15 00:00:00'), Timestamp('1981-02-21 00:00:00'), Timestamp('1981-02-22 00:00:00'), Timestamp('1981-02-28 00:00:00'), Timestamp('1981-03-01 00:00:00'), Timestamp('1981-03-07 00:00:00'), Timestamp('1981-03-08 00:00:00'), Timestamp('1981-03-14 00:00:00'), Timestamp('1981-03-15 00:00:00'), Timestamp('1981-03-21 00:00:00'), Timestamp('1981-03-22 00:00:00'), Timestamp('1981-03-28 00:00:00'), Timestamp('1981-03-29 00:00:00'), Timestamp('1981-04-04 00:00:00'), Timestamp('1981-04-05 00:00:00'), Timestamp('1981-04-11 00:00:00'), Timestamp('1981-04-12 00:00:00'), Timestamp('1981-04-18 00:00:00'), Timestamp('1981-04-19 00:00:00'), Timestamp('1981-04-25 00:00:00'), Timestamp('1981-04-26 00:00:00'), Timestamp('1981-05-02 00:00:00'), Timestamp('1981-05-03 00:00:00'), Timestamp('1981-05-09 00:00:00'), Timestamp('1981-05-10 00:00:00'), Timestamp('1981-05-16 00:00:00'), Timestamp('1981-05-17 00:00:00'), Timestamp('1981-05-23 00:00:00'), Timestamp('1981-05-24 00:00:00'), Timestamp('1981-05-30 00:00:00'), Timestamp('1981-05-31 00:00:00'), Timestamp('1981-06-06 00:00:00'), Timestamp('1981-06-07 00:00:00'), Timestamp('1981-06-13 00:00:00'), Timestamp('1981-06-14 00:00:00'), Timestamp('1981-06-20 00:00:00'), Timestamp('1981-06-21 00:00:00'), Timestamp('1981-06-27 00:00:00'), Timestamp('1981-06-28 00:00:00'), Timestamp('1981-07-04 00:00:00'), Timestamp('1981-07-05 00:00:00'), Timestamp('1981-07-11 00:00:00'), Timestamp('1981-07-12 00:00:00'), Timestamp('1981-07-18 00:00:00'), Timestamp('1981-07-19 00:00:00'), Timestamp('1981-07-25 00:00:00'), Timestamp('1981-07-26 00:00:00'), Timestamp('1981-08-01 00:00:00'), Timestamp('1981-08-02 00:00:00'), Timestamp('1981-08-08 00:00:00'), Timestamp('1981-08-09 00:00:00'), Timestamp('1981-08-15 00:00:00'), Timestamp('1981-08-16 00:00:00'), Timestamp('1981-08-22 00:00:00'), Timestamp('1981-08-23 00:00:00'), Timestamp('1981-08-29 00:00:00'), Timestamp('1981-08-30 00:00:00'), Timestamp('1981-09-05 00:00:00'), Timestamp('1981-09-06 00:00:00'), Timestamp('1981-09-12 00:00:00'), Timestamp('1981-09-13 00:00:00'), Timestamp('1981-09-19 00:00:00'), Timestamp('1981-09-20 00:00:00'), Timestamp('1981-09-26 00:00:00'), Timestamp('1981-09-27 00:00:00'), Timestamp('1981-10-03 00:00:00'), Timestamp('1981-10-04 00:00:00'), Timestamp('1981-10-10 00:00:00'), Timestamp('1981-10-11 00:00:00'), Timestamp('1981-10-17 00:00:00'), Timestamp('1981-10-18 00:00:00'), Timestamp('1981-10-24 00:00:00'), Timestamp('1981-10-25 00:00:00'), Timestamp('1981-10-31 00:00:00'), Timestamp('1981-11-01 00:00:00'), Timestamp('1981-11-07 00:00:00'), Timestamp('1981-11-08 00:00:00'), Timestamp('1981-11-14 00:00:00'), Timestamp('1981-11-15 00:00:00'), Timestamp('1981-11-21 00:00:00'), Timestamp('1981-11-22 00:00:00'), Timestamp('1981-11-28 00:00:00'), Timestamp('1981-11-29 00:00:00'), Timestamp('1981-12-05 00:00:00'), Timestamp('1981-12-06 00:00:00'), Timestamp('1981-12-12 00:00:00'), Timestamp('1981-12-13 00:00:00'), Timestamp('1981-12-19 00:00:00'), Timestamp('1981-12-20 00:00:00'), Timestamp('1981-12-26 00:00:00'), Timestamp('1981-12-27 00:00:00'), Timestamp('1982-01-02 00:00:00'), Timestamp('1982-01-03 00:00:00'), Timestamp('1982-01-09 00:00:00'), Timestamp('1982-01-10 00:00:00'), Timestamp('1982-01-16 00:00:00'), Timestamp('1982-01-17 00:00:00'), Timestamp('1982-01-23 00:00:00'), Timestamp('1982-01-24 00:00:00'), Timestamp('1982-01-30 00:00:00'), Timestamp('1982-01-31 00:00:00'), Timestamp('1982-02-06 00:00:00'), Timestamp('1982-02-07 00:00:00'), Timestamp('1982-02-13 00:00:00'), Timestamp('1982-02-14 00:00:00'), Timestamp('1982-02-20 00:00:00'), Timestamp('1982-02-21 00:00:00'), Timestamp('1982-02-27 00:00:00'), Timestamp('1982-02-28 00:00:00'), Timestamp('1982-03-06 00:00:00'), Timestamp('1982-03-07 00:00:00'), Timestamp('1982-03-13 00:00:00'), Timestamp('1982-03-14 00:00:00'), Timestamp('1982-03-20 00:00:00'), Timestamp('1982-03-21 00:00:00'), Timestamp('1982-03-27 00:00:00'), Timestamp('1982-03-28 00:00:00'), Timestamp('1982-04-03 00:00:00'), Timestamp('1982-04-04 00:00:00'), Timestamp('1982-04-10 00:00:00'), Timestamp('1982-04-11 00:00:00'), Timestamp('1982-04-17 00:00:00'), Timestamp('1982-04-18 00:00:00'), Timestamp('1982-04-24 00:00:00'), Timestamp('1982-04-25 00:00:00'), Timestamp('1982-05-01 00:00:00'), Timestamp('1982-05-02 00:00:00'), Timestamp('1982-05-08 00:00:00'), Timestamp('1982-05-09 00:00:00'), Timestamp('1982-05-15 00:00:00'), Timestamp('1982-05-16 00:00:00'), Timestamp('1982-05-22 00:00:00'), Timestamp('1982-05-23 00:00:00'), Timestamp('1982-05-29 00:00:00'), Timestamp('1982-05-30 00:00:00'), Timestamp('1982-06-05 00:00:00'), Timestamp('1982-06-06 00:00:00'), Timestamp('1982-06-12 00:00:00'), Timestamp('1982-06-13 00:00:00'), Timestamp('1982-06-19 00:00:00'), Timestamp('1982-06-20 00:00:00'), Timestamp('1982-06-26 00:00:00'), Timestamp('1982-06-27 00:00:00'), Timestamp('1982-07-03 00:00:00'), Timestamp('1982-07-04 00:00:00'), Timestamp('1982-07-10 00:00:00'), Timestamp('1982-07-11 00:00:00'), Timestamp('1982-07-17 00:00:00'), Timestamp('1982-07-18 00:00:00'), Timestamp('1982-07-24 00:00:00'), Timestamp('1982-07-25 00:00:00'), Timestamp('1982-07-31 00:00:00'), Timestamp('1982-08-01 00:00:00'), Timestamp('1982-08-07 00:00:00'), Timestamp('1982-08-08 00:00:00'), Timestamp('1982-08-14 00:00:00'), Timestamp('1982-08-15 00:00:00'), Timestamp('1982-08-21 00:00:00'), Timestamp('1982-08-22 00:00:00'), Timestamp('1982-08-28 00:00:00'), Timestamp('1982-08-29 00:00:00'), Timestamp('1982-09-04 00:00:00'), Timestamp('1982-09-05 00:00:00'), Timestamp('1982-09-11 00:00:00'), Timestamp('1982-09-12 00:00:00'), Timestamp('1982-09-18 00:00:00'), Timestamp('1982-09-19 00:00:00'), Timestamp('1982-09-25 00:00:00'), Timestamp('1982-09-26 00:00:00'), Timestamp('1982-10-02 00:00:00'), Timestamp('1982-10-03 00:00:00'), Timestamp('1982-10-09 00:00:00'), Timestamp('1982-10-10 00:00:00'), Timestamp('1982-10-16 00:00:00'), Timestamp('1982-10-17 00:00:00'), Timestamp('1982-10-23 00:00:00'), Timestamp('1982-10-24 00:00:00'), Timestamp('1982-10-30 00:00:00'), Timestamp('1982-10-31 00:00:00'), Timestamp('1982-11-06 00:00:00'), Timestamp('1982-11-07 00:00:00'), Timestamp('1982-11-13 00:00:00'), Timestamp('1982-11-14 00:00:00'), Timestamp('1982-11-20 00:00:00'), Timestamp('1982-11-21 00:00:00'), Timestamp('1982-11-27 00:00:00'), Timestamp('1982-11-28 00:00:00'), Timestamp('1982-12-04 00:00:00'), Timestamp('1982-12-05 00:00:00'), Timestamp('1982-12-11 00:00:00'), Timestamp('1982-12-12 00:00:00'), Timestamp('1982-12-18 00:00:00'), Timestamp('1982-12-19 00:00:00'), Timestamp('1982-12-25 00:00:00'), Timestamp('1982-12-26 00:00:00'), Timestamp('1983-01-01 00:00:00'), Timestamp('1983-01-02 00:00:00'), Timestamp('1983-01-08 00:00:00'), Timestamp('1983-01-09 00:00:00'), Timestamp('1983-01-15 00:00:00'), Timestamp('1983-01-16 00:00:00'), Timestamp('1983-01-22 00:00:00'), Timestamp('1983-01-23 00:00:00'), Timestamp('1983-01-29 00:00:00'), Timestamp('1983-01-30 00:00:00'), Timestamp('1983-02-05 00:00:00'), Timestamp('1983-02-06 00:00:00'), Timestamp('1983-02-12 00:00:00'), Timestamp('1983-02-13 00:00:00'), Timestamp('1983-02-19 00:00:00'), Timestamp('1983-02-20 00:00:00'), Timestamp('1983-02-26 00:00:00'), Timestamp('1983-02-27 00:00:00'), Timestamp('1983-03-05 00:00:00'), Timestamp('1983-03-06 00:00:00'), Timestamp('1983-03-12 00:00:00'), Timestamp('1983-03-13 00:00:00'), Timestamp('1983-03-19 00:00:00'), Timestamp('1983-03-20 00:00:00'), Timestamp('1983-03-26 00:00:00'), Timestamp('1983-03-27 00:00:00'), Timestamp('1983-04-02 00:00:00'), Timestamp('1983-04-03 00:00:00'), Timestamp('1983-04-09 00:00:00'), Timestamp('1983-04-10 00:00:00'), Timestamp('1983-04-16 00:00:00'), Timestamp('1983-04-17 00:00:00'), Timestamp('1983-04-23 00:00:00'), Timestamp('1983-04-24 00:00:00'), Timestamp('1983-04-30 00:00:00'), Timestamp('1983-05-01 00:00:00'), Timestamp('1983-05-07 00:00:00'), Timestamp('1983-05-08 00:00:00'), Timestamp('1983-05-14 00:00:00'), Timestamp('1983-05-15 00:00:00'), Timestamp('1983-05-21 00:00:00'), Timestamp('1983-05-22 00:00:00'), Timestamp('1983-05-28 00:00:00'), Timestamp('1983-05-29 00:00:00'), Timestamp('1983-06-04 00:00:00'), Timestamp('1983-06-05 00:00:00'), Timestamp('1983-06-11 00:00:00'), Timestamp('1983-06-12 00:00:00'), Timestamp('1983-06-18 00:00:00'), Timestamp('1983-06-19 00:00:00'), Timestamp('1983-06-25 00:00:00'), Timestamp('1983-06-26 00:00:00'), Timestamp('1983-07-02 00:00:00'), Timestamp('1983-07-03 00:00:00'), Timestamp('1983-07-09 00:00:00'), Timestamp('1983-07-10 00:00:00'), Timestamp('1983-07-16 00:00:00'), Timestamp('1983-07-17 00:00:00'), Timestamp('1983-07-23 00:00:00'), Timestamp('1983-07-24 00:00:00'), Timestamp('1983-07-30 00:00:00'), Timestamp('1983-07-31 00:00:00'), Timestamp('1983-08-06 00:00:00'), Timestamp('1983-08-07 00:00:00'), Timestamp('1983-08-13 00:00:00'), Timestamp('1983-08-14 00:00:00'), Timestamp('1983-08-20 00:00:00'), Timestamp('1983-08-21 00:00:00'), Timestamp('1983-08-27 00:00:00'), Timestamp('1983-08-28 00:00:00'), Timestamp('1983-09-03 00:00:00'), Timestamp('1983-09-04 00:00:00'), Timestamp('1983-09-10 00:00:00'), Timestamp('1983-09-11 00:00:00'), Timestamp('1983-09-17 00:00:00'), Timestamp('1983-09-18 00:00:00'), Timestamp('1983-09-24 00:00:00'), Timestamp('1983-09-25 00:00:00'), Timestamp('1983-10-01 00:00:00'), Timestamp('1983-10-02 00:00:00'), Timestamp('1983-10-08 00:00:00'), Timestamp('1983-10-09 00:00:00'), Timestamp('1983-10-15 00:00:00'), Timestamp('1983-10-16 00:00:00'), Timestamp('1983-10-22 00:00:00'), Timestamp('1983-10-23 00:00:00'), Timestamp('1983-10-29 00:00:00'), Timestamp('1983-10-30 00:00:00'), Timestamp('1983-11-05 00:00:00'), Timestamp('1983-11-06 00:00:00'), Timestamp('1983-11-12 00:00:00'), Timestamp('1983-11-13 00:00:00'), Timestamp('1983-11-19 00:00:00'), Timestamp('1983-11-20 00:00:00'), Timestamp('1983-11-26 00:00:00'), Timestamp('1983-11-27 00:00:00'), Timestamp('1983-12-03 00:00:00'), Timestamp('1983-12-04 00:00:00'), Timestamp('1983-12-10 00:00:00'), Timestamp('1983-12-11 00:00:00'), Timestamp('1983-12-17 00:00:00'), Timestamp('1983-12-18 00:00:00'), Timestamp('1983-12-24 00:00:00'), Timestamp('1983-12-25 00:00:00'), Timestamp('1983-12-31 00:00:00'), Timestamp('1984-01-01 00:00:00'), Timestamp('1984-01-07 00:00:00'), Timestamp('1984-01-08 00:00:00'), Timestamp('1984-01-14 00:00:00'), Timestamp('1984-01-15 00:00:00'), Timestamp('1984-01-21 00:00:00'), Timestamp('1984-01-22 00:00:00'), Timestamp('1984-01-28 00:00:00'), Timestamp('1984-01-29 00:00:00'), Timestamp('1984-02-04 00:00:00'), Timestamp('1984-02-05 00:00:00'), Timestamp('1984-02-11 00:00:00'), Timestamp('1984-02-12 00:00:00'), Timestamp('1984-02-18 00:00:00'), Timestamp('1984-02-19 00:00:00'), Timestamp('1984-02-25 00:00:00'), Timestamp('1984-02-26 00:00:00'), Timestamp('1984-03-03 00:00:00'), Timestamp('1984-03-04 00:00:00'), Timestamp('1984-03-10 00:00:00'), Timestamp('1984-03-11 00:00:00'), Timestamp('1984-03-17 00:00:00'), Timestamp('1984-03-18 00:00:00'), Timestamp('1984-03-24 00:00:00'), Timestamp('1984-03-25 00:00:00'), Timestamp('1984-03-31 00:00:00'), Timestamp('1984-04-01 00:00:00'), Timestamp('1984-04-07 00:00:00'), Timestamp('1984-04-08 00:00:00'), Timestamp('1984-04-14 00:00:00'), Timestamp('1984-04-15 00:00:00'), Timestamp('1984-04-21 00:00:00'), Timestamp('1984-04-22 00:00:00'), Timestamp('1984-04-28 00:00:00'), Timestamp('1984-04-29 00:00:00'), Timestamp('1984-05-05 00:00:00'), Timestamp('1984-05-06 00:00:00'), Timestamp('1984-05-12 00:00:00'), Timestamp('1984-05-13 00:00:00'), Timestamp('1984-05-19 00:00:00'), Timestamp('1984-05-20 00:00:00'), Timestamp('1984-05-26 00:00:00'), Timestamp('1984-05-27 00:00:00'), Timestamp('1984-06-02 00:00:00'), Timestamp('1984-06-03 00:00:00'), Timestamp('1984-06-09 00:00:00'), Timestamp('1984-06-10 00:00:00'), Timestamp('1984-06-16 00:00:00'), Timestamp('1984-06-17 00:00:00'), Timestamp('1984-06-23 00:00:00'), Timestamp('1984-06-24 00:00:00'), Timestamp('1984-06-30 00:00:00'), Timestamp('1984-07-01 00:00:00'), Timestamp('1984-07-07 00:00:00'), Timestamp('1984-07-08 00:00:00'), Timestamp('1984-07-14 00:00:00'), Timestamp('1984-07-15 00:00:00'), Timestamp('1984-07-21 00:00:00'), Timestamp('1984-07-22 00:00:00'), Timestamp('1984-07-28 00:00:00'), Timestamp('1984-07-29 00:00:00'), Timestamp('1984-08-04 00:00:00'), Timestamp('1984-08-05 00:00:00'), Timestamp('1984-08-11 00:00:00'), Timestamp('1984-08-12 00:00:00'), Timestamp('1984-08-18 00:00:00'), Timestamp('1984-08-19 00:00:00'), Timestamp('1984-08-25 00:00:00'), Timestamp('1984-08-26 00:00:00'), Timestamp('1984-09-01 00:00:00'), Timestamp('1984-09-02 00:00:00'), Timestamp('1984-09-08 00:00:00'), Timestamp('1984-09-09 00:00:00'), Timestamp('1984-09-15 00:00:00'), Timestamp('1984-09-16 00:00:00'), Timestamp('1984-09-22 00:00:00'), Timestamp('1984-09-23 00:00:00'), Timestamp('1984-09-29 00:00:00'), Timestamp('1984-09-30 00:00:00'), Timestamp('1984-10-06 00:00:00'), Timestamp('1984-10-07 00:00:00'), Timestamp('1984-10-13 00:00:00'), Timestamp('1984-10-14 00:00:00'), Timestamp('1984-10-20 00:00:00'), Timestamp('1984-10-21 00:00:00'), Timestamp('1984-10-27 00:00:00'), Timestamp('1984-10-28 00:00:00'), Timestamp('1984-11-03 00:00:00'), Timestamp('1984-11-04 00:00:00'), Timestamp('1984-11-10 00:00:00'), Timestamp('1984-11-11 00:00:00'), Timestamp('1984-11-17 00:00:00'), Timestamp('1984-11-18 00:00:00'), Timestamp('1984-11-24 00:00:00'), Timestamp('1984-11-25 00:00:00'), Timestamp('1984-12-01 00:00:00'), Timestamp('1984-12-02 00:00:00'), Timestamp('1984-12-08 00:00:00'), Timestamp('1984-12-09 00:00:00'), Timestamp('1984-12-15 00:00:00'), Timestamp('1984-12-16 00:00:00'), Timestamp('1984-12-22 00:00:00'), Timestamp('1984-12-23 00:00:00'), Timestamp('1984-12-29 00:00:00'), Timestamp('1984-12-30 00:00:00'), Timestamp('1985-01-05 00:00:00'), Timestamp('1985-01-06 00:00:00'), Timestamp('1985-01-12 00:00:00'), Timestamp('1985-01-13 00:00:00'), Timestamp('1985-01-19 00:00:00'), Timestamp('1985-01-20 00:00:00'), Timestamp('1985-01-26 00:00:00'), Timestamp('1985-01-27 00:00:00'), Timestamp('1985-02-02 00:00:00'), Timestamp('1985-02-03 00:00:00'), Timestamp('1985-02-09 00:00:00'), Timestamp('1985-02-10 00:00:00'), Timestamp('1985-02-16 00:00:00'), Timestamp('1985-02-17 00:00:00'), Timestamp('1985-02-23 00:00:00'), Timestamp('1985-02-24 00:00:00'), Timestamp('1985-03-02 00:00:00'), Timestamp('1985-03-03 00:00:00'), Timestamp('1985-03-09 00:00:00'), Timestamp('1985-03-10 00:00:00'), Timestamp('1985-03-16 00:00:00'), Timestamp('1985-03-17 00:00:00'), Timestamp('1985-03-23 00:00:00'), Timestamp('1985-03-24 00:00:00'), Timestamp('1985-03-30 00:00:00'), Timestamp('1985-03-31 00:00:00'), Timestamp('1985-04-06 00:00:00'), Timestamp('1985-04-07 00:00:00'), Timestamp('1985-04-13 00:00:00'), Timestamp('1985-04-14 00:00:00'), Timestamp('1985-04-20 00:00:00'), Timestamp('1985-04-21 00:00:00'), Timestamp('1985-04-27 00:00:00'), Timestamp('1985-04-28 00:00:00'), Timestamp('1985-05-04 00:00:00'), Timestamp('1985-05-05 00:00:00'), Timestamp('1985-05-11 00:00:00'), Timestamp('1985-05-12 00:00:00'), Timestamp('1985-05-18 00:00:00'), Timestamp('1985-05-19 00:00:00'), Timestamp('1985-05-25 00:00:00'), Timestamp('1985-05-26 00:00:00'), Timestamp('1985-06-01 00:00:00'), Timestamp('1985-06-02 00:00:00'), Timestamp('1985-06-08 00:00:00'), Timestamp('1985-06-09 00:00:00'), Timestamp('1985-06-15 00:00:00'), Timestamp('1985-06-16 00:00:00'), Timestamp('1985-06-22 00:00:00'), Timestamp('1985-06-23 00:00:00'), Timestamp('1985-06-29 00:00:00'), Timestamp('1985-06-30 00:00:00'), Timestamp('1985-07-06 00:00:00'), Timestamp('1985-07-07 00:00:00'), Timestamp('1985-07-13 00:00:00'), Timestamp('1985-07-14 00:00:00'), Timestamp('1985-07-20 00:00:00'), Timestamp('1985-07-21 00:00:00'), Timestamp('1985-07-27 00:00:00'), Timestamp('1985-07-28 00:00:00'), Timestamp('1985-08-03 00:00:00'), Timestamp('1985-08-04 00:00:00'), Timestamp('1985-08-10 00:00:00'), Timestamp('1985-08-11 00:00:00'), Timestamp('1985-08-17 00:00:00'), Timestamp('1985-08-18 00:00:00'), Timestamp('1985-08-24 00:00:00'), Timestamp('1985-08-25 00:00:00'), Timestamp('1985-08-31 00:00:00'), Timestamp('1985-09-01 00:00:00'), Timestamp('1985-09-07 00:00:00'), Timestamp('1985-09-08 00:00:00'), Timestamp('1985-09-14 00:00:00'), Timestamp('1985-09-15 00:00:00'), Timestamp('1985-09-21 00:00:00'), Timestamp('1985-09-22 00:00:00'), Timestamp('1985-09-28 00:00:00'), Timestamp('1985-09-29 00:00:00'), Timestamp('1985-10-05 00:00:00'), Timestamp('1985-10-06 00:00:00'), Timestamp('1985-10-12 00:00:00'), Timestamp('1985-10-13 00:00:00'), Timestamp('1985-10-19 00:00:00'), Timestamp('1985-10-20 00:00:00'), Timestamp('1985-10-26 00:00:00'), Timestamp('1985-10-27 00:00:00'), Timestamp('1985-11-02 00:00:00'), Timestamp('1985-11-03 00:00:00'), Timestamp('1985-11-09 00:00:00'), Timestamp('1985-11-10 00:00:00'), Timestamp('1985-11-16 00:00:00'), Timestamp('1985-11-17 00:00:00'), Timestamp('1985-11-23 00:00:00'), Timestamp('1985-11-24 00:00:00'), Timestamp('1985-11-30 00:00:00'), Timestamp('1985-12-01 00:00:00'), Timestamp('1985-12-07 00:00:00'), Timestamp('1985-12-08 00:00:00'), Timestamp('1985-12-14 00:00:00'), Timestamp('1985-12-15 00:00:00'), Timestamp('1985-12-21 00:00:00'), Timestamp('1985-12-22 00:00:00'), Timestamp('1985-12-28 00:00:00'), Timestamp('1985-12-29 00:00:00'), Timestamp('1986-01-04 00:00:00'), Timestamp('1986-01-05 00:00:00'), Timestamp('1986-01-11 00:00:00'), Timestamp('1986-01-12 00:00:00'), Timestamp('1986-01-18 00:00:00'), Timestamp('1986-01-19 00:00:00'), Timestamp('1986-01-25 00:00:00'), Timestamp('1986-01-26 00:00:00'), Timestamp('1986-02-01 00:00:00'), Timestamp('1986-02-02 00:00:00'), Timestamp('1986-02-08 00:00:00'), Timestamp('1986-02-09 00:00:00'), Timestamp('1986-02-15 00:00:00'), Timestamp('1986-02-16 00:00:00'), Timestamp('1986-02-22 00:00:00'), Timestamp('1986-02-23 00:00:00'), Timestamp('1986-03-01 00:00:00'), Timestamp('1986-03-02 00:00:00'), Timestamp('1986-03-08 00:00:00'), Timestamp('1986-03-09 00:00:00'), Timestamp('1986-03-15 00:00:00'), Timestamp('1986-03-16 00:00:00'), Timestamp('1986-03-22 00:00:00'), Timestamp('1986-03-23 00:00:00'), Timestamp('1986-03-29 00:00:00'), Timestamp('1986-03-30 00:00:00'), Timestamp('1986-04-05 00:00:00'), Timestamp('1986-04-06 00:00:00'), Timestamp('1986-04-12 00:00:00'), Timestamp('1986-04-13 00:00:00'), Timestamp('1986-04-19 00:00:00'), Timestamp('1986-04-20 00:00:00'), Timestamp('1986-04-26 00:00:00'), Timestamp('1986-04-27 00:00:00'), Timestamp('1986-05-03 00:00:00'), Timestamp('1986-05-04 00:00:00'), Timestamp('1986-05-10 00:00:00'), Timestamp('1986-05-11 00:00:00'), Timestamp('1986-05-17 00:00:00'), Timestamp('1986-05-18 00:00:00'), Timestamp('1986-05-24 00:00:00'), Timestamp('1986-05-25 00:00:00'), Timestamp('1986-05-31 00:00:00'), Timestamp('1986-06-01 00:00:00'), Timestamp('1986-06-07 00:00:00'), Timestamp('1986-06-08 00:00:00'), Timestamp('1986-06-14 00:00:00'), Timestamp('1986-06-15 00:00:00'), Timestamp('1986-06-21 00:00:00'), Timestamp('1986-06-22 00:00:00'), Timestamp('1986-06-28 00:00:00'), Timestamp('1986-06-29 00:00:00'), Timestamp('1986-07-05 00:00:00'), Timestamp('1986-07-06 00:00:00'), Timestamp('1986-07-12 00:00:00'), Timestamp('1986-07-13 00:00:00'), Timestamp('1986-07-19 00:00:00'), Timestamp('1986-07-20 00:00:00'), Timestamp('1986-07-26 00:00:00'), Timestamp('1986-07-27 00:00:00'), Timestamp('1986-08-02 00:00:00'), Timestamp('1986-08-03 00:00:00'), Timestamp('1986-08-09 00:00:00'), Timestamp('1986-08-10 00:00:00'), Timestamp('1986-08-16 00:00:00'), Timestamp('1986-08-17 00:00:00'), Timestamp('1986-08-23 00:00:00'), Timestamp('1986-08-24 00:00:00'), Timestamp('1986-08-30 00:00:00'), Timestamp('1986-08-31 00:00:00'), Timestamp('1986-09-06 00:00:00'), Timestamp('1986-09-07 00:00:00'), Timestamp('1986-09-13 00:00:00'), Timestamp('1986-09-14 00:00:00'), Timestamp('1986-09-20 00:00:00'), Timestamp('1986-09-21 00:00:00'), Timestamp('1986-09-27 00:00:00'), Timestamp('1986-09-28 00:00:00'), Timestamp('1986-10-04 00:00:00'), Timestamp('1986-10-05 00:00:00'), Timestamp('1986-10-11 00:00:00'), Timestamp('1986-10-12 00:00:00'), Timestamp('1986-10-18 00:00:00'), Timestamp('1986-10-19 00:00:00'), Timestamp('1986-10-25 00:00:00'), Timestamp('1986-10-26 00:00:00'), Timestamp('1986-11-01 00:00:00'), Timestamp('1986-11-02 00:00:00'), Timestamp('1986-11-08 00:00:00'), Timestamp('1986-11-09 00:00:00'), Timestamp('1986-11-15 00:00:00'), Timestamp('1986-11-16 00:00:00'), Timestamp('1986-11-22 00:00:00'), Timestamp('1986-11-23 00:00:00'), Timestamp('1986-11-29 00:00:00'), Timestamp('1986-11-30 00:00:00'), Timestamp('1986-12-06 00:00:00'), Timestamp('1986-12-07 00:00:00'), Timestamp('1986-12-13 00:00:00'), Timestamp('1986-12-14 00:00:00'), Timestamp('1986-12-20 00:00:00'), Timestamp('1986-12-21 00:00:00'), Timestamp('1986-12-27 00:00:00'), Timestamp('1986-12-28 00:00:00'), Timestamp('1987-01-03 00:00:00'), Timestamp('1987-01-04 00:00:00'), Timestamp('1987-01-10 00:00:00'), Timestamp('1987-01-11 00:00:00'), Timestamp('1987-01-17 00:00:00'), Timestamp('1987-01-18 00:00:00'), Timestamp('1987-01-24 00:00:00'), Timestamp('1987-01-25 00:00:00'), Timestamp('1987-01-31 00:00:00'), Timestamp('1987-02-01 00:00:00'), Timestamp('1987-02-07 00:00:00'), Timestamp('1987-02-08 00:00:00'), Timestamp('1987-02-14 00:00:00'), Timestamp('1987-02-15 00:00:00'), Timestamp('1987-02-21 00:00:00'), Timestamp('1987-02-22 00:00:00'), Timestamp('1987-02-28 00:00:00'), Timestamp('1987-03-01 00:00:00'), Timestamp('1987-03-07 00:00:00'), Timestamp('1987-03-08 00:00:00'), Timestamp('1987-03-14 00:00:00'), Timestamp('1987-03-15 00:00:00'), Timestamp('1987-03-21 00:00:00'), Timestamp('1987-03-22 00:00:00'), Timestamp('1987-03-28 00:00:00'), Timestamp('1987-03-29 00:00:00'), Timestamp('1987-04-04 00:00:00'), Timestamp('1987-04-05 00:00:00'), Timestamp('1987-04-11 00:00:00'), Timestamp('1987-04-12 00:00:00'), Timestamp('1987-04-18 00:00:00'), Timestamp('1987-04-19 00:00:00'), Timestamp('1987-04-25 00:00:00'), Timestamp('1987-04-26 00:00:00'), Timestamp('1987-05-02 00:00:00'), Timestamp('1987-05-03 00:00:00'), Timestamp('1987-05-09 00:00:00'), Timestamp('1987-05-10 00:00:00'), Timestamp('1987-05-16 00:00:00'), Timestamp('1987-05-17 00:00:00'), Timestamp('1987-05-23 00:00:00'), Timestamp('1987-05-24 00:00:00'), Timestamp('1987-05-30 00:00:00'), Timestamp('1987-05-31 00:00:00'), Timestamp('1987-06-06 00:00:00'), Timestamp('1987-06-07 00:00:00'), Timestamp('1987-06-13 00:00:00'), Timestamp('1987-06-14 00:00:00'), Timestamp('1987-06-20 00:00:00'), Timestamp('1987-06-21 00:00:00'), Timestamp('1987-06-27 00:00:00'), Timestamp('1987-06-28 00:00:00'), Timestamp('1987-07-04 00:00:00'), Timestamp('1987-07-05 00:00:00'), Timestamp('1987-07-11 00:00:00'), Timestamp('1987-07-12 00:00:00'), Timestamp('1987-07-18 00:00:00'), Timestamp('1987-07-19 00:00:00'), Timestamp('1987-07-25 00:00:00'), Timestamp('1987-07-26 00:00:00'), Timestamp('1987-08-01 00:00:00'), Timestamp('1987-08-02 00:00:00'), Timestamp('1987-08-08 00:00:00'), Timestamp('1987-08-09 00:00:00'), Timestamp('1987-08-15 00:00:00'), Timestamp('1987-08-16 00:00:00'), Timestamp('1987-08-22 00:00:00'), Timestamp('1987-08-23 00:00:00'), Timestamp('1987-08-29 00:00:00'), Timestamp('1987-08-30 00:00:00'), Timestamp('1987-09-05 00:00:00'), Timestamp('1987-09-06 00:00:00'), Timestamp('1987-09-12 00:00:00'), Timestamp('1987-09-13 00:00:00'), Timestamp('1987-09-19 00:00:00'), Timestamp('1987-09-20 00:00:00'), Timestamp('1987-09-26 00:00:00'), Timestamp('1987-09-27 00:00:00'), Timestamp('1987-10-03 00:00:00'), Timestamp('1987-10-04 00:00:00'), Timestamp('1987-10-10 00:00:00'), Timestamp('1987-10-11 00:00:00'), Timestamp('1987-10-17 00:00:00'), Timestamp('1987-10-18 00:00:00'), Timestamp('1987-10-24 00:00:00'), Timestamp('1987-10-25 00:00:00'), Timestamp('1987-10-31 00:00:00'), Timestamp('1987-11-01 00:00:00'), Timestamp('1987-11-07 00:00:00'), Timestamp('1987-11-08 00:00:00'), Timestamp('1987-11-14 00:00:00'), Timestamp('1987-11-15 00:00:00'), Timestamp('1987-11-21 00:00:00'), Timestamp('1987-11-22 00:00:00'), Timestamp('1987-11-28 00:00:00'), Timestamp('1987-11-29 00:00:00'), Timestamp('1987-12-05 00:00:00'), Timestamp('1987-12-06 00:00:00'), Timestamp('1987-12-12 00:00:00'), Timestamp('1987-12-13 00:00:00'), Timestamp('1987-12-19 00:00:00'), Timestamp('1987-12-20 00:00:00'), Timestamp('1987-12-26 00:00:00'), Timestamp('1987-12-27 00:00:00'), Timestamp('1988-01-02 00:00:00'), Timestamp('1988-01-03 00:00:00'), Timestamp('1988-01-09 00:00:00'), Timestamp('1988-01-10 00:00:00'), Timestamp('1988-01-16 00:00:00'), Timestamp('1988-01-17 00:00:00'), Timestamp('1988-01-23 00:00:00'), Timestamp('1988-01-24 00:00:00'), Timestamp('1988-01-30 00:00:00'), Timestamp('1988-01-31 00:00:00'), Timestamp('1988-02-06 00:00:00'), Timestamp('1988-02-07 00:00:00'), Timestamp('1988-02-13 00:00:00'), Timestamp('1988-02-14 00:00:00'), Timestamp('1988-02-20 00:00:00'), Timestamp('1988-02-21 00:00:00'), Timestamp('1988-02-27 00:00:00'), Timestamp('1988-02-28 00:00:00'), Timestamp('1988-03-05 00:00:00'), Timestamp('1988-03-06 00:00:00'), Timestamp('1988-03-12 00:00:00'), Timestamp('1988-03-13 00:00:00'), Timestamp('1988-03-19 00:00:00'), Timestamp('1988-03-20 00:00:00'), Timestamp('1988-03-26 00:00:00'), Timestamp('1988-03-27 00:00:00'), Timestamp('1988-04-02 00:00:00'), Timestamp('1988-04-03 00:00:00'), Timestamp('1988-04-09 00:00:00'), Timestamp('1988-04-10 00:00:00'), Timestamp('1988-04-16 00:00:00'), Timestamp('1988-04-17 00:00:00'), Timestamp('1988-04-23 00:00:00'), Timestamp('1988-04-24 00:00:00'), Timestamp('1988-04-30 00:00:00'), Timestamp('1988-05-01 00:00:00'), Timestamp('1988-05-07 00:00:00'), Timestamp('1988-05-08 00:00:00'), Timestamp('1988-05-14 00:00:00'), Timestamp('1988-05-15 00:00:00'), Timestamp('1988-05-21 00:00:00'), Timestamp('1988-05-22 00:00:00'), Timestamp('1988-05-28 00:00:00'), Timestamp('1988-05-29 00:00:00'), Timestamp('1988-06-04 00:00:00'), Timestamp('1988-06-05 00:00:00'), Timestamp('1988-06-11 00:00:00'), Timestamp('1988-06-12 00:00:00'), Timestamp('1988-06-18 00:00:00'), Timestamp('1988-06-19 00:00:00'), Timestamp('1988-06-25 00:00:00'), Timestamp('1988-06-26 00:00:00'), Timestamp('1988-07-02 00:00:00'), Timestamp('1988-07-03 00:00:00'), Timestamp('1988-07-09 00:00:00'), Timestamp('1988-07-10 00:00:00'), Timestamp('1988-07-16 00:00:00'), Timestamp('1988-07-17 00:00:00'), Timestamp('1988-07-23 00:00:00'), Timestamp('1988-07-24 00:00:00'), Timestamp('1988-07-30 00:00:00'), Timestamp('1988-07-31 00:00:00'), Timestamp('1988-08-06 00:00:00'), Timestamp('1988-08-07 00:00:00'), Timestamp('1988-08-13 00:00:00'), Timestamp('1988-08-14 00:00:00'), Timestamp('1988-08-20 00:00:00'), Timestamp('1988-08-21 00:00:00'), Timestamp('1988-08-27 00:00:00'), Timestamp('1988-08-28 00:00:00'), Timestamp('1988-09-03 00:00:00'), Timestamp('1988-09-04 00:00:00'), Timestamp('1988-09-10 00:00:00'), Timestamp('1988-09-11 00:00:00'), Timestamp('1988-09-17 00:00:00'), Timestamp('1988-09-18 00:00:00'), Timestamp('1988-09-24 00:00:00'), Timestamp('1988-09-25 00:00:00'), Timestamp('1988-10-01 00:00:00'), Timestamp('1988-10-02 00:00:00'), Timestamp('1988-10-08 00:00:00'), Timestamp('1988-10-09 00:00:00'), Timestamp('1988-10-15 00:00:00'), Timestamp('1988-10-16 00:00:00'), Timestamp('1988-10-22 00:00:00'), Timestamp('1988-10-23 00:00:00'), Timestamp('1988-10-29 00:00:00'), Timestamp('1988-10-30 00:00:00'), Timestamp('1988-11-05 00:00:00'), Timestamp('1988-11-06 00:00:00'), Timestamp('1988-11-12 00:00:00'), Timestamp('1988-11-13 00:00:00'), Timestamp('1988-11-19 00:00:00'), Timestamp('1988-11-20 00:00:00'), Timestamp('1988-11-26 00:00:00'), Timestamp('1988-11-27 00:00:00'), Timestamp('1988-12-03 00:00:00'), Timestamp('1988-12-04 00:00:00'), Timestamp('1988-12-10 00:00:00'), Timestamp('1988-12-11 00:00:00'), Timestamp('1988-12-17 00:00:00'), Timestamp('1988-12-18 00:00:00'), Timestamp('1988-12-24 00:00:00'), Timestamp('1988-12-25 00:00:00'), Timestamp('1988-12-31 00:00:00'), Timestamp('1989-01-01 00:00:00'), Timestamp('1989-01-07 00:00:00'), Timestamp('1989-01-08 00:00:00'), Timestamp('1989-01-14 00:00:00'), Timestamp('1989-01-15 00:00:00'), Timestamp('1989-01-21 00:00:00'), Timestamp('1989-01-22 00:00:00'), Timestamp('1989-01-28 00:00:00'), Timestamp('1989-01-29 00:00:00'), Timestamp('1989-02-04 00:00:00'), Timestamp('1989-02-05 00:00:00'), Timestamp('1989-02-11 00:00:00'), Timestamp('1989-02-12 00:00:00'), Timestamp('1989-02-18 00:00:00'), Timestamp('1989-02-19 00:00:00'), Timestamp('1989-02-25 00:00:00'), Timestamp('1989-02-26 00:00:00'), Timestamp('1989-03-04 00:00:00'), Timestamp('1989-03-05 00:00:00'), Timestamp('1989-03-11 00:00:00'), Timestamp('1989-03-12 00:00:00'), Timestamp('1989-03-18 00:00:00'), Timestamp('1989-03-19 00:00:00'), Timestamp('1989-03-25 00:00:00'), Timestamp('1989-03-26 00:00:00'), Timestamp('1989-04-01 00:00:00'), Timestamp('1989-04-02 00:00:00'), Timestamp('1989-04-08 00:00:00'), Timestamp('1989-04-09 00:00:00'), Timestamp('1989-04-15 00:00:00'), Timestamp('1989-04-16 00:00:00'), Timestamp('1989-04-22 00:00:00'), Timestamp('1989-04-23 00:00:00'), Timestamp('1989-04-29 00:00:00'), Timestamp('1989-04-30 00:00:00'), Timestamp('1989-05-06 00:00:00'), Timestamp('1989-05-07 00:00:00'), Timestamp('1989-05-13 00:00:00'), Timestamp('1989-05-14 00:00:00'), Timestamp('1989-05-20 00:00:00'), Timestamp('1989-05-21 00:00:00'), Timestamp('1989-05-27 00:00:00'), Timestamp('1989-05-28 00:00:00'), Timestamp('1989-06-03 00:00:00'), Timestamp('1989-06-04 00:00:00'), Timestamp('1989-06-10 00:00:00'), Timestamp('1989-06-11 00:00:00'), Timestamp('1989-06-17 00:00:00'), Timestamp('1989-06-18 00:00:00'), Timestamp('1989-06-24 00:00:00'), Timestamp('1989-06-25 00:00:00'), Timestamp('1989-07-01 00:00:00'), Timestamp('1989-07-02 00:00:00'), Timestamp('1989-07-08 00:00:00'), Timestamp('1989-07-09 00:00:00'), Timestamp('1989-07-15 00:00:00'), Timestamp('1989-07-16 00:00:00'), Timestamp('1989-07-22 00:00:00'), Timestamp('1989-07-23 00:00:00'), Timestamp('1989-07-29 00:00:00'), Timestamp('1989-07-30 00:00:00'), Timestamp('1989-08-05 00:00:00'), Timestamp('1989-08-06 00:00:00'), Timestamp('1989-08-12 00:00:00'), Timestamp('1989-08-13 00:00:00'), Timestamp('1989-08-19 00:00:00'), Timestamp('1989-08-20 00:00:00'), Timestamp('1989-08-26 00:00:00'), Timestamp('1989-08-27 00:00:00'), Timestamp('1989-09-02 00:00:00'), Timestamp('1989-09-03 00:00:00'), Timestamp('1989-09-09 00:00:00'), Timestamp('1989-09-10 00:00:00'), Timestamp('1989-09-16 00:00:00'), Timestamp('1989-09-17 00:00:00'), Timestamp('1989-09-23 00:00:00'), Timestamp('1989-09-24 00:00:00'), Timestamp('1989-09-30 00:00:00'), Timestamp('1989-10-01 00:00:00'), Timestamp('1989-10-07 00:00:00'), Timestamp('1989-10-08 00:00:00'), Timestamp('1989-10-14 00:00:00'), Timestamp('1989-10-15 00:00:00'), Timestamp('1989-10-21 00:00:00'), Timestamp('1989-10-22 00:00:00'), Timestamp('1989-10-28 00:00:00'), Timestamp('1989-10-29 00:00:00'), Timestamp('1989-11-04 00:00:00'), Timestamp('1989-11-05 00:00:00'), Timestamp('1989-11-11 00:00:00'), Timestamp('1989-11-12 00:00:00'), Timestamp('1989-11-18 00:00:00'), Timestamp('1989-11-19 00:00:00'), Timestamp('1989-11-25 00:00:00'), Timestamp('1989-11-26 00:00:00'), Timestamp('1989-12-02 00:00:00'), Timestamp('1989-12-03 00:00:00'), Timestamp('1989-12-09 00:00:00'), Timestamp('1989-12-10 00:00:00'), Timestamp('1989-12-16 00:00:00'), Timestamp('1989-12-17 00:00:00'), Timestamp('1989-12-23 00:00:00'), Timestamp('1989-12-24 00:00:00'), Timestamp('1989-12-30 00:00:00'), Timestamp('1989-12-31 00:00:00'), Timestamp('1990-01-06 00:00:00'), Timestamp('1990-01-07 00:00:00'), Timestamp('1990-01-13 00:00:00'), Timestamp('1990-01-14 00:00:00'), Timestamp('1990-01-20 00:00:00'), Timestamp('1990-01-21 00:00:00'), Timestamp('1990-01-27 00:00:00'), Timestamp('1990-01-28 00:00:00'), Timestamp('1990-02-03 00:00:00'), Timestamp('1990-02-04 00:00:00'), Timestamp('1990-02-10 00:00:00'), Timestamp('1990-02-11 00:00:00'), Timestamp('1990-02-17 00:00:00'), Timestamp('1990-02-18 00:00:00'), Timestamp('1990-02-24 00:00:00'), Timestamp('1990-02-25 00:00:00'), Timestamp('1990-03-03 00:00:00'), Timestamp('1990-03-04 00:00:00'), Timestamp('1990-03-10 00:00:00'), Timestamp('1990-03-11 00:00:00'), Timestamp('1990-03-17 00:00:00'), Timestamp('1990-03-18 00:00:00'), Timestamp('1990-03-24 00:00:00'), Timestamp('1990-03-25 00:00:00'), Timestamp('1990-03-31 00:00:00'), Timestamp('1990-04-01 00:00:00'), Timestamp('1990-04-07 00:00:00'), Timestamp('1990-04-08 00:00:00'), Timestamp('1990-04-14 00:00:00'), Timestamp('1990-04-15 00:00:00'), Timestamp('1990-04-21 00:00:00'), Timestamp('1990-04-22 00:00:00'), Timestamp('1990-04-28 00:00:00'), Timestamp('1990-04-29 00:00:00'), Timestamp('1990-05-05 00:00:00'), Timestamp('1990-05-06 00:00:00'), Timestamp('1990-05-12 00:00:00'), Timestamp('1990-05-13 00:00:00'), Timestamp('1990-05-19 00:00:00'), Timestamp('1990-05-20 00:00:00'), Timestamp('1990-05-26 00:00:00'), Timestamp('1990-05-27 00:00:00'), Timestamp('1990-06-02 00:00:00'), Timestamp('1990-06-03 00:00:00'), Timestamp('1990-06-09 00:00:00'), Timestamp('1990-06-10 00:00:00'), Timestamp('1990-06-16 00:00:00'), Timestamp('1990-06-17 00:00:00'), Timestamp('1990-06-23 00:00:00'), Timestamp('1990-06-24 00:00:00'), Timestamp('1990-06-30 00:00:00'), Timestamp('1990-07-01 00:00:00'), Timestamp('1990-07-07 00:00:00'), Timestamp('1990-07-08 00:00:00'), Timestamp('1990-07-14 00:00:00'), Timestamp('1990-07-15 00:00:00'), Timestamp('1990-07-21 00:00:00'), Timestamp('1990-07-22 00:00:00'), Timestamp('1990-07-28 00:00:00'), Timestamp('1990-07-29 00:00:00'), Timestamp('1990-08-04 00:00:00'), Timestamp('1990-08-05 00:00:00'), Timestamp('1990-08-11 00:00:00'), Timestamp('1990-08-12 00:00:00'), Timestamp('1990-08-18 00:00:00'), Timestamp('1990-08-19 00:00:00'), Timestamp('1990-08-25 00:00:00'), Timestamp('1990-08-26 00:00:00'), Timestamp('1990-09-01 00:00:00'), Timestamp('1990-09-02 00:00:00'), Timestamp('1990-09-08 00:00:00'), Timestamp('1990-09-09 00:00:00'), Timestamp('1990-09-15 00:00:00'), Timestamp('1990-09-16 00:00:00'), Timestamp('1990-09-22 00:00:00'), Timestamp('1990-09-23 00:00:00'), Timestamp('1990-09-29 00:00:00'), Timestamp('1990-09-30 00:00:00'), Timestamp('1990-10-06 00:00:00'), Timestamp('1990-10-07 00:00:00'), Timestamp('1990-10-13 00:00:00'), Timestamp('1990-10-14 00:00:00'), Timestamp('1990-10-20 00:00:00'), Timestamp('1990-10-21 00:00:00'), Timestamp('1990-10-27 00:00:00'), Timestamp('1990-10-28 00:00:00'), Timestamp('1990-11-03 00:00:00'), Timestamp('1990-11-04 00:00:00'), Timestamp('1990-11-10 00:00:00'), Timestamp('1990-11-11 00:00:00'), Timestamp('1990-11-17 00:00:00'), Timestamp('1990-11-18 00:00:00'), Timestamp('1990-11-24 00:00:00'), Timestamp('1990-11-25 00:00:00'), Timestamp('1990-12-01 00:00:00'), Timestamp('1990-12-02 00:00:00'), Timestamp('1990-12-08 00:00:00'), Timestamp('1990-12-09 00:00:00'), Timestamp('1990-12-15 00:00:00'), Timestamp('1990-12-16 00:00:00'), Timestamp('1990-12-22 00:00:00'), Timestamp('1990-12-23 00:00:00'), Timestamp('1990-12-29 00:00:00'), Timestamp('1990-12-30 00:00:00'), Timestamp('1991-01-05 00:00:00'), Timestamp('1991-01-06 00:00:00'), Timestamp('1991-01-12 00:00:00'), Timestamp('1991-01-13 00:00:00'), Timestamp('1991-01-19 00:00:00'), Timestamp('1991-01-20 00:00:00'), Timestamp('1991-01-26 00:00:00'), Timestamp('1991-01-27 00:00:00'), Timestamp('1991-02-02 00:00:00'), Timestamp('1991-02-03 00:00:00'), Timestamp('1991-02-09 00:00:00'), Timestamp('1991-02-10 00:00:00'), Timestamp('1991-02-16 00:00:00'), Timestamp('1991-02-17 00:00:00'), Timestamp('1991-02-23 00:00:00'), Timestamp('1991-02-24 00:00:00'), Timestamp('1991-03-02 00:00:00'), Timestamp('1991-03-03 00:00:00'), Timestamp('1991-03-09 00:00:00'), Timestamp('1991-03-10 00:00:00'), Timestamp('1991-03-16 00:00:00'), Timestamp('1991-03-17 00:00:00'), Timestamp('1991-03-23 00:00:00'), Timestamp('1991-03-24 00:00:00'), Timestamp('1991-03-30 00:00:00'), Timestamp('1991-03-31 00:00:00'), Timestamp('1991-04-06 00:00:00'), Timestamp('1991-04-07 00:00:00'), Timestamp('1991-04-13 00:00:00'), Timestamp('1991-04-14 00:00:00'), Timestamp('1991-04-20 00:00:00'), Timestamp('1991-04-21 00:00:00'), Timestamp('1991-04-27 00:00:00'), Timestamp('1991-04-28 00:00:00'), Timestamp('1991-05-04 00:00:00'), Timestamp('1991-05-05 00:00:00'), Timestamp('1991-05-11 00:00:00'), Timestamp('1991-05-12 00:00:00'), Timestamp('1991-05-18 00:00:00'), Timestamp('1991-05-19 00:00:00'), Timestamp('1991-05-25 00:00:00'), Timestamp('1991-05-26 00:00:00'), Timestamp('1991-06-01 00:00:00'), Timestamp('1991-06-02 00:00:00'), Timestamp('1991-06-08 00:00:00'), Timestamp('1991-06-09 00:00:00'), Timestamp('1991-06-15 00:00:00'), Timestamp('1991-06-16 00:00:00'), Timestamp('1991-06-22 00:00:00'), Timestamp('1991-06-23 00:00:00'), Timestamp('1991-06-29 00:00:00'), Timestamp('1991-06-30 00:00:00'), Timestamp('1991-07-06 00:00:00'), Timestamp('1991-07-07 00:00:00'), Timestamp('1991-07-13 00:00:00'), Timestamp('1991-07-14 00:00:00'), Timestamp('1991-07-20 00:00:00'), Timestamp('1991-07-21 00:00:00'), Timestamp('1991-07-27 00:00:00'), Timestamp('1991-07-28 00:00:00'), Timestamp('1991-08-03 00:00:00'), Timestamp('1991-08-04 00:00:00'), Timestamp('1991-08-10 00:00:00'), Timestamp('1991-08-11 00:00:00'), Timestamp('1991-08-17 00:00:00'), Timestamp('1991-08-18 00:00:00'), Timestamp('1991-08-24 00:00:00'), Timestamp('1991-08-25 00:00:00'), Timestamp('1991-08-31 00:00:00'), Timestamp('1991-09-01 00:00:00'), Timestamp('1991-09-07 00:00:00'), Timestamp('1991-09-08 00:00:00'), Timestamp('1991-09-14 00:00:00'), Timestamp('1991-09-15 00:00:00'), Timestamp('1991-09-21 00:00:00'), Timestamp('1991-09-22 00:00:00'), Timestamp('1991-09-28 00:00:00'), Timestamp('1991-09-29 00:00:00'), Timestamp('1991-10-05 00:00:00'), Timestamp('1991-10-06 00:00:00'), Timestamp('1991-10-12 00:00:00'), Timestamp('1991-10-13 00:00:00'), Timestamp('1991-10-19 00:00:00'), Timestamp('1991-10-20 00:00:00'), Timestamp('1991-10-26 00:00:00'), Timestamp('1991-10-27 00:00:00'), Timestamp('1991-11-02 00:00:00'), Timestamp('1991-11-03 00:00:00'), Timestamp('1991-11-09 00:00:00'), Timestamp('1991-11-10 00:00:00'), Timestamp('1991-11-16 00:00:00'), Timestamp('1991-11-17 00:00:00'), Timestamp('1991-11-23 00:00:00'), Timestamp('1991-11-24 00:00:00'), Timestamp('1991-11-30 00:00:00'), Timestamp('1991-12-01 00:00:00'), Timestamp('1991-12-07 00:00:00'), Timestamp('1991-12-08 00:00:00'), Timestamp('1991-12-14 00:00:00'), Timestamp('1991-12-15 00:00:00'), Timestamp('1991-12-21 00:00:00'), Timestamp('1991-12-22 00:00:00'), Timestamp('1991-12-28 00:00:00'), Timestamp('1991-12-29 00:00:00'), Timestamp('1992-01-04 00:00:00'), Timestamp('1992-01-05 00:00:00'), Timestamp('1992-01-11 00:00:00'), Timestamp('1992-01-12 00:00:00'), Timestamp('1992-01-18 00:00:00'), Timestamp('1992-01-19 00:00:00'), Timestamp('1992-01-25 00:00:00'), Timestamp('1992-01-26 00:00:00'), Timestamp('1992-02-01 00:00:00'), Timestamp('1992-02-02 00:00:00'), Timestamp('1992-02-08 00:00:00'), Timestamp('1992-02-09 00:00:00'), Timestamp('1992-02-15 00:00:00'), Timestamp('1992-02-16 00:00:00'), Timestamp('1992-02-22 00:00:00'), Timestamp('1992-02-23 00:00:00'), Timestamp('1992-02-29 00:00:00'), Timestamp('1992-03-01 00:00:00'), Timestamp('1992-03-07 00:00:00'), Timestamp('1992-03-08 00:00:00'), Timestamp('1992-03-14 00:00:00'), Timestamp('1992-03-15 00:00:00'), Timestamp('1992-03-21 00:00:00'), Timestamp('1992-03-22 00:00:00'), Timestamp('1992-03-28 00:00:00'), Timestamp('1992-03-29 00:00:00'), Timestamp('1992-04-04 00:00:00'), Timestamp('1992-04-05 00:00:00'), Timestamp('1992-04-11 00:00:00'), Timestamp('1992-04-12 00:00:00'), Timestamp('1992-04-18 00:00:00'), Timestamp('1992-04-19 00:00:00'), Timestamp('1992-04-25 00:00:00'), Timestamp('1992-04-26 00:00:00'), Timestamp('1992-05-02 00:00:00'), Timestamp('1992-05-03 00:00:00'), Timestamp('1992-05-09 00:00:00'), Timestamp('1992-05-10 00:00:00'), Timestamp('1992-05-16 00:00:00'), Timestamp('1992-05-17 00:00:00'), Timestamp('1992-05-23 00:00:00'), Timestamp('1992-05-24 00:00:00'), Timestamp('1992-05-30 00:00:00'), Timestamp('1992-05-31 00:00:00'), Timestamp('1992-06-06 00:00:00'), Timestamp('1992-06-07 00:00:00'), Timestamp('1992-06-13 00:00:00'), Timestamp('1992-06-14 00:00:00'), Timestamp('1992-06-20 00:00:00'), Timestamp('1992-06-21 00:00:00'), Timestamp('1992-06-27 00:00:00'), Timestamp('1992-06-28 00:00:00'), Timestamp('1992-07-04 00:00:00'), Timestamp('1992-07-05 00:00:00'), Timestamp('1992-07-11 00:00:00'), Timestamp('1992-07-12 00:00:00'), Timestamp('1992-07-18 00:00:00'), Timestamp('1992-07-19 00:00:00'), Timestamp('1992-07-25 00:00:00'), Timestamp('1992-07-26 00:00:00'), Timestamp('1992-08-01 00:00:00'), Timestamp('1992-08-02 00:00:00'), Timestamp('1992-08-08 00:00:00'), Timestamp('1992-08-09 00:00:00'), Timestamp('1992-08-15 00:00:00'), Timestamp('1992-08-16 00:00:00'), Timestamp('1992-08-22 00:00:00'), Timestamp('1992-08-23 00:00:00'), Timestamp('1992-08-29 00:00:00'), Timestamp('1992-08-30 00:00:00'), Timestamp('1992-09-05 00:00:00'), Timestamp('1992-09-06 00:00:00'), Timestamp('1992-09-12 00:00:00'), Timestamp('1992-09-13 00:00:00'), Timestamp('1992-09-19 00:00:00'), Timestamp('1992-09-20 00:00:00'), Timestamp('1992-09-26 00:00:00'), Timestamp('1992-09-27 00:00:00'), Timestamp('1992-10-03 00:00:00'), Timestamp('1992-10-04 00:00:00'), Timestamp('1992-10-10 00:00:00'), Timestamp('1992-10-11 00:00:00'), Timestamp('1992-10-17 00:00:00'), Timestamp('1992-10-18 00:00:00'), Timestamp('1992-10-24 00:00:00'), Timestamp('1992-10-25 00:00:00'), Timestamp('1992-10-31 00:00:00'), Timestamp('1992-11-01 00:00:00'), Timestamp('1992-11-07 00:00:00'), Timestamp('1992-11-08 00:00:00'), Timestamp('1992-11-14 00:00:00'), Timestamp('1992-11-15 00:00:00'), Timestamp('1992-11-21 00:00:00'), Timestamp('1992-11-22 00:00:00'), Timestamp('1992-11-28 00:00:00'), Timestamp('1992-11-29 00:00:00'), Timestamp('1992-12-05 00:00:00'), Timestamp('1992-12-06 00:00:00'), Timestamp('1992-12-12 00:00:00'), Timestamp('1992-12-13 00:00:00'), Timestamp('1992-12-19 00:00:00'), Timestamp('1992-12-20 00:00:00'), Timestamp('1992-12-26 00:00:00'), Timestamp('1992-12-27 00:00:00'), Timestamp('1993-01-02 00:00:00'), Timestamp('1993-01-03 00:00:00'), Timestamp('1993-01-09 00:00:00'), Timestamp('1993-01-10 00:00:00'), Timestamp('1993-01-16 00:00:00'), Timestamp('1993-01-17 00:00:00'), Timestamp('1993-01-23 00:00:00'), Timestamp('1993-01-24 00:00:00'), Timestamp('1993-01-30 00:00:00'), Timestamp('1993-01-31 00:00:00'), Timestamp('1993-02-06 00:00:00'), Timestamp('1993-02-07 00:00:00'), Timestamp('1993-02-13 00:00:00'), Timestamp('1993-02-14 00:00:00'), Timestamp('1993-02-20 00:00:00'), Timestamp('1993-02-21 00:00:00'), Timestamp('1993-02-27 00:00:00'), Timestamp('1993-02-28 00:00:00'), Timestamp('1993-03-06 00:00:00'), Timestamp('1993-03-07 00:00:00'), Timestamp('1993-03-13 00:00:00'), Timestamp('1993-03-14 00:00:00'), Timestamp('1993-03-20 00:00:00'), Timestamp('1993-03-21 00:00:00'), Timestamp('1993-03-27 00:00:00'), Timestamp('1993-03-28 00:00:00'), Timestamp('1993-04-03 00:00:00'), Timestamp('1993-04-04 00:00:00'), Timestamp('1993-04-10 00:00:00'), Timestamp('1993-04-11 00:00:00'), Timestamp('1993-04-17 00:00:00'), Timestamp('1993-04-18 00:00:00'), Timestamp('1993-04-24 00:00:00'), Timestamp('1993-04-25 00:00:00'), Timestamp('1993-05-01 00:00:00'), Timestamp('1993-05-02 00:00:00'), Timestamp('1993-05-08 00:00:00'), Timestamp('1993-05-09 00:00:00'), Timestamp('1993-05-15 00:00:00'), Timestamp('1993-05-16 00:00:00'), Timestamp('1993-05-22 00:00:00'), Timestamp('1993-05-23 00:00:00'), Timestamp('1993-05-29 00:00:00'), Timestamp('1993-05-30 00:00:00'), Timestamp('1993-06-05 00:00:00'), Timestamp('1993-06-06 00:00:00'), Timestamp('1993-06-12 00:00:00'), Timestamp('1993-06-13 00:00:00'), Timestamp('1993-06-19 00:00:00'), Timestamp('1993-06-20 00:00:00'), Timestamp('1993-06-26 00:00:00'), Timestamp('1993-06-27 00:00:00'), Timestamp('1993-07-03 00:00:00'), Timestamp('1993-07-04 00:00:00'), Timestamp('1993-07-10 00:00:00'), Timestamp('1993-07-11 00:00:00'), Timestamp('1993-07-17 00:00:00'), Timestamp('1993-07-18 00:00:00'), Timestamp('1993-07-24 00:00:00'), Timestamp('1993-07-25 00:00:00'), Timestamp('1993-07-31 00:00:00'), Timestamp('1993-08-01 00:00:00'), Timestamp('1993-08-07 00:00:00'), Timestamp('1993-08-08 00:00:00'), Timestamp('1993-08-14 00:00:00'), Timestamp('1993-08-15 00:00:00'), Timestamp('1993-08-21 00:00:00'), Timestamp('1993-08-22 00:00:00'), Timestamp('1993-08-28 00:00:00'), Timestamp('1993-08-29 00:00:00'), Timestamp('1993-09-04 00:00:00'), Timestamp('1993-09-05 00:00:00'), Timestamp('1993-09-11 00:00:00'), Timestamp('1993-09-12 00:00:00'), Timestamp('1993-09-18 00:00:00'), Timestamp('1993-09-19 00:00:00'), Timestamp('1993-09-25 00:00:00'), Timestamp('1993-09-26 00:00:00'), Timestamp('1993-10-02 00:00:00'), Timestamp('1993-10-03 00:00:00'), Timestamp('1993-10-09 00:00:00'), Timestamp('1993-10-10 00:00:00'), Timestamp('1993-10-16 00:00:00'), Timestamp('1993-10-17 00:00:00'), Timestamp('1993-10-23 00:00:00'), Timestamp('1993-10-24 00:00:00'), Timestamp('1993-10-30 00:00:00'), Timestamp('1993-10-31 00:00:00'), Timestamp('1993-11-06 00:00:00'), Timestamp('1993-11-07 00:00:00'), Timestamp('1993-11-13 00:00:00'), Timestamp('1993-11-14 00:00:00'), Timestamp('1993-11-20 00:00:00'), Timestamp('1993-11-21 00:00:00'), Timestamp('1993-11-27 00:00:00'), Timestamp('1993-11-28 00:00:00'), Timestamp('1993-12-04 00:00:00'), Timestamp('1993-12-05 00:00:00'), Timestamp('1993-12-11 00:00:00'), Timestamp('1993-12-12 00:00:00'), Timestamp('1993-12-18 00:00:00'), Timestamp('1993-12-19 00:00:00'), Timestamp('1993-12-25 00:00:00'), Timestamp('1993-12-26 00:00:00'), Timestamp('1994-01-01 00:00:00'), Timestamp('1994-01-02 00:00:00'), Timestamp('1994-01-08 00:00:00'), Timestamp('1994-01-09 00:00:00'), Timestamp('1994-01-15 00:00:00'), Timestamp('1994-01-16 00:00:00'), Timestamp('1994-01-22 00:00:00'), Timestamp('1994-01-23 00:00:00'), Timestamp('1994-01-29 00:00:00'), Timestamp('1994-01-30 00:00:00'), Timestamp('1994-02-05 00:00:00'), Timestamp('1994-02-06 00:00:00'), Timestamp('1994-02-12 00:00:00'), Timestamp('1994-02-13 00:00:00'), Timestamp('1994-02-19 00:00:00'), Timestamp('1994-02-20 00:00:00'), Timestamp('1994-02-26 00:00:00'), Timestamp('1994-02-27 00:00:00'), Timestamp('1994-03-05 00:00:00'), Timestamp('1994-03-06 00:00:00'), Timestamp('1994-03-12 00:00:00'), Timestamp('1994-03-13 00:00:00'), Timestamp('1994-03-19 00:00:00'), Timestamp('1994-03-20 00:00:00'), Timestamp('1994-03-26 00:00:00'), Timestamp('1994-03-27 00:00:00'), Timestamp('1994-04-02 00:00:00'), Timestamp('1994-04-03 00:00:00'), Timestamp('1994-04-09 00:00:00'), Timestamp('1994-04-10 00:00:00'), Timestamp('1994-04-16 00:00:00'), Timestamp('1994-04-17 00:00:00'), Timestamp('1994-04-23 00:00:00'), Timestamp('1994-04-24 00:00:00'), Timestamp('1994-04-30 00:00:00'), Timestamp('1994-05-01 00:00:00'), Timestamp('1994-05-07 00:00:00'), Timestamp('1994-05-08 00:00:00'), Timestamp('1994-05-14 00:00:00'), Timestamp('1994-05-15 00:00:00'), Timestamp('1994-05-21 00:00:00'), Timestamp('1994-05-22 00:00:00'), Timestamp('1994-05-28 00:00:00'), Timestamp('1994-05-29 00:00:00'), Timestamp('1994-06-04 00:00:00'), Timestamp('1994-06-05 00:00:00'), Timestamp('1994-06-11 00:00:00'), Timestamp('1994-06-12 00:00:00'), Timestamp('1994-06-18 00:00:00'), Timestamp('1994-06-19 00:00:00'), Timestamp('1994-06-25 00:00:00'), Timestamp('1994-06-26 00:00:00'), Timestamp('1994-07-02 00:00:00'), Timestamp('1994-07-03 00:00:00'), Timestamp('1994-07-09 00:00:00'), Timestamp('1994-07-10 00:00:00'), Timestamp('1994-07-16 00:00:00'), Timestamp('1994-07-17 00:00:00'), Timestamp('1994-07-23 00:00:00'), Timestamp('1994-07-24 00:00:00'), Timestamp('1994-07-30 00:00:00'), Timestamp('1994-07-31 00:00:00'), Timestamp('1994-08-06 00:00:00'), Timestamp('1994-08-07 00:00:00'), Timestamp('1994-08-13 00:00:00'), Timestamp('1994-08-14 00:00:00'), Timestamp('1994-08-20 00:00:00'), Timestamp('1994-08-21 00:00:00'), Timestamp('1994-08-27 00:00:00'), Timestamp('1994-08-28 00:00:00'), Timestamp('1994-09-03 00:00:00'), Timestamp('1994-09-04 00:00:00'), Timestamp('1994-09-10 00:00:00'), Timestamp('1994-09-11 00:00:00'), Timestamp('1994-09-17 00:00:00'), Timestamp('1994-09-18 00:00:00'), Timestamp('1994-09-24 00:00:00'), Timestamp('1994-09-25 00:00:00'), Timestamp('1994-10-01 00:00:00'), Timestamp('1994-10-02 00:00:00'), Timestamp('1994-10-08 00:00:00'), Timestamp('1994-10-09 00:00:00'), Timestamp('1994-10-15 00:00:00'), Timestamp('1994-10-16 00:00:00'), Timestamp('1994-10-22 00:00:00'), Timestamp('1994-10-23 00:00:00'), Timestamp('1994-10-29 00:00:00'), Timestamp('1994-10-30 00:00:00'), Timestamp('1994-11-05 00:00:00'), Timestamp('1994-11-06 00:00:00'), Timestamp('1994-11-12 00:00:00'), Timestamp('1994-11-13 00:00:00'), Timestamp('1994-11-19 00:00:00'), Timestamp('1994-11-20 00:00:00'), Timestamp('1994-11-26 00:00:00'), Timestamp('1994-11-27 00:00:00'), Timestamp('1994-12-03 00:00:00'), Timestamp('1994-12-04 00:00:00'), Timestamp('1994-12-10 00:00:00'), Timestamp('1994-12-11 00:00:00'), Timestamp('1994-12-17 00:00:00'), Timestamp('1994-12-18 00:00:00'), Timestamp('1994-12-24 00:00:00'), Timestamp('1994-12-25 00:00:00'), Timestamp('1994-12-31 00:00:00'), Timestamp('1995-01-01 00:00:00'), Timestamp('1995-01-07 00:00:00'), Timestamp('1995-01-08 00:00:00'), Timestamp('1995-01-14 00:00:00'), Timestamp('1995-01-15 00:00:00'), Timestamp('1995-01-21 00:00:00'), Timestamp('1995-01-22 00:00:00'), Timestamp('1995-01-28 00:00:00'), Timestamp('1995-01-29 00:00:00'), Timestamp('1995-02-04 00:00:00'), Timestamp('1995-02-05 00:00:00'), Timestamp('1995-02-11 00:00:00'), Timestamp('1995-02-12 00:00:00'), Timestamp('1995-02-18 00:00:00'), Timestamp('1995-02-19 00:00:00'), Timestamp('1995-02-25 00:00:00'), Timestamp('1995-02-26 00:00:00'), Timestamp('1995-03-04 00:00:00'), Timestamp('1995-03-05 00:00:00'), Timestamp('1995-03-11 00:00:00'), Timestamp('1995-03-12 00:00:00'), Timestamp('1995-03-18 00:00:00'), Timestamp('1995-03-19 00:00:00'), Timestamp('1995-03-25 00:00:00'), Timestamp('1995-03-26 00:00:00'), Timestamp('1995-04-01 00:00:00'), Timestamp('1995-04-02 00:00:00'), Timestamp('1995-04-08 00:00:00'), Timestamp('1995-04-09 00:00:00'), Timestamp('1995-04-15 00:00:00'), Timestamp('1995-04-16 00:00:00'), Timestamp('1995-04-22 00:00:00'), Timestamp('1995-04-23 00:00:00'), Timestamp('1995-04-29 00:00:00'), Timestamp('1995-04-30 00:00:00'), Timestamp('1995-05-06 00:00:00'), Timestamp('1995-05-07 00:00:00'), Timestamp('1995-05-13 00:00:00'), Timestamp('1995-05-14 00:00:00'), Timestamp('1995-05-20 00:00:00'), Timestamp('1995-05-21 00:00:00'), Timestamp('1995-05-27 00:00:00'), Timestamp('1995-05-28 00:00:00'), Timestamp('1995-06-03 00:00:00'), Timestamp('1995-06-04 00:00:00'), Timestamp('1995-06-10 00:00:00'), Timestamp('1995-06-11 00:00:00'), Timestamp('1995-06-17 00:00:00'), Timestamp('1995-06-18 00:00:00'), Timestamp('1995-06-24 00:00:00'), Timestamp('1995-06-25 00:00:00'), Timestamp('1995-07-01 00:00:00'), Timestamp('1995-07-02 00:00:00'), Timestamp('1995-07-08 00:00:00'), Timestamp('1995-07-09 00:00:00'), Timestamp('1995-07-15 00:00:00'), Timestamp('1995-07-16 00:00:00'), Timestamp('1995-07-22 00:00:00'), Timestamp('1995-07-23 00:00:00'), Timestamp('1995-07-29 00:00:00'), Timestamp('1995-07-30 00:00:00'), Timestamp('1995-08-05 00:00:00'), Timestamp('1995-08-06 00:00:00'), Timestamp('1995-08-12 00:00:00'), Timestamp('1995-08-13 00:00:00'), Timestamp('1995-08-19 00:00:00'), Timestamp('1995-08-20 00:00:00'), Timestamp('1995-08-26 00:00:00'), Timestamp('1995-08-27 00:00:00'), Timestamp('1995-09-02 00:00:00'), Timestamp('1995-09-03 00:00:00'), Timestamp('1995-09-09 00:00:00'), Timestamp('1995-09-10 00:00:00'), Timestamp('1995-09-16 00:00:00'), Timestamp('1995-09-17 00:00:00'), Timestamp('1995-09-23 00:00:00'), Timestamp('1995-09-24 00:00:00'), Timestamp('1995-09-30 00:00:00'), Timestamp('1995-10-01 00:00:00'), Timestamp('1995-10-07 00:00:00'), Timestamp('1995-10-08 00:00:00'), Timestamp('1995-10-14 00:00:00'), Timestamp('1995-10-15 00:00:00'), Timestamp('1995-10-21 00:00:00'), Timestamp('1995-10-22 00:00:00'), Timestamp('1995-10-28 00:00:00'), Timestamp('1995-10-29 00:00:00'), Timestamp('1995-11-04 00:00:00'), Timestamp('1995-11-05 00:00:00'), Timestamp('1995-11-11 00:00:00'), Timestamp('1995-11-12 00:00:00'), Timestamp('1995-11-18 00:00:00'), Timestamp('1995-11-19 00:00:00'), Timestamp('1995-11-25 00:00:00'), Timestamp('1995-11-26 00:00:00'), Timestamp('1995-12-02 00:00:00'), Timestamp('1995-12-03 00:00:00'), Timestamp('1995-12-09 00:00:00'), Timestamp('1995-12-10 00:00:00'), Timestamp('1995-12-16 00:00:00'), Timestamp('1995-12-17 00:00:00'), Timestamp('1995-12-23 00:00:00'), Timestamp('1995-12-24 00:00:00'), Timestamp('1995-12-30 00:00:00'), Timestamp('1995-12-31 00:00:00'), Timestamp('1996-01-06 00:00:00'), Timestamp('1996-01-07 00:00:00'), Timestamp('1996-01-13 00:00:00'), Timestamp('1996-01-14 00:00:00'), Timestamp('1996-01-20 00:00:00'), Timestamp('1996-01-21 00:00:00'), Timestamp('1996-01-27 00:00:00'), Timestamp('1996-01-28 00:00:00'), Timestamp('1996-02-03 00:00:00'), Timestamp('1996-02-04 00:00:00'), Timestamp('1996-02-10 00:00:00'), Timestamp('1996-02-11 00:00:00'), Timestamp('1996-02-17 00:00:00'), Timestamp('1996-02-18 00:00:00'), Timestamp('1996-02-24 00:00:00'), Timestamp('1996-02-25 00:00:00'), Timestamp('1996-03-02 00:00:00'), Timestamp('1996-03-03 00:00:00'), Timestamp('1996-03-09 00:00:00'), Timestamp('1996-03-10 00:00:00'), Timestamp('1996-03-16 00:00:00'), Timestamp('1996-03-17 00:00:00'), Timestamp('1996-03-23 00:00:00'), Timestamp('1996-03-24 00:00:00'), Timestamp('1996-03-30 00:00:00'), Timestamp('1996-03-31 00:00:00'), Timestamp('1996-04-06 00:00:00'), Timestamp('1996-04-07 00:00:00'), Timestamp('1996-04-13 00:00:00'), Timestamp('1996-04-14 00:00:00'), Timestamp('1996-04-20 00:00:00'), Timestamp('1996-04-21 00:00:00'), Timestamp('1996-04-27 00:00:00'), Timestamp('1996-04-28 00:00:00'), Timestamp('1996-05-04 00:00:00'), Timestamp('1996-05-05 00:00:00'), Timestamp('1996-05-11 00:00:00'), Timestamp('1996-05-12 00:00:00'), Timestamp('1996-05-18 00:00:00'), Timestamp('1996-05-19 00:00:00'), Timestamp('1996-05-25 00:00:00'), Timestamp('1996-05-26 00:00:00'), Timestamp('1996-06-01 00:00:00'), Timestamp('1996-06-02 00:00:00'), Timestamp('1996-06-08 00:00:00'), Timestamp('1996-06-09 00:00:00'), Timestamp('1996-06-15 00:00:00'), Timestamp('1996-06-16 00:00:00'), Timestamp('1996-06-22 00:00:00'), Timestamp('1996-06-23 00:00:00'), Timestamp('1996-06-29 00:00:00'), Timestamp('1996-06-30 00:00:00'), Timestamp('1996-07-06 00:00:00'), Timestamp('1996-07-07 00:00:00'), Timestamp('1996-07-13 00:00:00'), Timestamp('1996-07-14 00:00:00'), Timestamp('1996-07-20 00:00:00'), Timestamp('1996-07-21 00:00:00'), Timestamp('1996-07-27 00:00:00'), Timestamp('1996-07-28 00:00:00'), Timestamp('1996-08-03 00:00:00'), Timestamp('1996-08-04 00:00:00'), Timestamp('1996-08-10 00:00:00'), Timestamp('1996-08-11 00:00:00'), Timestamp('1996-08-17 00:00:00'), Timestamp('1996-08-18 00:00:00'), Timestamp('1996-08-24 00:00:00'), Timestamp('1996-08-25 00:00:00'), Timestamp('1996-08-31 00:00:00'), Timestamp('1996-09-01 00:00:00'), Timestamp('1996-09-07 00:00:00'), Timestamp('1996-09-08 00:00:00'), Timestamp('1996-09-14 00:00:00'), Timestamp('1996-09-15 00:00:00'), Timestamp('1996-09-21 00:00:00'), Timestamp('1996-09-22 00:00:00'), Timestamp('1996-09-28 00:00:00'), Timestamp('1996-09-29 00:00:00'), Timestamp('1996-10-05 00:00:00'), Timestamp('1996-10-06 00:00:00'), Timestamp('1996-10-12 00:00:00'), Timestamp('1996-10-13 00:00:00'), Timestamp('1996-10-19 00:00:00'), Timestamp('1996-10-20 00:00:00'), Timestamp('1996-10-26 00:00:00'), Timestamp('1996-10-27 00:00:00'), Timestamp('1996-11-02 00:00:00'), Timestamp('1996-11-03 00:00:00'), Timestamp('1996-11-09 00:00:00'), Timestamp('1996-11-10 00:00:00'), Timestamp('1996-11-16 00:00:00'), Timestamp('1996-11-17 00:00:00'), Timestamp('1996-11-23 00:00:00'), Timestamp('1996-11-24 00:00:00'), Timestamp('1996-11-30 00:00:00'), Timestamp('1996-12-01 00:00:00'), Timestamp('1996-12-07 00:00:00'), Timestamp('1996-12-08 00:00:00'), Timestamp('1996-12-14 00:00:00'), Timestamp('1996-12-15 00:00:00'), Timestamp('1996-12-21 00:00:00'), Timestamp('1996-12-22 00:00:00'), Timestamp('1996-12-28 00:00:00'), Timestamp('1996-12-29 00:00:00'), Timestamp('1997-01-04 00:00:00'), Timestamp('1997-01-05 00:00:00'), Timestamp('1997-01-11 00:00:00'), Timestamp('1997-01-12 00:00:00'), Timestamp('1997-01-18 00:00:00'), Timestamp('1997-01-19 00:00:00'), Timestamp('1997-01-25 00:00:00'), Timestamp('1997-01-26 00:00:00'), Timestamp('1997-02-01 00:00:00'), Timestamp('1997-02-02 00:00:00'), Timestamp('1997-02-08 00:00:00'), Timestamp('1997-02-09 00:00:00'), Timestamp('1997-02-15 00:00:00'), Timestamp('1997-02-16 00:00:00'), Timestamp('1997-02-22 00:00:00'), Timestamp('1997-02-23 00:00:00'), Timestamp('1997-03-01 00:00:00'), Timestamp('1997-03-02 00:00:00'), Timestamp('1997-03-08 00:00:00'), Timestamp('1997-03-09 00:00:00'), Timestamp('1997-03-15 00:00:00'), Timestamp('1997-03-16 00:00:00'), Timestamp('1997-03-22 00:00:00'), Timestamp('1997-03-23 00:00:00'), Timestamp('1997-03-29 00:00:00'), Timestamp('1997-03-30 00:00:00'), Timestamp('1997-04-05 00:00:00'), Timestamp('1997-04-06 00:00:00'), Timestamp('1997-04-12 00:00:00'), Timestamp('1997-04-13 00:00:00'), Timestamp('1997-04-19 00:00:00'), Timestamp('1997-04-20 00:00:00'), Timestamp('1997-04-26 00:00:00'), Timestamp('1997-04-27 00:00:00'), Timestamp('1997-05-03 00:00:00'), Timestamp('1997-05-04 00:00:00'), Timestamp('1997-05-10 00:00:00'), Timestamp('1997-05-11 00:00:00'), Timestamp('1997-05-17 00:00:00'), Timestamp('1997-05-18 00:00:00'), Timestamp('1997-05-24 00:00:00'), Timestamp('1997-05-25 00:00:00'), Timestamp('1997-05-31 00:00:00'), Timestamp('1997-06-01 00:00:00'), Timestamp('1997-06-07 00:00:00'), Timestamp('1997-06-08 00:00:00'), Timestamp('1997-06-14 00:00:00'), Timestamp('1997-06-15 00:00:00'), Timestamp('1997-06-21 00:00:00'), Timestamp('1997-06-22 00:00:00'), Timestamp('1997-06-28 00:00:00'), Timestamp('1997-06-29 00:00:00'), Timestamp('1997-07-05 00:00:00'), Timestamp('1997-07-06 00:00:00'), Timestamp('1997-07-12 00:00:00'), Timestamp('1997-07-13 00:00:00'), Timestamp('1997-07-19 00:00:00'), Timestamp('1997-07-20 00:00:00'), Timestamp('1997-07-26 00:00:00'), Timestamp('1997-07-27 00:00:00'), Timestamp('1997-08-02 00:00:00'), Timestamp('1997-08-03 00:00:00'), Timestamp('1997-08-09 00:00:00'), Timestamp('1997-08-10 00:00:00'), Timestamp('1997-08-16 00:00:00'), Timestamp('1997-08-17 00:00:00'), Timestamp('1997-08-23 00:00:00'), Timestamp('1997-08-24 00:00:00'), Timestamp('1997-08-30 00:00:00'), Timestamp('1997-08-31 00:00:00'), Timestamp('1997-09-06 00:00:00'), Timestamp('1997-09-07 00:00:00'), Timestamp('1997-09-13 00:00:00'), Timestamp('1997-09-14 00:00:00'), Timestamp('1997-09-20 00:00:00'), Timestamp('1997-09-21 00:00:00'), Timestamp('1997-09-27 00:00:00'), Timestamp('1997-09-28 00:00:00'), Timestamp('1997-10-04 00:00:00'), Timestamp('1997-10-05 00:00:00'), Timestamp('1997-10-11 00:00:00'), Timestamp('1997-10-12 00:00:00'), Timestamp('1997-10-18 00:00:00'), Timestamp('1997-10-19 00:00:00'), Timestamp('1997-10-25 00:00:00'), Timestamp('1997-10-26 00:00:00'), Timestamp('1997-11-01 00:00:00'), Timestamp('1997-11-02 00:00:00'), Timestamp('1997-11-08 00:00:00'), Timestamp('1997-11-09 00:00:00'), Timestamp('1997-11-15 00:00:00'), Timestamp('1997-11-16 00:00:00'), Timestamp('1997-11-22 00:00:00'), Timestamp('1997-11-23 00:00:00'), Timestamp('1997-11-29 00:00:00'), Timestamp('1997-11-30 00:00:00'), Timestamp('1997-12-06 00:00:00'), Timestamp('1997-12-07 00:00:00'), Timestamp('1997-12-13 00:00:00'), Timestamp('1997-12-14 00:00:00'), Timestamp('1997-12-20 00:00:00'), Timestamp('1997-12-21 00:00:00'), Timestamp('1997-12-27 00:00:00'), Timestamp('1997-12-28 00:00:00'), Timestamp('1998-01-03 00:00:00'), Timestamp('1998-01-04 00:00:00'), Timestamp('1998-01-10 00:00:00'), Timestamp('1998-01-11 00:00:00'), Timestamp('1998-01-17 00:00:00'), Timestamp('1998-01-18 00:00:00'), Timestamp('1998-01-24 00:00:00'), Timestamp('1998-01-25 00:00:00'), Timestamp('1998-01-31 00:00:00'), Timestamp('1998-02-01 00:00:00'), Timestamp('1998-02-07 00:00:00'), Timestamp('1998-02-08 00:00:00'), Timestamp('1998-02-14 00:00:00'), Timestamp('1998-02-15 00:00:00'), Timestamp('1998-02-21 00:00:00'), Timestamp('1998-02-22 00:00:00'), Timestamp('1998-02-28 00:00:00'), Timestamp('1998-03-01 00:00:00'), Timestamp('1998-03-07 00:00:00'), Timestamp('1998-03-08 00:00:00'), Timestamp('1998-03-14 00:00:00'), Timestamp('1998-03-15 00:00:00'), Timestamp('1998-03-21 00:00:00'), Timestamp('1998-03-22 00:00:00'), Timestamp('1998-03-28 00:00:00'), Timestamp('1998-03-29 00:00:00'), Timestamp('1998-04-04 00:00:00'), Timestamp('1998-04-05 00:00:00'), Timestamp('1998-04-11 00:00:00'), Timestamp('1998-04-12 00:00:00'), Timestamp('1998-04-18 00:00:00'), Timestamp('1998-04-19 00:00:00'), Timestamp('1998-04-25 00:00:00'), Timestamp('1998-04-26 00:00:00'), Timestamp('1998-05-02 00:00:00'), Timestamp('1998-05-03 00:00:00'), Timestamp('1998-05-09 00:00:00'), Timestamp('1998-05-10 00:00:00'), Timestamp('1998-05-16 00:00:00'), Timestamp('1998-05-17 00:00:00'), Timestamp('1998-05-23 00:00:00'), Timestamp('1998-05-24 00:00:00'), Timestamp('1998-05-30 00:00:00'), Timestamp('1998-05-31 00:00:00'), Timestamp('1998-06-06 00:00:00'), Timestamp('1998-06-07 00:00:00'), Timestamp('1998-06-13 00:00:00'), Timestamp('1998-06-14 00:00:00'), Timestamp('1998-06-20 00:00:00'), Timestamp('1998-06-21 00:00:00'), Timestamp('1998-06-27 00:00:00'), Timestamp('1998-06-28 00:00:00'), Timestamp('1998-07-04 00:00:00'), Timestamp('1998-07-05 00:00:00'), Timestamp('1998-07-11 00:00:00'), Timestamp('1998-07-12 00:00:00'), Timestamp('1998-07-18 00:00:00'), Timestamp('1998-07-19 00:00:00'), Timestamp('1998-07-25 00:00:00'), Timestamp('1998-07-26 00:00:00'), Timestamp('1998-08-01 00:00:00'), Timestamp('1998-08-02 00:00:00'), Timestamp('1998-08-08 00:00:00'), Timestamp('1998-08-09 00:00:00'), Timestamp('1998-08-15 00:00:00'), Timestamp('1998-08-16 00:00:00'), Timestamp('1998-08-22 00:00:00'), Timestamp('1998-08-23 00:00:00'), Timestamp('1998-08-29 00:00:00'), Timestamp('1998-08-30 00:00:00'), Timestamp('1998-09-05 00:00:00'), Timestamp('1998-09-06 00:00:00'), Timestamp('1998-09-12 00:00:00'), Timestamp('1998-09-13 00:00:00'), Timestamp('1998-09-19 00:00:00'), Timestamp('1998-09-20 00:00:00'), Timestamp('1998-09-26 00:00:00'), Timestamp('1998-09-27 00:00:00'), Timestamp('1998-10-03 00:00:00'), Timestamp('1998-10-04 00:00:00'), Timestamp('1998-10-10 00:00:00'), Timestamp('1998-10-11 00:00:00'), Timestamp('1998-10-17 00:00:00'), Timestamp('1998-10-18 00:00:00'), Timestamp('1998-10-24 00:00:00'), Timestamp('1998-10-25 00:00:00'), Timestamp('1998-10-31 00:00:00'), Timestamp('1998-11-01 00:00:00'), Timestamp('1998-11-07 00:00:00'), Timestamp('1998-11-08 00:00:00'), Timestamp('1998-11-14 00:00:00'), Timestamp('1998-11-15 00:00:00'), Timestamp('1998-11-21 00:00:00'), Timestamp('1998-11-22 00:00:00'), Timestamp('1998-11-28 00:00:00'), Timestamp('1998-11-29 00:00:00'), Timestamp('1998-12-05 00:00:00'), Timestamp('1998-12-06 00:00:00'), Timestamp('1998-12-12 00:00:00'), Timestamp('1998-12-13 00:00:00'), Timestamp('1998-12-19 00:00:00'), Timestamp('1998-12-20 00:00:00'), Timestamp('1998-12-26 00:00:00'), Timestamp('1998-12-27 00:00:00'), Timestamp('1999-01-02 00:00:00'), Timestamp('1999-01-03 00:00:00'), Timestamp('1999-01-09 00:00:00'), Timestamp('1999-01-10 00:00:00'), Timestamp('1999-01-16 00:00:00'), Timestamp('1999-01-17 00:00:00'), Timestamp('1999-01-23 00:00:00'), Timestamp('1999-01-24 00:00:00'), Timestamp('1999-01-30 00:00:00'), Timestamp('1999-01-31 00:00:00'), Timestamp('1999-02-06 00:00:00'), Timestamp('1999-02-07 00:00:00'), Timestamp('1999-02-13 00:00:00'), Timestamp('1999-02-14 00:00:00'), Timestamp('1999-02-20 00:00:00'), Timestamp('1999-02-21 00:00:00'), Timestamp('1999-02-27 00:00:00'), Timestamp('1999-02-28 00:00:00'), Timestamp('1999-03-06 00:00:00'), Timestamp('1999-03-07 00:00:00'), Timestamp('1999-03-13 00:00:00'), Timestamp('1999-03-14 00:00:00'), Timestamp('1999-03-20 00:00:00'), Timestamp('1999-03-21 00:00:00'), Timestamp('1999-03-27 00:00:00'), Timestamp('1999-03-28 00:00:00'), Timestamp('1999-04-03 00:00:00'), Timestamp('1999-04-04 00:00:00'), Timestamp('1999-04-10 00:00:00'), Timestamp('1999-04-11 00:00:00'), Timestamp('1999-04-17 00:00:00'), Timestamp('1999-04-18 00:00:00'), Timestamp('1999-04-24 00:00:00'), Timestamp('1999-04-25 00:00:00'), Timestamp('1999-05-01 00:00:00'), Timestamp('1999-05-02 00:00:00'), Timestamp('1999-05-08 00:00:00'), Timestamp('1999-05-09 00:00:00'), Timestamp('1999-05-15 00:00:00'), Timestamp('1999-05-16 00:00:00'), Timestamp('1999-05-22 00:00:00'), Timestamp('1999-05-23 00:00:00'), Timestamp('1999-05-29 00:00:00'), Timestamp('1999-05-30 00:00:00'), Timestamp('1999-06-05 00:00:00'), Timestamp('1999-06-06 00:00:00'), Timestamp('1999-06-12 00:00:00'), Timestamp('1999-06-13 00:00:00'), Timestamp('1999-06-19 00:00:00'), Timestamp('1999-06-20 00:00:00'), Timestamp('1999-06-26 00:00:00'), Timestamp('1999-06-27 00:00:00'), Timestamp('1999-07-03 00:00:00'), Timestamp('1999-07-04 00:00:00'), Timestamp('1999-07-10 00:00:00'), Timestamp('1999-07-11 00:00:00'), Timestamp('1999-07-17 00:00:00'), Timestamp('1999-07-18 00:00:00'), Timestamp('1999-07-24 00:00:00'), Timestamp('1999-07-25 00:00:00'), Timestamp('1999-07-31 00:00:00'), Timestamp('1999-08-01 00:00:00'), Timestamp('1999-08-07 00:00:00'), Timestamp('1999-08-08 00:00:00'), Timestamp('1999-08-14 00:00:00'), Timestamp('1999-08-15 00:00:00'), Timestamp('1999-08-21 00:00:00'), Timestamp('1999-08-22 00:00:00'), Timestamp('1999-08-28 00:00:00'), Timestamp('1999-08-29 00:00:00'), Timestamp('1999-09-04 00:00:00'), Timestamp('1999-09-05 00:00:00'), Timestamp('1999-09-11 00:00:00'), Timestamp('1999-09-12 00:00:00'), Timestamp('1999-09-18 00:00:00'), Timestamp('1999-09-19 00:00:00'), Timestamp('1999-09-25 00:00:00'), Timestamp('1999-09-26 00:00:00'), Timestamp('1999-10-02 00:00:00'), Timestamp('1999-10-03 00:00:00'), Timestamp('1999-10-09 00:00:00'), Timestamp('1999-10-10 00:00:00'), Timestamp('1999-10-16 00:00:00'), Timestamp('1999-10-17 00:00:00'), Timestamp('1999-10-23 00:00:00'), Timestamp('1999-10-24 00:00:00'), Timestamp('1999-10-30 00:00:00'), Timestamp('1999-10-31 00:00:00'), Timestamp('1999-11-06 00:00:00'), Timestamp('1999-11-07 00:00:00'), Timestamp('1999-11-13 00:00:00'), Timestamp('1999-11-14 00:00:00'), Timestamp('1999-11-20 00:00:00'), Timestamp('1999-11-21 00:00:00'), Timestamp('1999-11-27 00:00:00'), Timestamp('1999-11-28 00:00:00'), Timestamp('1999-12-04 00:00:00'), Timestamp('1999-12-05 00:00:00'), Timestamp('1999-12-11 00:00:00'), Timestamp('1999-12-12 00:00:00'), Timestamp('1999-12-18 00:00:00'), Timestamp('1999-12-19 00:00:00'), Timestamp('1999-12-25 00:00:00'), Timestamp('1999-12-26 00:00:00'), Timestamp('2000-01-01 00:00:00'), Timestamp('2000-01-02 00:00:00'), Timestamp('2000-01-08 00:00:00'), Timestamp('2000-01-09 00:00:00'), Timestamp('2000-01-15 00:00:00'), Timestamp('2000-01-16 00:00:00'), Timestamp('2000-01-22 00:00:00'), Timestamp('2000-01-23 00:00:00'), Timestamp('2000-01-29 00:00:00'), Timestamp('2000-01-30 00:00:00'), Timestamp('2000-02-05 00:00:00'), Timestamp('2000-02-06 00:00:00'), Timestamp('2000-02-12 00:00:00'), Timestamp('2000-02-13 00:00:00'), Timestamp('2000-02-19 00:00:00'), Timestamp('2000-02-20 00:00:00'), Timestamp('2000-02-26 00:00:00'), Timestamp('2000-02-27 00:00:00'), Timestamp('2000-03-04 00:00:00'), Timestamp('2000-03-05 00:00:00'), Timestamp('2000-03-11 00:00:00'), Timestamp('2000-03-12 00:00:00'), Timestamp('2000-03-18 00:00:00'), Timestamp('2000-03-19 00:00:00'), Timestamp('2000-03-25 00:00:00'), Timestamp('2000-03-26 00:00:00'), Timestamp('2000-04-01 00:00:00'), Timestamp('2000-04-02 00:00:00'), Timestamp('2000-04-08 00:00:00'), Timestamp('2000-04-09 00:00:00'), Timestamp('2000-04-15 00:00:00'), Timestamp('2000-04-16 00:00:00'), Timestamp('2000-04-22 00:00:00'), Timestamp('2000-04-23 00:00:00'), Timestamp('2000-04-29 00:00:00'), Timestamp('2000-04-30 00:00:00'), Timestamp('2000-05-06 00:00:00'), Timestamp('2000-05-07 00:00:00'), Timestamp('2000-05-13 00:00:00'), Timestamp('2000-05-14 00:00:00'), Timestamp('2000-05-20 00:00:00'), Timestamp('2000-05-21 00:00:00'), Timestamp('2000-05-27 00:00:00'), Timestamp('2000-05-28 00:00:00'), Timestamp('2000-06-03 00:00:00'), Timestamp('2000-06-04 00:00:00'), Timestamp('2000-06-10 00:00:00'), Timestamp('2000-06-11 00:00:00'), Timestamp('2000-06-17 00:00:00'), Timestamp('2000-06-18 00:00:00'), Timestamp('2000-06-24 00:00:00'), Timestamp('2000-06-25 00:00:00'), Timestamp('2000-07-01 00:00:00'), Timestamp('2000-07-02 00:00:00'), Timestamp('2000-07-08 00:00:00'), Timestamp('2000-07-09 00:00:00'), Timestamp('2000-07-15 00:00:00'), Timestamp('2000-07-16 00:00:00'), Timestamp('2000-07-22 00:00:00'), Timestamp('2000-07-23 00:00:00'), Timestamp('2000-07-29 00:00:00'), Timestamp('2000-07-30 00:00:00'), Timestamp('2000-08-05 00:00:00'), Timestamp('2000-08-06 00:00:00'), Timestamp('2000-08-12 00:00:00'), Timestamp('2000-08-13 00:00:00'), Timestamp('2000-08-19 00:00:00'), Timestamp('2000-08-20 00:00:00'), Timestamp('2000-08-26 00:00:00'), Timestamp('2000-08-27 00:00:00'), Timestamp('2000-09-02 00:00:00'), Timestamp('2000-09-03 00:00:00'), Timestamp('2000-09-09 00:00:00'), Timestamp('2000-09-10 00:00:00'), Timestamp('2000-09-16 00:00:00'), Timestamp('2000-09-17 00:00:00'), Timestamp('2000-09-23 00:00:00'), Timestamp('2000-09-24 00:00:00'), Timestamp('2000-09-30 00:00:00'), Timestamp('2000-10-01 00:00:00'), Timestamp('2000-10-07 00:00:00'), Timestamp('2000-10-08 00:00:00'), Timestamp('2000-10-14 00:00:00'), Timestamp('2000-10-15 00:00:00'), Timestamp('2000-10-21 00:00:00'), Timestamp('2000-10-22 00:00:00'), Timestamp('2000-10-28 00:00:00'), Timestamp('2000-10-29 00:00:00'), Timestamp('2000-11-04 00:00:00'), Timestamp('2000-11-05 00:00:00'), Timestamp('2000-11-11 00:00:00'), Timestamp('2000-11-12 00:00:00'), Timestamp('2000-11-18 00:00:00'), Timestamp('2000-11-19 00:00:00'), Timestamp('2000-11-25 00:00:00'), Timestamp('2000-11-26 00:00:00'), Timestamp('2000-12-02 00:00:00'), Timestamp('2000-12-03 00:00:00'), Timestamp('2000-12-09 00:00:00'), Timestamp('2000-12-10 00:00:00'), Timestamp('2000-12-16 00:00:00'), Timestamp('2000-12-17 00:00:00'), Timestamp('2000-12-23 00:00:00'), Timestamp('2000-12-24 00:00:00'), Timestamp('2000-12-30 00:00:00'), Timestamp('2000-12-31 00:00:00'), Timestamp('2001-01-06 00:00:00'), Timestamp('2001-01-07 00:00:00'), Timestamp('2001-01-13 00:00:00'), Timestamp('2001-01-14 00:00:00'), Timestamp('2001-01-20 00:00:00'), Timestamp('2001-01-21 00:00:00'), Timestamp('2001-01-27 00:00:00'), Timestamp('2001-01-28 00:00:00'), Timestamp('2001-02-03 00:00:00'), Timestamp('2001-02-04 00:00:00'), Timestamp('2001-02-10 00:00:00'), Timestamp('2001-02-11 00:00:00'), Timestamp('2001-02-17 00:00:00'), Timestamp('2001-02-18 00:00:00'), Timestamp('2001-02-24 00:00:00'), Timestamp('2001-02-25 00:00:00'), Timestamp('2001-03-03 00:00:00'), Timestamp('2001-03-04 00:00:00'), Timestamp('2001-03-10 00:00:00'), Timestamp('2001-03-11 00:00:00'), Timestamp('2001-03-17 00:00:00'), Timestamp('2001-03-18 00:00:00'), Timestamp('2001-03-24 00:00:00'), Timestamp('2001-03-25 00:00:00'), Timestamp('2001-03-31 00:00:00'), Timestamp('2001-04-01 00:00:00'), Timestamp('2001-04-07 00:00:00'), Timestamp('2001-04-08 00:00:00'), Timestamp('2001-04-14 00:00:00'), Timestamp('2001-04-15 00:00:00'), Timestamp('2001-04-21 00:00:00'), Timestamp('2001-04-22 00:00:00'), Timestamp('2001-04-28 00:00:00'), Timestamp('2001-04-29 00:00:00'), Timestamp('2001-05-05 00:00:00'), Timestamp('2001-05-06 00:00:00'), Timestamp('2001-05-12 00:00:00'), Timestamp('2001-05-13 00:00:00'), Timestamp('2001-05-19 00:00:00'), Timestamp('2001-05-20 00:00:00'), Timestamp('2001-05-26 00:00:00'), Timestamp('2001-05-27 00:00:00'), Timestamp('2001-06-02 00:00:00'), Timestamp('2001-06-03 00:00:00'), Timestamp('2001-06-09 00:00:00'), Timestamp('2001-06-10 00:00:00'), Timestamp('2001-06-16 00:00:00'), Timestamp('2001-06-17 00:00:00'), Timestamp('2001-06-23 00:00:00'), Timestamp('2001-06-24 00:00:00'), Timestamp('2001-06-30 00:00:00'), Timestamp('2001-07-01 00:00:00'), Timestamp('2001-07-07 00:00:00'), Timestamp('2001-07-08 00:00:00'), Timestamp('2001-07-14 00:00:00'), Timestamp('2001-07-15 00:00:00'), Timestamp('2001-07-21 00:00:00'), Timestamp('2001-07-22 00:00:00'), Timestamp('2001-07-28 00:00:00'), Timestamp('2001-07-29 00:00:00'), Timestamp('2001-08-04 00:00:00'), Timestamp('2001-08-05 00:00:00'), Timestamp('2001-08-11 00:00:00'), Timestamp('2001-08-12 00:00:00'), Timestamp('2001-08-18 00:00:00'), Timestamp('2001-08-19 00:00:00'), Timestamp('2001-08-25 00:00:00'), Timestamp('2001-08-26 00:00:00'), Timestamp('2001-09-01 00:00:00'), Timestamp('2001-09-02 00:00:00'), Timestamp('2001-09-08 00:00:00'), Timestamp('2001-09-09 00:00:00'), Timestamp('2001-09-15 00:00:00'), Timestamp('2001-09-16 00:00:00'), Timestamp('2001-09-22 00:00:00'), Timestamp('2001-09-23 00:00:00'), Timestamp('2001-09-29 00:00:00'), Timestamp('2001-09-30 00:00:00'), Timestamp('2001-10-06 00:00:00'), Timestamp('2001-10-07 00:00:00'), Timestamp('2001-10-13 00:00:00'), Timestamp('2001-10-14 00:00:00'), Timestamp('2001-10-20 00:00:00'), Timestamp('2001-10-21 00:00:00'), Timestamp('2001-10-27 00:00:00'), Timestamp('2001-10-28 00:00:00'), Timestamp('2001-11-03 00:00:00'), Timestamp('2001-11-04 00:00:00'), Timestamp('2001-11-10 00:00:00'), Timestamp('2001-11-11 00:00:00'), Timestamp('2001-11-17 00:00:00'), Timestamp('2001-11-18 00:00:00'), Timestamp('2001-11-24 00:00:00'), Timestamp('2001-11-25 00:00:00'), Timestamp('2001-12-01 00:00:00'), Timestamp('2001-12-02 00:00:00'), Timestamp('2001-12-08 00:00:00'), Timestamp('2001-12-09 00:00:00'), Timestamp('2001-12-15 00:00:00'), Timestamp('2001-12-16 00:00:00'), Timestamp('2001-12-22 00:00:00'), Timestamp('2001-12-23 00:00:00'), Timestamp('2001-12-29 00:00:00'), Timestamp('2001-12-30 00:00:00'), Timestamp('2002-01-05 00:00:00'), Timestamp('2002-01-06 00:00:00'), Timestamp('2002-01-12 00:00:00'), Timestamp('2002-01-13 00:00:00'), Timestamp('2002-01-19 00:00:00'), Timestamp('2002-01-20 00:00:00'), Timestamp('2002-01-26 00:00:00'), Timestamp('2002-01-27 00:00:00'), Timestamp('2002-02-02 00:00:00'), Timestamp('2002-02-03 00:00:00'), Timestamp('2002-02-09 00:00:00'), Timestamp('2002-02-10 00:00:00'), Timestamp('2002-02-16 00:00:00'), Timestamp('2002-02-17 00:00:00'), Timestamp('2002-02-23 00:00:00'), Timestamp('2002-02-24 00:00:00'), Timestamp('2002-03-02 00:00:00'), Timestamp('2002-03-03 00:00:00'), Timestamp('2002-03-09 00:00:00'), Timestamp('2002-03-10 00:00:00'), Timestamp('2002-03-16 00:00:00'), Timestamp('2002-03-17 00:00:00'), Timestamp('2002-03-23 00:00:00'), Timestamp('2002-03-24 00:00:00'), Timestamp('2002-03-30 00:00:00'), Timestamp('2002-03-31 00:00:00'), Timestamp('2002-04-06 00:00:00'), Timestamp('2002-04-07 00:00:00'), Timestamp('2002-04-13 00:00:00'), Timestamp('2002-04-14 00:00:00'), Timestamp('2002-04-20 00:00:00'), Timestamp('2002-04-21 00:00:00'), Timestamp('2002-04-27 00:00:00'), Timestamp('2002-04-28 00:00:00'), Timestamp('2002-05-04 00:00:00'), Timestamp('2002-05-05 00:00:00'), Timestamp('2002-05-11 00:00:00'), Timestamp('2002-05-12 00:00:00'), Timestamp('2002-05-18 00:00:00'), Timestamp('2002-05-19 00:00:00'), Timestamp('2002-05-25 00:00:00'), Timestamp('2002-05-26 00:00:00'), Timestamp('2002-06-01 00:00:00'), Timestamp('2002-06-02 00:00:00'), Timestamp('2002-06-08 00:00:00'), Timestamp('2002-06-09 00:00:00'), Timestamp('2002-06-15 00:00:00'), Timestamp('2002-06-16 00:00:00'), Timestamp('2002-06-22 00:00:00'), Timestamp('2002-06-23 00:00:00'), Timestamp('2002-06-29 00:00:00'), Timestamp('2002-06-30 00:00:00'), Timestamp('2002-07-06 00:00:00'), Timestamp('2002-07-07 00:00:00'), Timestamp('2002-07-13 00:00:00'), Timestamp('2002-07-14 00:00:00'), Timestamp('2002-07-20 00:00:00'), Timestamp('2002-07-21 00:00:00'), Timestamp('2002-07-27 00:00:00'), Timestamp('2002-07-28 00:00:00'), Timestamp('2002-08-03 00:00:00'), Timestamp('2002-08-04 00:00:00'), Timestamp('2002-08-10 00:00:00'), Timestamp('2002-08-11 00:00:00'), Timestamp('2002-08-17 00:00:00'), Timestamp('2002-08-18 00:00:00'), Timestamp('2002-08-24 00:00:00'), Timestamp('2002-08-25 00:00:00'), Timestamp('2002-08-31 00:00:00'), Timestamp('2002-09-01 00:00:00'), Timestamp('2002-09-07 00:00:00'), Timestamp('2002-09-08 00:00:00'), Timestamp('2002-09-14 00:00:00'), Timestamp('2002-09-15 00:00:00'), Timestamp('2002-09-21 00:00:00'), Timestamp('2002-09-22 00:00:00'), Timestamp('2002-09-28 00:00:00'), Timestamp('2002-09-29 00:00:00'), Timestamp('2002-10-05 00:00:00'), Timestamp('2002-10-06 00:00:00'), Timestamp('2002-10-12 00:00:00'), Timestamp('2002-10-13 00:00:00'), Timestamp('2002-10-19 00:00:00'), Timestamp('2002-10-20 00:00:00'), Timestamp('2002-10-26 00:00:00'), Timestamp('2002-10-27 00:00:00'), Timestamp('2002-11-02 00:00:00'), Timestamp('2002-11-03 00:00:00'), Timestamp('2002-11-09 00:00:00'), Timestamp('2002-11-10 00:00:00'), Timestamp('2002-11-16 00:00:00'), Timestamp('2002-11-17 00:00:00'), Timestamp('2002-11-23 00:00:00'), Timestamp('2002-11-24 00:00:00'), Timestamp('2002-11-30 00:00:00'), Timestamp('2002-12-01 00:00:00'), Timestamp('2002-12-07 00:00:00'), Timestamp('2002-12-08 00:00:00'), Timestamp('2002-12-14 00:00:00'), Timestamp('2002-12-15 00:00:00'), Timestamp('2002-12-21 00:00:00'), Timestamp('2002-12-22 00:00:00'), Timestamp('2002-12-28 00:00:00'), Timestamp('2002-12-29 00:00:00'), Timestamp('2003-01-04 00:00:00'), Timestamp('2003-01-05 00:00:00'), Timestamp('2003-01-11 00:00:00'), Timestamp('2003-01-12 00:00:00'), Timestamp('2003-01-18 00:00:00'), Timestamp('2003-01-19 00:00:00'), Timestamp('2003-01-25 00:00:00'), Timestamp('2003-01-26 00:00:00'), Timestamp('2003-02-01 00:00:00'), Timestamp('2003-02-02 00:00:00'), Timestamp('2003-02-08 00:00:00'), Timestamp('2003-02-09 00:00:00'), Timestamp('2003-02-15 00:00:00'), Timestamp('2003-02-16 00:00:00'), Timestamp('2003-02-22 00:00:00'), Timestamp('2003-02-23 00:00:00'), Timestamp('2003-03-01 00:00:00'), Timestamp('2003-03-02 00:00:00'), Timestamp('2003-03-08 00:00:00'), Timestamp('2003-03-09 00:00:00'), Timestamp('2003-03-15 00:00:00'), Timestamp('2003-03-16 00:00:00'), Timestamp('2003-03-22 00:00:00'), Timestamp('2003-03-23 00:00:00'), Timestamp('2003-03-29 00:00:00'), Timestamp('2003-03-30 00:00:00'), Timestamp('2003-04-05 00:00:00'), Timestamp('2003-04-06 00:00:00'), Timestamp('2003-04-12 00:00:00'), Timestamp('2003-04-13 00:00:00'), Timestamp('2003-04-19 00:00:00'), Timestamp('2003-04-20 00:00:00'), Timestamp('2003-04-26 00:00:00'), Timestamp('2003-04-27 00:00:00'), Timestamp('2003-05-03 00:00:00'), Timestamp('2003-05-04 00:00:00'), Timestamp('2003-05-10 00:00:00'), Timestamp('2003-05-11 00:00:00'), Timestamp('2003-05-17 00:00:00'), Timestamp('2003-05-18 00:00:00'), Timestamp('2003-05-24 00:00:00'), Timestamp('2003-05-25 00:00:00'), Timestamp('2003-05-31 00:00:00'), Timestamp('2003-06-01 00:00:00'), Timestamp('2003-06-07 00:00:00'), Timestamp('2003-06-08 00:00:00'), Timestamp('2003-06-14 00:00:00'), Timestamp('2003-06-15 00:00:00'), Timestamp('2003-06-21 00:00:00'), Timestamp('2003-06-22 00:00:00'), Timestamp('2003-06-28 00:00:00'), Timestamp('2003-06-29 00:00:00'), Timestamp('2003-07-05 00:00:00'), Timestamp('2003-07-06 00:00:00'), Timestamp('2003-07-12 00:00:00'), Timestamp('2003-07-13 00:00:00'), Timestamp('2003-07-19 00:00:00'), Timestamp('2003-07-20 00:00:00'), Timestamp('2003-07-26 00:00:00'), Timestamp('2003-07-27 00:00:00'), Timestamp('2003-08-02 00:00:00'), Timestamp('2003-08-03 00:00:00'), Timestamp('2003-08-09 00:00:00'), Timestamp('2003-08-10 00:00:00'), Timestamp('2003-08-16 00:00:00'), Timestamp('2003-08-17 00:00:00'), Timestamp('2003-08-23 00:00:00'), Timestamp('2003-08-24 00:00:00'), Timestamp('2003-08-30 00:00:00'), Timestamp('2003-08-31 00:00:00'), Timestamp('2003-09-06 00:00:00'), Timestamp('2003-09-07 00:00:00'), Timestamp('2003-09-13 00:00:00'), Timestamp('2003-09-14 00:00:00'), Timestamp('2003-09-20 00:00:00'), Timestamp('2003-09-21 00:00:00'), Timestamp('2003-09-27 00:00:00'), Timestamp('2003-09-28 00:00:00'), Timestamp('2003-10-04 00:00:00'), Timestamp('2003-10-05 00:00:00'), Timestamp('2003-10-11 00:00:00'), Timestamp('2003-10-12 00:00:00'), Timestamp('2003-10-18 00:00:00'), Timestamp('2003-10-19 00:00:00'), Timestamp('2003-10-25 00:00:00'), Timestamp('2003-10-26 00:00:00'), Timestamp('2003-11-01 00:00:00'), Timestamp('2003-11-02 00:00:00'), Timestamp('2003-11-08 00:00:00'), Timestamp('2003-11-09 00:00:00'), Timestamp('2003-11-15 00:00:00'), Timestamp('2003-11-16 00:00:00'), Timestamp('2003-11-22 00:00:00'), Timestamp('2003-11-23 00:00:00'), Timestamp('2003-11-29 00:00:00'), Timestamp('2003-11-30 00:00:00'), Timestamp('2003-12-06 00:00:00'), Timestamp('2003-12-07 00:00:00'), Timestamp('2003-12-13 00:00:00'), Timestamp('2003-12-14 00:00:00'), Timestamp('2003-12-20 00:00:00'), Timestamp('2003-12-21 00:00:00'), Timestamp('2003-12-27 00:00:00'), Timestamp('2003-12-28 00:00:00'), Timestamp('2004-01-03 00:00:00'), Timestamp('2004-01-04 00:00:00'), Timestamp('2004-01-10 00:00:00'), Timestamp('2004-01-11 00:00:00'), Timestamp('2004-01-17 00:00:00'), Timestamp('2004-01-18 00:00:00'), Timestamp('2004-01-24 00:00:00'), Timestamp('2004-01-25 00:00:00'), Timestamp('2004-01-31 00:00:00'), Timestamp('2004-02-01 00:00:00'), Timestamp('2004-02-07 00:00:00'), Timestamp('2004-02-08 00:00:00'), Timestamp('2004-02-14 00:00:00'), Timestamp('2004-02-15 00:00:00'), Timestamp('2004-02-21 00:00:00'), Timestamp('2004-02-22 00:00:00'), Timestamp('2004-02-28 00:00:00'), Timestamp('2004-02-29 00:00:00'), Timestamp('2004-03-06 00:00:00'), Timestamp('2004-03-07 00:00:00'), Timestamp('2004-03-13 00:00:00'), Timestamp('2004-03-14 00:00:00'), Timestamp('2004-03-20 00:00:00'), Timestamp('2004-03-21 00:00:00'), Timestamp('2004-03-27 00:00:00'), Timestamp('2004-03-28 00:00:00'), Timestamp('2004-04-03 00:00:00'), Timestamp('2004-04-04 00:00:00'), Timestamp('2004-04-10 00:00:00'), Timestamp('2004-04-11 00:00:00'), Timestamp('2004-04-17 00:00:00'), Timestamp('2004-04-18 00:00:00'), Timestamp('2004-04-24 00:00:00'), Timestamp('2004-04-25 00:00:00'), Timestamp('2004-05-01 00:00:00'), Timestamp('2004-05-02 00:00:00'), Timestamp('2004-05-08 00:00:00'), Timestamp('2004-05-09 00:00:00'), Timestamp('2004-05-15 00:00:00'), Timestamp('2004-05-16 00:00:00'), Timestamp('2004-05-22 00:00:00'), Timestamp('2004-05-23 00:00:00'), Timestamp('2004-05-29 00:00:00'), Timestamp('2004-05-30 00:00:00'), Timestamp('2004-06-05 00:00:00'), Timestamp('2004-06-06 00:00:00'), Timestamp('2004-06-12 00:00:00'), Timestamp('2004-06-13 00:00:00'), Timestamp('2004-06-19 00:00:00'), Timestamp('2004-06-20 00:00:00'), Timestamp('2004-06-26 00:00:00'), Timestamp('2004-06-27 00:00:00'), Timestamp('2004-07-03 00:00:00'), Timestamp('2004-07-04 00:00:00'), Timestamp('2004-07-10 00:00:00'), Timestamp('2004-07-11 00:00:00'), Timestamp('2004-07-17 00:00:00'), Timestamp('2004-07-18 00:00:00'), Timestamp('2004-07-24 00:00:00'), Timestamp('2004-07-25 00:00:00'), Timestamp('2004-07-31 00:00:00'), Timestamp('2004-08-01 00:00:00'), Timestamp('2004-08-07 00:00:00'), Timestamp('2004-08-08 00:00:00'), Timestamp('2004-08-14 00:00:00'), Timestamp('2004-08-15 00:00:00'), Timestamp('2004-08-21 00:00:00'), Timestamp('2004-08-22 00:00:00'), Timestamp('2004-08-28 00:00:00'), Timestamp('2004-08-29 00:00:00'), Timestamp('2004-09-04 00:00:00'), Timestamp('2004-09-05 00:00:00'), Timestamp('2004-09-11 00:00:00'), Timestamp('2004-09-12 00:00:00'), Timestamp('2004-09-18 00:00:00'), Timestamp('2004-09-19 00:00:00'), Timestamp('2004-09-25 00:00:00'), Timestamp('2004-09-26 00:00:00'), Timestamp('2004-10-02 00:00:00'), Timestamp('2004-10-03 00:00:00'), Timestamp('2004-10-09 00:00:00'), Timestamp('2004-10-10 00:00:00'), Timestamp('2004-10-16 00:00:00'), Timestamp('2004-10-17 00:00:00'), Timestamp('2004-10-23 00:00:00'), Timestamp('2004-10-24 00:00:00'), Timestamp('2004-10-30 00:00:00'), Timestamp('2004-10-31 00:00:00'), Timestamp('2004-11-06 00:00:00'), Timestamp('2004-11-07 00:00:00'), Timestamp('2004-11-13 00:00:00'), Timestamp('2004-11-14 00:00:00'), Timestamp('2004-11-20 00:00:00'), Timestamp('2004-11-21 00:00:00'), Timestamp('2004-11-27 00:00:00'), Timestamp('2004-11-28 00:00:00'), Timestamp('2004-12-04 00:00:00'), Timestamp('2004-12-05 00:00:00'), Timestamp('2004-12-11 00:00:00'), Timestamp('2004-12-12 00:00:00'), Timestamp('2004-12-18 00:00:00'), Timestamp('2004-12-19 00:00:00'), Timestamp('2004-12-25 00:00:00'), Timestamp('2004-12-26 00:00:00'), Timestamp('2005-01-01 00:00:00'), Timestamp('2005-01-02 00:00:00'), Timestamp('2005-01-08 00:00:00'), Timestamp('2005-01-09 00:00:00'), Timestamp('2005-01-15 00:00:00'), Timestamp('2005-01-16 00:00:00'), Timestamp('2005-01-22 00:00:00'), Timestamp('2005-01-23 00:00:00'), Timestamp('2005-01-29 00:00:00'), Timestamp('2005-01-30 00:00:00'), Timestamp('2005-02-05 00:00:00'), Timestamp('2005-02-06 00:00:00'), Timestamp('2005-02-12 00:00:00'), Timestamp('2005-02-13 00:00:00'), Timestamp('2005-02-19 00:00:00'), Timestamp('2005-02-20 00:00:00'), Timestamp('2005-02-26 00:00:00'), Timestamp('2005-02-27 00:00:00'), Timestamp('2005-03-05 00:00:00'), Timestamp('2005-03-06 00:00:00'), Timestamp('2005-03-12 00:00:00'), Timestamp('2005-03-13 00:00:00'), Timestamp('2005-03-19 00:00:00'), Timestamp('2005-03-20 00:00:00'), Timestamp('2005-03-26 00:00:00'), Timestamp('2005-03-27 00:00:00'), Timestamp('2005-04-02 00:00:00'), Timestamp('2005-04-03 00:00:00'), Timestamp('2005-04-09 00:00:00'), Timestamp('2005-04-10 00:00:00'), Timestamp('2005-04-16 00:00:00'), Timestamp('2005-04-17 00:00:00'), Timestamp('2005-04-23 00:00:00'), Timestamp('2005-04-24 00:00:00'), Timestamp('2005-04-30 00:00:00'), Timestamp('2005-05-01 00:00:00'), Timestamp('2005-05-07 00:00:00'), Timestamp('2005-05-08 00:00:00'), Timestamp('2005-05-14 00:00:00'), Timestamp('2005-05-15 00:00:00'), Timestamp('2005-05-21 00:00:00'), Timestamp('2005-05-22 00:00:00'), Timestamp('2005-05-28 00:00:00'), Timestamp('2005-05-29 00:00:00'), Timestamp('2005-06-04 00:00:00'), Timestamp('2005-06-05 00:00:00'), Timestamp('2005-06-11 00:00:00'), Timestamp('2005-06-12 00:00:00'), Timestamp('2005-06-18 00:00:00'), Timestamp('2005-06-19 00:00:00'), Timestamp('2005-06-25 00:00:00'), Timestamp('2005-06-26 00:00:00'), Timestamp('2005-07-02 00:00:00'), Timestamp('2005-07-03 00:00:00'), Timestamp('2005-07-09 00:00:00'), Timestamp('2005-07-10 00:00:00'), Timestamp('2005-07-16 00:00:00'), Timestamp('2005-07-17 00:00:00'), Timestamp('2005-07-23 00:00:00'), Timestamp('2005-07-24 00:00:00'), Timestamp('2005-07-30 00:00:00'), Timestamp('2005-07-31 00:00:00'), Timestamp('2005-08-06 00:00:00'), Timestamp('2005-08-07 00:00:00'), Timestamp('2005-08-13 00:00:00'), Timestamp('2005-08-14 00:00:00'), Timestamp('2005-08-20 00:00:00'), Timestamp('2005-08-21 00:00:00'), Timestamp('2005-08-27 00:00:00'), Timestamp('2005-08-28 00:00:00'), Timestamp('2005-09-03 00:00:00'), Timestamp('2005-09-04 00:00:00'), Timestamp('2005-09-10 00:00:00'), Timestamp('2005-09-11 00:00:00'), Timestamp('2005-09-17 00:00:00'), Timestamp('2005-09-18 00:00:00'), Timestamp('2005-09-24 00:00:00'), Timestamp('2005-09-25 00:00:00'), Timestamp('2005-10-01 00:00:00'), Timestamp('2005-10-02 00:00:00'), Timestamp('2005-10-08 00:00:00'), Timestamp('2005-10-09 00:00:00'), Timestamp('2005-10-15 00:00:00'), Timestamp('2005-10-16 00:00:00'), Timestamp('2005-10-22 00:00:00'), Timestamp('2005-10-23 00:00:00'), Timestamp('2005-10-29 00:00:00'), Timestamp('2005-10-30 00:00:00'), Timestamp('2005-11-05 00:00:00'), Timestamp('2005-11-06 00:00:00'), Timestamp('2005-11-12 00:00:00'), Timestamp('2005-11-13 00:00:00'), Timestamp('2005-11-19 00:00:00'), Timestamp('2005-11-20 00:00:00'), Timestamp('2005-11-26 00:00:00'), Timestamp('2005-11-27 00:00:00'), Timestamp('2005-12-03 00:00:00'), Timestamp('2005-12-04 00:00:00'), Timestamp('2005-12-10 00:00:00'), Timestamp('2005-12-11 00:00:00'), Timestamp('2005-12-17 00:00:00'), Timestamp('2005-12-18 00:00:00'), Timestamp('2005-12-24 00:00:00'), Timestamp('2005-12-25 00:00:00'), Timestamp('2005-12-31 00:00:00'), Timestamp('2006-01-01 00:00:00'), Timestamp('2006-01-07 00:00:00'), Timestamp('2006-01-08 00:00:00'), Timestamp('2006-01-14 00:00:00'), Timestamp('2006-01-15 00:00:00'), Timestamp('2006-01-21 00:00:00'), Timestamp('2006-01-22 00:00:00'), Timestamp('2006-01-28 00:00:00'), Timestamp('2006-01-29 00:00:00'), Timestamp('2006-02-04 00:00:00'), Timestamp('2006-02-05 00:00:00'), Timestamp('2006-02-11 00:00:00'), Timestamp('2006-02-12 00:00:00'), Timestamp('2006-02-18 00:00:00'), Timestamp('2006-02-19 00:00:00'), Timestamp('2006-02-25 00:00:00'), Timestamp('2006-02-26 00:00:00'), Timestamp('2006-03-04 00:00:00'), Timestamp('2006-03-05 00:00:00'), Timestamp('2006-03-11 00:00:00'), Timestamp('2006-03-12 00:00:00'), Timestamp('2006-03-18 00:00:00'), Timestamp('2006-03-19 00:00:00'), Timestamp('2006-03-25 00:00:00'), Timestamp('2006-03-26 00:00:00'), Timestamp('2006-04-01 00:00:00'), Timestamp('2006-04-02 00:00:00'), Timestamp('2006-04-08 00:00:00'), Timestamp('2006-04-09 00:00:00'), Timestamp('2006-04-15 00:00:00'), Timestamp('2006-04-16 00:00:00'), Timestamp('2006-04-22 00:00:00'), Timestamp('2006-04-23 00:00:00'), Timestamp('2006-04-29 00:00:00'), Timestamp('2006-04-30 00:00:00'), Timestamp('2006-05-06 00:00:00'), Timestamp('2006-05-07 00:00:00'), Timestamp('2006-05-13 00:00:00'), Timestamp('2006-05-14 00:00:00'), Timestamp('2006-05-20 00:00:00'), Timestamp('2006-05-21 00:00:00'), Timestamp('2006-05-27 00:00:00'), Timestamp('2006-05-28 00:00:00'), Timestamp('2006-06-03 00:00:00'), Timestamp('2006-06-04 00:00:00'), Timestamp('2006-06-10 00:00:00'), Timestamp('2006-06-11 00:00:00'), Timestamp('2006-06-17 00:00:00'), Timestamp('2006-06-18 00:00:00'), Timestamp('2006-06-24 00:00:00'), Timestamp('2006-06-25 00:00:00'), Timestamp('2006-07-01 00:00:00'), Timestamp('2006-07-02 00:00:00'), Timestamp('2006-07-08 00:00:00'), Timestamp('2006-07-09 00:00:00'), Timestamp('2006-07-15 00:00:00'), Timestamp('2006-07-16 00:00:00'), Timestamp('2006-07-22 00:00:00'), Timestamp('2006-07-23 00:00:00'), Timestamp('2006-07-29 00:00:00'), Timestamp('2006-07-30 00:00:00'), Timestamp('2006-08-05 00:00:00'), Timestamp('2006-08-06 00:00:00'), Timestamp('2006-08-12 00:00:00'), Timestamp('2006-08-13 00:00:00'), Timestamp('2006-08-19 00:00:00'), Timestamp('2006-08-20 00:00:00'), Timestamp('2006-08-26 00:00:00'), Timestamp('2006-08-27 00:00:00'), Timestamp('2006-09-02 00:00:00'), Timestamp('2006-09-03 00:00:00'), Timestamp('2006-09-09 00:00:00'), Timestamp('2006-09-10 00:00:00'), Timestamp('2006-09-16 00:00:00'), Timestamp('2006-09-17 00:00:00'), Timestamp('2006-09-23 00:00:00'), Timestamp('2006-09-24 00:00:00'), Timestamp('2006-09-30 00:00:00'), Timestamp('2006-10-01 00:00:00'), Timestamp('2006-10-07 00:00:00'), Timestamp('2006-10-08 00:00:00'), Timestamp('2006-10-14 00:00:00'), Timestamp('2006-10-15 00:00:00'), Timestamp('2006-10-21 00:00:00'), Timestamp('2006-10-22 00:00:00'), Timestamp('2006-10-28 00:00:00'), Timestamp('2006-10-29 00:00:00'), Timestamp('2006-11-04 00:00:00'), Timestamp('2006-11-05 00:00:00'), Timestamp('2006-11-11 00:00:00'), Timestamp('2006-11-12 00:00:00'), Timestamp('2006-11-18 00:00:00'), Timestamp('2006-11-19 00:00:00'), Timestamp('2006-11-25 00:00:00'), Timestamp('2006-11-26 00:00:00'), Timestamp('2006-12-02 00:00:00'), Timestamp('2006-12-03 00:00:00'), Timestamp('2006-12-09 00:00:00'), Timestamp('2006-12-10 00:00:00'), Timestamp('2006-12-16 00:00:00'), Timestamp('2006-12-17 00:00:00'), Timestamp('2006-12-23 00:00:00'), Timestamp('2006-12-24 00:00:00'), Timestamp('2006-12-30 00:00:00'), Timestamp('2006-12-31 00:00:00'), Timestamp('2007-01-06 00:00:00'), Timestamp('2007-01-07 00:00:00'), Timestamp('2007-01-13 00:00:00'), Timestamp('2007-01-14 00:00:00'), Timestamp('2007-01-20 00:00:00'), Timestamp('2007-01-21 00:00:00'), Timestamp('2007-01-27 00:00:00'), Timestamp('2007-01-28 00:00:00'), Timestamp('2007-02-03 00:00:00'), Timestamp('2007-02-04 00:00:00'), Timestamp('2007-02-10 00:00:00'), Timestamp('2007-02-11 00:00:00'), Timestamp('2007-02-17 00:00:00'), Timestamp('2007-02-18 00:00:00'), Timestamp('2007-02-24 00:00:00'), Timestamp('2007-02-25 00:00:00'), Timestamp('2007-03-03 00:00:00'), Timestamp('2007-03-04 00:00:00'), Timestamp('2007-03-10 00:00:00'), Timestamp('2007-03-11 00:00:00'), Timestamp('2007-03-17 00:00:00'), Timestamp('2007-03-18 00:00:00'), Timestamp('2007-03-24 00:00:00'), Timestamp('2007-03-25 00:00:00'), Timestamp('2007-03-31 00:00:00'), Timestamp('2007-04-01 00:00:00'), Timestamp('2007-04-07 00:00:00'), Timestamp('2007-04-08 00:00:00'), Timestamp('2007-04-14 00:00:00'), Timestamp('2007-04-15 00:00:00'), Timestamp('2007-04-21 00:00:00'), Timestamp('2007-04-22 00:00:00'), Timestamp('2007-04-28 00:00:00'), Timestamp('2007-04-29 00:00:00'), Timestamp('2007-05-05 00:00:00'), Timestamp('2007-05-06 00:00:00'), Timestamp('2007-05-12 00:00:00'), Timestamp('2007-05-13 00:00:00'), Timestamp('2007-05-19 00:00:00'), Timestamp('2007-05-20 00:00:00'), Timestamp('2007-05-26 00:00:00'), Timestamp('2007-05-27 00:00:00'), Timestamp('2007-06-02 00:00:00'), Timestamp('2007-06-03 00:00:00'), Timestamp('2007-06-09 00:00:00'), Timestamp('2007-06-10 00:00:00'), Timestamp('2007-06-16 00:00:00'), Timestamp('2007-06-17 00:00:00'), Timestamp('2007-06-23 00:00:00'), Timestamp('2007-06-24 00:00:00'), Timestamp('2007-06-30 00:00:00'), Timestamp('2007-07-01 00:00:00'), Timestamp('2007-07-07 00:00:00'), Timestamp('2007-07-08 00:00:00'), Timestamp('2007-07-14 00:00:00'), Timestamp('2007-07-15 00:00:00'), Timestamp('2007-07-21 00:00:00'), Timestamp('2007-07-22 00:00:00'), Timestamp('2007-07-28 00:00:00'), Timestamp('2007-07-29 00:00:00'), Timestamp('2007-08-04 00:00:00'), Timestamp('2007-08-05 00:00:00'), Timestamp('2007-08-11 00:00:00'), Timestamp('2007-08-12 00:00:00'), Timestamp('2007-08-18 00:00:00'), Timestamp('2007-08-19 00:00:00'), Timestamp('2007-08-25 00:00:00'), Timestamp('2007-08-26 00:00:00'), Timestamp('2007-09-01 00:00:00'), Timestamp('2007-09-02 00:00:00'), Timestamp('2007-09-08 00:00:00'), Timestamp('2007-09-09 00:00:00'), Timestamp('2007-09-15 00:00:00'), Timestamp('2007-09-16 00:00:00'), Timestamp('2007-09-22 00:00:00'), Timestamp('2007-09-23 00:00:00'), Timestamp('2007-09-29 00:00:00'), Timestamp('2007-09-30 00:00:00'), Timestamp('2007-10-06 00:00:00'), Timestamp('2007-10-07 00:00:00'), Timestamp('2007-10-13 00:00:00'), Timestamp('2007-10-14 00:00:00'), Timestamp('2007-10-20 00:00:00'), Timestamp('2007-10-21 00:00:00'), Timestamp('2007-10-27 00:00:00'), Timestamp('2007-10-28 00:00:00'), Timestamp('2007-11-03 00:00:00'), Timestamp('2007-11-04 00:00:00'), Timestamp('2007-11-10 00:00:00'), Timestamp('2007-11-11 00:00:00'), Timestamp('2007-11-17 00:00:00'), Timestamp('2007-11-18 00:00:00'), Timestamp('2007-11-24 00:00:00'), Timestamp('2007-11-25 00:00:00'), Timestamp('2007-12-01 00:00:00'), Timestamp('2007-12-02 00:00:00'), Timestamp('2007-12-08 00:00:00'), Timestamp('2007-12-09 00:00:00'), Timestamp('2007-12-15 00:00:00'), Timestamp('2007-12-16 00:00:00'), Timestamp('2007-12-22 00:00:00'), Timestamp('2007-12-23 00:00:00'), Timestamp('2007-12-29 00:00:00'), Timestamp('2007-12-30 00:00:00'), Timestamp('2008-01-05 00:00:00'), Timestamp('2008-01-06 00:00:00'), Timestamp('2008-01-12 00:00:00'), Timestamp('2008-01-13 00:00:00'), Timestamp('2008-01-19 00:00:00'), Timestamp('2008-01-20 00:00:00'), Timestamp('2008-01-26 00:00:00'), Timestamp('2008-01-27 00:00:00'), Timestamp('2008-02-02 00:00:00'), Timestamp('2008-02-03 00:00:00'), Timestamp('2008-02-09 00:00:00'), Timestamp('2008-02-10 00:00:00'), Timestamp('2008-02-16 00:00:00'), Timestamp('2008-02-17 00:00:00'), Timestamp('2008-02-23 00:00:00'), Timestamp('2008-02-24 00:00:00'), Timestamp('2008-03-01 00:00:00'), Timestamp('2008-03-02 00:00:00'), Timestamp('2008-03-08 00:00:00'), Timestamp('2008-03-09 00:00:00'), Timestamp('2008-03-15 00:00:00'), Timestamp('2008-03-16 00:00:00'), Timestamp('2008-03-22 00:00:00'), Timestamp('2008-03-23 00:00:00'), Timestamp('2008-03-29 00:00:00'), Timestamp('2008-03-30 00:00:00'), Timestamp('2008-04-05 00:00:00'), Timestamp('2008-04-06 00:00:00'), Timestamp('2008-04-12 00:00:00'), Timestamp('2008-04-13 00:00:00'), Timestamp('2008-04-19 00:00:00'), Timestamp('2008-04-20 00:00:00'), Timestamp('2008-04-26 00:00:00'), Timestamp('2008-04-27 00:00:00'), Timestamp('2008-05-03 00:00:00'), Timestamp('2008-05-04 00:00:00'), Timestamp('2008-05-10 00:00:00'), Timestamp('2008-05-11 00:00:00'), Timestamp('2008-05-17 00:00:00'), Timestamp('2008-05-18 00:00:00'), Timestamp('2008-05-24 00:00:00'), Timestamp('2008-05-25 00:00:00'), Timestamp('2008-05-31 00:00:00'), Timestamp('2008-06-01 00:00:00'), Timestamp('2008-06-07 00:00:00'), Timestamp('2008-06-08 00:00:00'), Timestamp('2008-06-14 00:00:00'), Timestamp('2008-06-15 00:00:00'), Timestamp('2008-06-21 00:00:00'), Timestamp('2008-06-22 00:00:00'), Timestamp('2008-06-28 00:00:00'), Timestamp('2008-06-29 00:00:00'), Timestamp('2008-07-05 00:00:00'), Timestamp('2008-07-06 00:00:00'), Timestamp('2008-07-12 00:00:00'), Timestamp('2008-07-13 00:00:00'), Timestamp('2008-07-19 00:00:00'), Timestamp('2008-07-20 00:00:00'), Timestamp('2008-07-26 00:00:00'), Timestamp('2008-07-27 00:00:00'), Timestamp('2008-08-02 00:00:00'), Timestamp('2008-08-03 00:00:00'), Timestamp('2008-08-09 00:00:00'), Timestamp('2008-08-10 00:00:00'), Timestamp('2008-08-16 00:00:00'), Timestamp('2008-08-17 00:00:00'), Timestamp('2008-08-23 00:00:00'), Timestamp('2008-08-24 00:00:00'), Timestamp('2008-08-30 00:00:00'), Timestamp('2008-08-31 00:00:00'), Timestamp('2008-09-06 00:00:00'), Timestamp('2008-09-07 00:00:00'), Timestamp('2008-09-13 00:00:00'), Timestamp('2008-09-14 00:00:00'), Timestamp('2008-09-20 00:00:00'), Timestamp('2008-09-21 00:00:00'), Timestamp('2008-09-27 00:00:00'), Timestamp('2008-09-28 00:00:00'), Timestamp('2008-10-04 00:00:00'), Timestamp('2008-10-05 00:00:00'), Timestamp('2008-10-11 00:00:00'), Timestamp('2008-10-12 00:00:00'), Timestamp('2008-10-18 00:00:00'), Timestamp('2008-10-19 00:00:00'), Timestamp('2008-10-25 00:00:00'), Timestamp('2008-10-26 00:00:00'), Timestamp('2008-11-01 00:00:00'), Timestamp('2008-11-02 00:00:00'), Timestamp('2008-11-08 00:00:00'), Timestamp('2008-11-09 00:00:00'), Timestamp('2008-11-15 00:00:00'), Timestamp('2008-11-16 00:00:00'), Timestamp('2008-11-22 00:00:00'), Timestamp('2008-11-23 00:00:00'), Timestamp('2008-11-29 00:00:00'), Timestamp('2008-11-30 00:00:00'), Timestamp('2008-12-06 00:00:00'), Timestamp('2008-12-07 00:00:00'), Timestamp('2008-12-13 00:00:00'), Timestamp('2008-12-14 00:00:00'), Timestamp('2008-12-20 00:00:00'), Timestamp('2008-12-21 00:00:00'), Timestamp('2008-12-27 00:00:00'), Timestamp('2008-12-28 00:00:00'), Timestamp('2009-01-03 00:00:00'), Timestamp('2009-01-04 00:00:00'), Timestamp('2009-01-10 00:00:00'), Timestamp('2009-01-11 00:00:00'), Timestamp('2009-01-17 00:00:00'), Timestamp('2009-01-18 00:00:00'), Timestamp('2009-01-24 00:00:00'), Timestamp('2009-01-25 00:00:00'), Timestamp('2009-01-31 00:00:00'), Timestamp('2009-02-01 00:00:00'), Timestamp('2009-02-07 00:00:00'), Timestamp('2009-02-08 00:00:00'), Timestamp('2009-02-14 00:00:00'), Timestamp('2009-02-15 00:00:00'), Timestamp('2009-02-21 00:00:00'), Timestamp('2009-02-22 00:00:00'), Timestamp('2009-02-28 00:00:00'), Timestamp('2009-03-01 00:00:00'), Timestamp('2009-03-07 00:00:00'), Timestamp('2009-03-08 00:00:00'), Timestamp('2009-03-14 00:00:00'), Timestamp('2009-03-15 00:00:00'), Timestamp('2009-03-21 00:00:00'), Timestamp('2009-03-22 00:00:00'), Timestamp('2009-03-28 00:00:00'), Timestamp('2009-03-29 00:00:00'), Timestamp('2009-04-04 00:00:00'), Timestamp('2009-04-05 00:00:00'), Timestamp('2009-04-11 00:00:00'), Timestamp('2009-04-12 00:00:00'), Timestamp('2009-04-18 00:00:00'), Timestamp('2009-04-19 00:00:00'), Timestamp('2009-04-25 00:00:00'), Timestamp('2009-04-26 00:00:00'), Timestamp('2009-05-02 00:00:00'), Timestamp('2009-05-03 00:00:00'), Timestamp('2009-05-09 00:00:00'), Timestamp('2009-05-10 00:00:00'), Timestamp('2009-05-16 00:00:00'), Timestamp('2009-05-17 00:00:00'), Timestamp('2009-05-23 00:00:00'), Timestamp('2009-05-24 00:00:00'), Timestamp('2009-05-30 00:00:00'), Timestamp('2009-05-31 00:00:00'), Timestamp('2009-06-06 00:00:00'), Timestamp('2009-06-07 00:00:00'), Timestamp('2009-06-13 00:00:00'), Timestamp('2009-06-14 00:00:00'), Timestamp('2009-06-20 00:00:00'), Timestamp('2009-06-21 00:00:00'), Timestamp('2009-06-27 00:00:00'), Timestamp('2009-06-28 00:00:00'), Timestamp('2009-07-04 00:00:00'), Timestamp('2009-07-05 00:00:00'), Timestamp('2009-07-11 00:00:00'), Timestamp('2009-07-12 00:00:00'), Timestamp('2009-07-18 00:00:00'), Timestamp('2009-07-19 00:00:00'), Timestamp('2009-07-25 00:00:00'), Timestamp('2009-07-26 00:00:00'), Timestamp('2009-08-01 00:00:00'), Timestamp('2009-08-02 00:00:00'), Timestamp('2009-08-08 00:00:00'), Timestamp('2009-08-09 00:00:00'), Timestamp('2009-08-15 00:00:00'), Timestamp('2009-08-16 00:00:00'), Timestamp('2009-08-22 00:00:00'), Timestamp('2009-08-23 00:00:00'), Timestamp('2009-08-29 00:00:00'), Timestamp('2009-08-30 00:00:00'), Timestamp('2009-09-05 00:00:00'), Timestamp('2009-09-06 00:00:00'), Timestamp('2009-09-12 00:00:00'), Timestamp('2009-09-13 00:00:00'), Timestamp('2009-09-19 00:00:00'), Timestamp('2009-09-20 00:00:00'), Timestamp('2009-09-26 00:00:00'), Timestamp('2009-09-27 00:00:00'), Timestamp('2009-10-03 00:00:00'), Timestamp('2009-10-04 00:00:00'), Timestamp('2009-10-10 00:00:00'), Timestamp('2009-10-11 00:00:00'), Timestamp('2009-10-17 00:00:00'), Timestamp('2009-10-18 00:00:00'), Timestamp('2009-10-24 00:00:00'), Timestamp('2009-10-25 00:00:00'), Timestamp('2009-10-31 00:00:00'), Timestamp('2009-11-01 00:00:00'), Timestamp('2009-11-07 00:00:00'), Timestamp('2009-11-08 00:00:00'), Timestamp('2009-11-14 00:00:00'), Timestamp('2009-11-15 00:00:00'), Timestamp('2009-11-21 00:00:00'), Timestamp('2009-11-22 00:00:00'), Timestamp('2009-11-28 00:00:00'), Timestamp('2009-11-29 00:00:00'), Timestamp('2009-12-05 00:00:00'), Timestamp('2009-12-06 00:00:00'), Timestamp('2009-12-12 00:00:00'), Timestamp('2009-12-13 00:00:00'), Timestamp('2009-12-19 00:00:00'), Timestamp('2009-12-20 00:00:00'), Timestamp('2009-12-26 00:00:00'), Timestamp('2009-12-27 00:00:00'), Timestamp('2010-01-02 00:00:00'), Timestamp('2010-01-03 00:00:00'), Timestamp('2010-01-09 00:00:00'), Timestamp('2010-01-10 00:00:00'), Timestamp('2010-01-16 00:00:00'), Timestamp('2010-01-17 00:00:00'), Timestamp('2010-01-23 00:00:00'), Timestamp('2010-01-24 00:00:00'), Timestamp('2010-01-30 00:00:00'), Timestamp('2010-01-31 00:00:00'), Timestamp('2010-02-06 00:00:00'), Timestamp('2010-02-07 00:00:00'), Timestamp('2010-02-13 00:00:00'), Timestamp('2010-02-14 00:00:00'), Timestamp('2010-02-20 00:00:00'), Timestamp('2010-02-21 00:00:00'), Timestamp('2010-02-27 00:00:00'), Timestamp('2010-02-28 00:00:00'), Timestamp('2010-03-06 00:00:00'), Timestamp('2010-03-07 00:00:00'), Timestamp('2010-03-13 00:00:00'), Timestamp('2010-03-14 00:00:00'), Timestamp('2010-03-20 00:00:00'), Timestamp('2010-03-21 00:00:00'), Timestamp('2010-03-27 00:00:00'), Timestamp('2010-03-28 00:00:00'), Timestamp('2010-04-03 00:00:00'), Timestamp('2010-04-04 00:00:00'), Timestamp('2010-04-10 00:00:00'), Timestamp('2010-04-11 00:00:00'), Timestamp('2010-04-17 00:00:00'), Timestamp('2010-04-18 00:00:00'), Timestamp('2010-04-24 00:00:00'), Timestamp('2010-04-25 00:00:00'), Timestamp('2010-05-01 00:00:00'), Timestamp('2010-05-02 00:00:00'), Timestamp('2010-05-08 00:00:00'), Timestamp('2010-05-09 00:00:00'), Timestamp('2010-05-15 00:00:00'), Timestamp('2010-05-16 00:00:00'), Timestamp('2010-05-22 00:00:00'), Timestamp('2010-05-23 00:00:00'), Timestamp('2010-05-29 00:00:00'), Timestamp('2010-05-30 00:00:00'), Timestamp('2010-06-05 00:00:00'), Timestamp('2010-06-06 00:00:00'), Timestamp('2010-06-12 00:00:00'), Timestamp('2010-06-13 00:00:00'), Timestamp('2010-06-19 00:00:00'), Timestamp('2010-06-20 00:00:00'), Timestamp('2010-06-26 00:00:00'), Timestamp('2010-06-27 00:00:00'), Timestamp('2010-07-03 00:00:00'), Timestamp('2010-07-04 00:00:00'), Timestamp('2010-07-10 00:00:00'), Timestamp('2010-07-11 00:00:00'), Timestamp('2010-07-17 00:00:00'), Timestamp('2010-07-18 00:00:00'), Timestamp('2010-07-24 00:00:00'), Timestamp('2010-07-25 00:00:00'), Timestamp('2010-07-31 00:00:00'), Timestamp('2010-08-01 00:00:00'), Timestamp('2010-08-07 00:00:00'), Timestamp('2010-08-08 00:00:00'), Timestamp('2010-08-14 00:00:00'), Timestamp('2010-08-15 00:00:00'), Timestamp('2010-08-21 00:00:00'), Timestamp('2010-08-22 00:00:00'), Timestamp('2010-08-28 00:00:00'), Timestamp('2010-08-29 00:00:00'), Timestamp('2010-09-04 00:00:00'), Timestamp('2010-09-05 00:00:00'), Timestamp('2010-09-11 00:00:00'), Timestamp('2010-09-12 00:00:00'), Timestamp('2010-09-18 00:00:00'), Timestamp('2010-09-19 00:00:00'), Timestamp('2010-09-25 00:00:00'), Timestamp('2010-09-26 00:00:00'), Timestamp('2010-10-02 00:00:00'), Timestamp('2010-10-03 00:00:00'), Timestamp('2010-10-09 00:00:00'), Timestamp('2010-10-10 00:00:00'), Timestamp('2010-10-16 00:00:00'), Timestamp('2010-10-17 00:00:00'), Timestamp('2010-10-23 00:00:00'), Timestamp('2010-10-24 00:00:00'), Timestamp('2010-10-30 00:00:00'), Timestamp('2010-10-31 00:00:00'), Timestamp('2010-11-06 00:00:00'), Timestamp('2010-11-07 00:00:00'), Timestamp('2010-11-13 00:00:00'), Timestamp('2010-11-14 00:00:00'), Timestamp('2010-11-20 00:00:00'), Timestamp('2010-11-21 00:00:00'), Timestamp('2010-11-27 00:00:00'), Timestamp('2010-11-28 00:00:00'), Timestamp('2010-12-04 00:00:00'), Timestamp('2010-12-05 00:00:00'), Timestamp('2010-12-11 00:00:00'), Timestamp('2010-12-12 00:00:00'), Timestamp('2010-12-18 00:00:00'), Timestamp('2010-12-19 00:00:00'), Timestamp('2010-12-25 00:00:00'), Timestamp('2010-12-26 00:00:00'), Timestamp('2011-01-01 00:00:00'), Timestamp('2011-01-02 00:00:00'), Timestamp('2011-01-08 00:00:00'), Timestamp('2011-01-09 00:00:00'), Timestamp('2011-01-15 00:00:00'), Timestamp('2011-01-16 00:00:00'), Timestamp('2011-01-22 00:00:00'), Timestamp('2011-01-23 00:00:00'), Timestamp('2011-01-29 00:00:00'), Timestamp('2011-01-30 00:00:00'), Timestamp('2011-02-05 00:00:00'), Timestamp('2011-02-06 00:00:00'), Timestamp('2011-02-12 00:00:00'), Timestamp('2011-02-13 00:00:00'), Timestamp('2011-02-19 00:00:00'), Timestamp('2011-02-20 00:00:00'), Timestamp('2011-02-26 00:00:00'), Timestamp('2011-02-27 00:00:00'), Timestamp('2011-03-05 00:00:00'), Timestamp('2011-03-06 00:00:00'), Timestamp('2011-03-12 00:00:00'), Timestamp('2011-03-13 00:00:00'), Timestamp('2011-03-19 00:00:00'), Timestamp('2011-03-20 00:00:00'), Timestamp('2011-03-26 00:00:00'), Timestamp('2011-03-27 00:00:00'), Timestamp('2011-04-02 00:00:00'), Timestamp('2011-04-03 00:00:00'), Timestamp('2011-04-09 00:00:00'), Timestamp('2011-04-10 00:00:00'), Timestamp('2011-04-16 00:00:00'), Timestamp('2011-04-17 00:00:00'), Timestamp('2011-04-23 00:00:00'), Timestamp('2011-04-24 00:00:00'), Timestamp('2011-04-30 00:00:00'), Timestamp('2011-05-01 00:00:00'), Timestamp('2011-05-07 00:00:00'), Timestamp('2011-05-08 00:00:00'), Timestamp('2011-05-14 00:00:00'), Timestamp('2011-05-15 00:00:00'), Timestamp('2011-05-21 00:00:00'), Timestamp('2011-05-22 00:00:00'), Timestamp('2011-05-28 00:00:00'), Timestamp('2011-05-29 00:00:00'), Timestamp('2011-06-04 00:00:00'), Timestamp('2011-06-05 00:00:00'), Timestamp('2011-06-11 00:00:00'), Timestamp('2011-06-12 00:00:00'), Timestamp('2011-06-18 00:00:00'), Timestamp('2011-06-19 00:00:00'), Timestamp('2011-06-25 00:00:00'), Timestamp('2011-06-26 00:00:00'), Timestamp('2011-07-02 00:00:00'), Timestamp('2011-07-03 00:00:00'), Timestamp('2011-07-09 00:00:00'), Timestamp('2011-07-10 00:00:00'), Timestamp('2011-07-16 00:00:00'), Timestamp('2011-07-17 00:00:00'), Timestamp('2011-07-23 00:00:00'), Timestamp('2011-07-24 00:00:00'), Timestamp('2011-07-30 00:00:00'), Timestamp('2011-07-31 00:00:00'), Timestamp('2011-08-06 00:00:00'), Timestamp('2011-08-07 00:00:00'), Timestamp('2011-08-13 00:00:00'), Timestamp('2011-08-14 00:00:00'), Timestamp('2011-08-20 00:00:00'), Timestamp('2011-08-21 00:00:00'), Timestamp('2011-08-27 00:00:00'), Timestamp('2011-08-28 00:00:00'), Timestamp('2011-09-03 00:00:00'), Timestamp('2011-09-04 00:00:00'), Timestamp('2011-09-10 00:00:00'), Timestamp('2011-09-11 00:00:00'), Timestamp('2011-09-17 00:00:00'), Timestamp('2011-09-18 00:00:00'), Timestamp('2011-09-24 00:00:00'), Timestamp('2011-09-25 00:00:00'), Timestamp('2011-10-01 00:00:00'), Timestamp('2011-10-02 00:00:00'), Timestamp('2011-10-08 00:00:00'), Timestamp('2011-10-09 00:00:00'), Timestamp('2011-10-15 00:00:00'), Timestamp('2011-10-16 00:00:00'), Timestamp('2011-10-22 00:00:00'), Timestamp('2011-10-23 00:00:00'), Timestamp('2011-10-29 00:00:00'), Timestamp('2011-10-30 00:00:00'), Timestamp('2011-11-05 00:00:00'), Timestamp('2011-11-06 00:00:00'), Timestamp('2011-11-12 00:00:00'), Timestamp('2011-11-13 00:00:00'), Timestamp('2011-11-19 00:00:00'), Timestamp('2011-11-20 00:00:00'), Timestamp('2011-11-26 00:00:00'), Timestamp('2011-11-27 00:00:00'), Timestamp('2011-12-03 00:00:00'), Timestamp('2011-12-04 00:00:00'), Timestamp('2011-12-10 00:00:00'), Timestamp('2011-12-11 00:00:00'), Timestamp('2011-12-17 00:00:00'), Timestamp('2011-12-18 00:00:00'), Timestamp('2011-12-24 00:00:00'), Timestamp('2011-12-25 00:00:00'), Timestamp('2011-12-31 00:00:00'), Timestamp('2012-01-01 00:00:00'), Timestamp('2012-01-07 00:00:00'), Timestamp('2012-01-08 00:00:00'), Timestamp('2012-01-14 00:00:00'), Timestamp('2012-01-15 00:00:00'), Timestamp('2012-01-21 00:00:00'), Timestamp('2012-01-22 00:00:00'), Timestamp('2012-01-28 00:00:00'), Timestamp('2012-01-29 00:00:00'), Timestamp('2012-02-04 00:00:00'), Timestamp('2012-02-05 00:00:00'), Timestamp('2012-02-11 00:00:00'), Timestamp('2012-02-12 00:00:00'), Timestamp('2012-02-18 00:00:00'), Timestamp('2012-02-19 00:00:00'), Timestamp('2012-02-25 00:00:00'), Timestamp('2012-02-26 00:00:00'), Timestamp('2012-03-03 00:00:00'), Timestamp('2012-03-04 00:00:00'), Timestamp('2012-03-10 00:00:00'), Timestamp('2012-03-11 00:00:00'), Timestamp('2012-03-17 00:00:00'), Timestamp('2012-03-18 00:00:00'), Timestamp('2012-03-24 00:00:00'), Timestamp('2012-03-25 00:00:00'), Timestamp('2012-03-31 00:00:00'), Timestamp('2012-04-01 00:00:00'), Timestamp('2012-04-07 00:00:00'), Timestamp('2012-04-08 00:00:00'), Timestamp('2012-04-14 00:00:00'), Timestamp('2012-04-15 00:00:00'), Timestamp('2012-04-21 00:00:00'), Timestamp('2012-04-22 00:00:00'), Timestamp('2012-04-28 00:00:00'), Timestamp('2012-04-29 00:00:00'), Timestamp('2012-05-05 00:00:00'), Timestamp('2012-05-06 00:00:00'), Timestamp('2012-05-12 00:00:00'), Timestamp('2012-05-13 00:00:00'), Timestamp('2012-05-19 00:00:00'), Timestamp('2012-05-20 00:00:00'), Timestamp('2012-05-26 00:00:00'), Timestamp('2012-05-27 00:00:00'), Timestamp('2012-06-02 00:00:00'), Timestamp('2012-06-03 00:00:00'), Timestamp('2012-06-09 00:00:00'), Timestamp('2012-06-10 00:00:00'), Timestamp('2012-06-16 00:00:00'), Timestamp('2012-06-17 00:00:00'), Timestamp('2012-06-23 00:00:00'), Timestamp('2012-06-24 00:00:00'), Timestamp('2012-06-30 00:00:00'), Timestamp('2012-07-01 00:00:00'), Timestamp('2012-07-07 00:00:00'), Timestamp('2012-07-08 00:00:00'), Timestamp('2012-07-14 00:00:00'), Timestamp('2012-07-15 00:00:00'), Timestamp('2012-07-21 00:00:00'), Timestamp('2012-07-22 00:00:00'), Timestamp('2012-07-28 00:00:00'), Timestamp('2012-07-29 00:00:00'), Timestamp('2012-08-04 00:00:00'), Timestamp('2012-08-05 00:00:00'), Timestamp('2012-08-11 00:00:00'), Timestamp('2012-08-12 00:00:00'), Timestamp('2012-08-18 00:00:00'), Timestamp('2012-08-19 00:00:00'), Timestamp('2012-08-25 00:00:00'), Timestamp('2012-08-26 00:00:00'), Timestamp('2012-09-01 00:00:00'), Timestamp('2012-09-02 00:00:00'), Timestamp('2012-09-08 00:00:00'), Timestamp('2012-09-09 00:00:00'), Timestamp('2012-09-15 00:00:00'), Timestamp('2012-09-16 00:00:00'), Timestamp('2012-09-22 00:00:00'), Timestamp('2012-09-23 00:00:00'), Timestamp('2012-09-29 00:00:00'), Timestamp('2012-09-30 00:00:00'), Timestamp('2012-10-06 00:00:00'), Timestamp('2012-10-07 00:00:00'), Timestamp('2012-10-13 00:00:00'), Timestamp('2012-10-14 00:00:00'), Timestamp('2012-10-20 00:00:00'), Timestamp('2012-10-21 00:00:00'), Timestamp('2012-10-27 00:00:00'), Timestamp('2012-10-28 00:00:00'), Timestamp('2012-11-03 00:00:00'), Timestamp('2012-11-04 00:00:00'), Timestamp('2012-11-10 00:00:00'), Timestamp('2012-11-11 00:00:00'), Timestamp('2012-11-17 00:00:00'), Timestamp('2012-11-18 00:00:00'), Timestamp('2012-11-24 00:00:00'), Timestamp('2012-11-25 00:00:00'), Timestamp('2012-12-01 00:00:00'), Timestamp('2012-12-02 00:00:00'), Timestamp('2012-12-08 00:00:00'), Timestamp('2012-12-09 00:00:00'), Timestamp('2012-12-15 00:00:00'), Timestamp('2012-12-16 00:00:00'), Timestamp('2012-12-22 00:00:00'), Timestamp('2012-12-23 00:00:00'), Timestamp('2012-12-29 00:00:00'), Timestamp('2012-12-30 00:00:00'), Timestamp('2013-01-05 00:00:00'), Timestamp('2013-01-06 00:00:00'), Timestamp('2013-01-12 00:00:00'), Timestamp('2013-01-13 00:00:00'), Timestamp('2013-01-19 00:00:00'), Timestamp('2013-01-20 00:00:00'), Timestamp('2013-01-26 00:00:00'), Timestamp('2013-01-27 00:00:00'), Timestamp('2013-02-02 00:00:00'), Timestamp('2013-02-03 00:00:00'), Timestamp('2013-02-09 00:00:00'), Timestamp('2013-02-10 00:00:00'), Timestamp('2013-02-16 00:00:00'), Timestamp('2013-02-17 00:00:00'), Timestamp('2013-02-23 00:00:00'), Timestamp('2013-02-24 00:00:00'), Timestamp('2013-03-02 00:00:00'), Timestamp('2013-03-03 00:00:00'), Timestamp('2013-03-09 00:00:00'), Timestamp('2013-03-10 00:00:00'), Timestamp('2013-03-16 00:00:00'), Timestamp('2013-03-17 00:00:00'), Timestamp('2013-03-23 00:00:00'), Timestamp('2013-03-24 00:00:00'), Timestamp('2013-03-30 00:00:00'), Timestamp('2013-03-31 00:00:00'), Timestamp('2013-04-06 00:00:00'), Timestamp('2013-04-07 00:00:00'), Timestamp('2013-04-13 00:00:00'), Timestamp('2013-04-14 00:00:00'), Timestamp('2013-04-20 00:00:00'), Timestamp('2013-04-21 00:00:00'), Timestamp('2013-04-27 00:00:00'), Timestamp('2013-04-28 00:00:00'), Timestamp('2013-05-04 00:00:00'), Timestamp('2013-05-05 00:00:00'), Timestamp('2013-05-11 00:00:00'), Timestamp('2013-05-12 00:00:00'), Timestamp('2013-05-18 00:00:00'), Timestamp('2013-05-19 00:00:00'), Timestamp('2013-05-25 00:00:00'), Timestamp('2013-05-26 00:00:00'), Timestamp('2013-06-01 00:00:00'), Timestamp('2013-06-02 00:00:00'), Timestamp('2013-06-08 00:00:00'), Timestamp('2013-06-09 00:00:00'), Timestamp('2013-06-15 00:00:00'), Timestamp('2013-06-16 00:00:00'), Timestamp('2013-06-22 00:00:00'), Timestamp('2013-06-23 00:00:00'), Timestamp('2013-06-29 00:00:00'), Timestamp('2013-06-30 00:00:00'), Timestamp('2013-07-06 00:00:00'), Timestamp('2013-07-07 00:00:00'), Timestamp('2013-07-13 00:00:00'), Timestamp('2013-07-14 00:00:00'), Timestamp('2013-07-20 00:00:00'), Timestamp('2013-07-21 00:00:00'), Timestamp('2013-07-27 00:00:00'), Timestamp('2013-07-28 00:00:00'), Timestamp('2013-08-03 00:00:00'), Timestamp('2013-08-04 00:00:00'), Timestamp('2013-08-10 00:00:00'), Timestamp('2013-08-11 00:00:00'), Timestamp('2013-08-17 00:00:00'), Timestamp('2013-08-18 00:00:00'), Timestamp('2013-08-24 00:00:00'), Timestamp('2013-08-25 00:00:00'), Timestamp('2013-08-31 00:00:00'), Timestamp('2013-09-01 00:00:00'), Timestamp('2013-09-07 00:00:00'), Timestamp('2013-09-08 00:00:00'), Timestamp('2013-09-14 00:00:00'), Timestamp('2013-09-15 00:00:00'), Timestamp('2013-09-21 00:00:00'), Timestamp('2013-09-22 00:00:00'), Timestamp('2013-09-28 00:00:00'), Timestamp('2013-09-29 00:00:00'), Timestamp('2013-10-05 00:00:00'), Timestamp('2013-10-06 00:00:00'), Timestamp('2013-10-12 00:00:00'), Timestamp('2013-10-13 00:00:00'), Timestamp('2013-10-19 00:00:00'), Timestamp('2013-10-20 00:00:00'), Timestamp('2013-10-26 00:00:00'), Timestamp('2013-10-27 00:00:00'), Timestamp('2013-11-02 00:00:00'), Timestamp('2013-11-03 00:00:00'), Timestamp('2013-11-09 00:00:00'), Timestamp('2013-11-10 00:00:00'), Timestamp('2013-11-16 00:00:00'), Timestamp('2013-11-17 00:00:00'), Timestamp('2013-11-23 00:00:00'), Timestamp('2013-11-24 00:00:00'), Timestamp('2013-11-30 00:00:00'), Timestamp('2013-12-01 00:00:00'), Timestamp('2013-12-07 00:00:00'), Timestamp('2013-12-08 00:00:00'), Timestamp('2013-12-14 00:00:00'), Timestamp('2013-12-15 00:00:00'), Timestamp('2013-12-21 00:00:00'), Timestamp('2013-12-22 00:00:00'), Timestamp('2013-12-28 00:00:00'), Timestamp('2013-12-29 00:00:00'), Timestamp('2014-01-04 00:00:00'), Timestamp('2014-01-05 00:00:00'), Timestamp('2014-01-11 00:00:00'), Timestamp('2014-01-12 00:00:00'), Timestamp('2014-01-18 00:00:00'), Timestamp('2014-01-19 00:00:00'), Timestamp('2014-01-25 00:00:00'), Timestamp('2014-01-26 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-02-02 00:00:00'), Timestamp('2014-02-08 00:00:00'), Timestamp('2014-02-09 00:00:00'), Timestamp('2014-02-15 00:00:00'), Timestamp('2014-02-16 00:00:00'), Timestamp('2014-02-22 00:00:00'), Timestamp('2014-02-23 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-03-02 00:00:00'), Timestamp('2014-03-08 00:00:00'), Timestamp('2014-03-09 00:00:00'), Timestamp('2014-03-15 00:00:00'), Timestamp('2014-03-16 00:00:00'), Timestamp('2014-03-22 00:00:00'), Timestamp('2014-03-23 00:00:00'), Timestamp('2014-03-29 00:00:00'), Timestamp('2014-03-30 00:00:00'), Timestamp('2014-04-05 00:00:00'), Timestamp('2014-04-06 00:00:00'), Timestamp('2014-04-12 00:00:00'), Timestamp('2014-04-13 00:00:00'), Timestamp('2014-04-19 00:00:00'), Timestamp('2014-04-20 00:00:00'), Timestamp('2014-04-26 00:00:00'), Timestamp('2014-04-27 00:00:00'), Timestamp('2014-05-03 00:00:00'), Timestamp('2014-05-04 00:00:00'), Timestamp('2014-05-10 00:00:00'), Timestamp('2014-05-11 00:00:00'), Timestamp('2014-05-17 00:00:00'), Timestamp('2014-05-18 00:00:00'), Timestamp('2014-05-24 00:00:00'), Timestamp('2014-05-25 00:00:00'), Timestamp('2014-05-31 00:00:00'), Timestamp('2014-06-01 00:00:00'), Timestamp('2014-06-07 00:00:00'), Timestamp('2014-06-08 00:00:00'), Timestamp('2014-06-14 00:00:00'), Timestamp('2014-06-15 00:00:00'), Timestamp('2014-06-21 00:00:00'), Timestamp('2014-06-22 00:00:00'), Timestamp('2014-06-28 00:00:00'), Timestamp('2014-06-29 00:00:00'), Timestamp('2014-07-05 00:00:00'), Timestamp('2014-07-06 00:00:00'), Timestamp('2014-07-12 00:00:00'), Timestamp('2014-07-13 00:00:00'), Timestamp('2014-07-19 00:00:00'), Timestamp('2014-07-20 00:00:00'), Timestamp('2014-07-26 00:00:00'), Timestamp('2014-07-27 00:00:00'), Timestamp('2014-08-02 00:00:00'), Timestamp('2014-08-03 00:00:00'), Timestamp('2014-08-09 00:00:00'), Timestamp('2014-08-10 00:00:00'), Timestamp('2014-08-16 00:00:00'), Timestamp('2014-08-17 00:00:00'), Timestamp('2014-08-23 00:00:00'), Timestamp('2014-08-24 00:00:00'), Timestamp('2014-08-30 00:00:00'), Timestamp('2014-08-31 00:00:00'), Timestamp('2014-09-06 00:00:00'), Timestamp('2014-09-07 00:00:00'), Timestamp('2014-09-13 00:00:00'), Timestamp('2014-09-14 00:00:00'), Timestamp('2014-09-20 00:00:00'), Timestamp('2014-09-21 00:00:00'), Timestamp('2014-09-27 00:00:00'), Timestamp('2014-09-28 00:00:00'), Timestamp('2014-10-04 00:00:00'), Timestamp('2014-10-05 00:00:00'), Timestamp('2014-10-11 00:00:00'), Timestamp('2014-10-12 00:00:00'), Timestamp('2014-10-18 00:00:00'), Timestamp('2014-10-19 00:00:00'), Timestamp('2014-10-25 00:00:00'), Timestamp('2014-10-26 00:00:00'), Timestamp('2014-11-01 00:00:00'), Timestamp('2014-11-02 00:00:00'), Timestamp('2014-11-08 00:00:00'), Timestamp('2014-11-09 00:00:00'), Timestamp('2014-11-15 00:00:00'), Timestamp('2014-11-16 00:00:00'), Timestamp('2014-11-22 00:00:00'), Timestamp('2014-11-23 00:00:00'), Timestamp('2014-11-29 00:00:00'), Timestamp('2014-11-30 00:00:00'), Timestamp('2014-12-06 00:00:00'), Timestamp('2014-12-07 00:00:00'), Timestamp('2014-12-13 00:00:00'), Timestamp('2014-12-14 00:00:00'), Timestamp('2014-12-20 00:00:00'), Timestamp('2014-12-21 00:00:00'), Timestamp('2014-12-27 00:00:00'), Timestamp('2014-12-28 00:00:00'), Timestamp('2015-01-03 00:00:00'), Timestamp('2015-01-04 00:00:00'), Timestamp('2015-01-10 00:00:00'), Timestamp('2015-01-11 00:00:00'), Timestamp('2015-01-17 00:00:00'), Timestamp('2015-01-18 00:00:00'), Timestamp('2015-01-24 00:00:00'), Timestamp('2015-01-25 00:00:00'), Timestamp('2015-01-31 00:00:00'), Timestamp('2015-02-01 00:00:00'), Timestamp('2015-02-07 00:00:00'), Timestamp('2015-02-08 00:00:00'), Timestamp('2015-02-14 00:00:00'), Timestamp('2015-02-15 00:00:00'), Timestamp('2015-02-21 00:00:00'), Timestamp('2015-02-22 00:00:00'), Timestamp('2015-02-28 00:00:00'), Timestamp('2015-03-01 00:00:00'), Timestamp('2015-03-07 00:00:00'), Timestamp('2015-03-08 00:00:00'), Timestamp('2015-03-14 00:00:00'), Timestamp('2015-03-15 00:00:00'), Timestamp('2015-03-21 00:00:00'), Timestamp('2015-03-22 00:00:00'), Timestamp('2015-03-28 00:00:00'), Timestamp('2015-03-29 00:00:00'), Timestamp('2015-04-04 00:00:00'), Timestamp('2015-04-05 00:00:00'), Timestamp('2015-04-11 00:00:00'), Timestamp('2015-04-12 00:00:00'), Timestamp('2015-04-18 00:00:00'), Timestamp('2015-04-19 00:00:00'), Timestamp('2015-04-25 00:00:00'), Timestamp('2015-04-26 00:00:00'), Timestamp('2015-05-02 00:00:00'), Timestamp('2015-05-03 00:00:00'), Timestamp('2015-05-09 00:00:00'), Timestamp('2015-05-10 00:00:00'), Timestamp('2015-05-16 00:00:00'), Timestamp('2015-05-17 00:00:00'), Timestamp('2015-05-23 00:00:00'), Timestamp('2015-05-24 00:00:00'), Timestamp('2015-05-30 00:00:00'), Timestamp('2015-05-31 00:00:00'), Timestamp('2015-06-06 00:00:00'), Timestamp('2015-06-07 00:00:00'), Timestamp('2015-06-13 00:00:00'), Timestamp('2015-06-14 00:00:00'), Timestamp('2015-06-20 00:00:00'), Timestamp('2015-06-21 00:00:00'), Timestamp('2015-06-27 00:00:00'), Timestamp('2015-06-28 00:00:00'), Timestamp('2015-07-04 00:00:00'), Timestamp('2015-07-05 00:00:00'), Timestamp('2015-07-11 00:00:00'), Timestamp('2015-07-12 00:00:00'), Timestamp('2015-07-18 00:00:00'), Timestamp('2015-07-19 00:00:00'), Timestamp('2015-07-25 00:00:00'), Timestamp('2015-07-26 00:00:00'), Timestamp('2015-08-01 00:00:00'), Timestamp('2015-08-02 00:00:00'), Timestamp('2015-08-08 00:00:00'), Timestamp('2015-08-09 00:00:00'), Timestamp('2015-08-15 00:00:00'), Timestamp('2015-08-16 00:00:00'), Timestamp('2015-08-22 00:00:00'), Timestamp('2015-08-23 00:00:00'), Timestamp('2015-08-29 00:00:00'), Timestamp('2015-08-30 00:00:00'), Timestamp('2015-09-05 00:00:00'), Timestamp('2015-09-06 00:00:00'), Timestamp('2015-09-12 00:00:00'), Timestamp('2015-09-13 00:00:00'), Timestamp('2015-09-19 00:00:00'), Timestamp('2015-09-20 00:00:00'), Timestamp('2015-09-26 00:00:00'), Timestamp('2015-09-27 00:00:00'), Timestamp('2015-10-03 00:00:00'), Timestamp('2015-10-04 00:00:00'), Timestamp('2015-10-10 00:00:00'), Timestamp('2015-10-11 00:00:00'), Timestamp('2015-10-17 00:00:00'), Timestamp('2015-10-18 00:00:00'), Timestamp('2015-10-24 00:00:00'), Timestamp('2015-10-25 00:00:00'), Timestamp('2015-10-31 00:00:00'), Timestamp('2015-11-01 00:00:00'), Timestamp('2015-11-07 00:00:00'), Timestamp('2015-11-08 00:00:00'), Timestamp('2015-11-14 00:00:00'), Timestamp('2015-11-15 00:00:00'), Timestamp('2015-11-21 00:00:00'), Timestamp('2015-11-22 00:00:00'), Timestamp('2015-11-28 00:00:00'), Timestamp('2015-11-29 00:00:00'), Timestamp('2015-12-05 00:00:00'), Timestamp('2015-12-06 00:00:00'), Timestamp('2015-12-12 00:00:00'), Timestamp('2015-12-13 00:00:00'), Timestamp('2015-12-19 00:00:00'), Timestamp('2015-12-20 00:00:00'), Timestamp('2015-12-26 00:00:00'), Timestamp('2015-12-27 00:00:00'), Timestamp('2016-01-02 00:00:00'), Timestamp('2016-01-03 00:00:00'), Timestamp('2016-01-09 00:00:00'), Timestamp('2016-01-10 00:00:00'), Timestamp('2016-01-16 00:00:00'), Timestamp('2016-01-17 00:00:00'), Timestamp('2016-01-23 00:00:00'), Timestamp('2016-01-24 00:00:00'), Timestamp('2016-01-30 00:00:00'), Timestamp('2016-01-31 00:00:00'), Timestamp('2016-02-06 00:00:00'), Timestamp('2016-02-07 00:00:00'), Timestamp('2016-02-13 00:00:00'), Timestamp('2016-02-14 00:00:00'), Timestamp('2016-02-20 00:00:00'), Timestamp('2016-02-21 00:00:00'), Timestamp('2016-02-27 00:00:00'), Timestamp('2016-02-28 00:00:00'), Timestamp('2016-03-05 00:00:00'), Timestamp('2016-03-06 00:00:00'), Timestamp('2016-03-12 00:00:00'), Timestamp('2016-03-13 00:00:00'), Timestamp('2016-03-19 00:00:00'), Timestamp('2016-03-20 00:00:00'), Timestamp('2016-03-26 00:00:00'), Timestamp('2016-03-27 00:00:00'), Timestamp('2016-04-02 00:00:00'), Timestamp('2016-04-03 00:00:00'), Timestamp('2016-04-09 00:00:00'), Timestamp('2016-04-10 00:00:00'), Timestamp('2016-04-16 00:00:00'), Timestamp('2016-04-17 00:00:00'), Timestamp('2016-04-23 00:00:00'), Timestamp('2016-04-24 00:00:00'), Timestamp('2016-04-30 00:00:00'), Timestamp('2016-05-01 00:00:00'), Timestamp('2016-05-07 00:00:00'), Timestamp('2016-05-08 00:00:00'), Timestamp('2016-05-14 00:00:00'), Timestamp('2016-05-15 00:00:00'), Timestamp('2016-05-21 00:00:00'), Timestamp('2016-05-22 00:00:00'), Timestamp('2016-05-28 00:00:00'), Timestamp('2016-05-29 00:00:00'), Timestamp('2016-06-04 00:00:00'), Timestamp('2016-06-05 00:00:00'), Timestamp('2016-06-11 00:00:00'), Timestamp('2016-06-12 00:00:00'), Timestamp('2016-06-18 00:00:00'), Timestamp('2016-06-19 00:00:00'), Timestamp('2016-06-25 00:00:00'), Timestamp('2016-06-26 00:00:00'), Timestamp('2016-07-02 00:00:00'), Timestamp('2016-07-03 00:00:00'), Timestamp('2016-07-09 00:00:00'), Timestamp('2016-07-10 00:00:00'), Timestamp('2016-07-16 00:00:00'), Timestamp('2016-07-17 00:00:00'), Timestamp('2016-07-23 00:00:00'), Timestamp('2016-07-24 00:00:00'), Timestamp('2016-07-30 00:00:00'), Timestamp('2016-07-31 00:00:00'), Timestamp('2016-08-06 00:00:00'), Timestamp('2016-08-07 00:00:00'), Timestamp('2016-08-13 00:00:00'), Timestamp('2016-08-14 00:00:00'), Timestamp('2016-08-20 00:00:00'), Timestamp('2016-08-21 00:00:00'), Timestamp('2016-08-27 00:00:00'), Timestamp('2016-08-28 00:00:00'), Timestamp('2016-09-03 00:00:00'), Timestamp('2016-09-04 00:00:00'), Timestamp('2016-09-10 00:00:00'), Timestamp('2016-09-11 00:00:00'), Timestamp('2016-09-17 00:00:00'), Timestamp('2016-09-18 00:00:00'), Timestamp('2016-09-24 00:00:00'), Timestamp('2016-09-25 00:00:00'), Timestamp('2016-10-01 00:00:00'), Timestamp('2016-10-02 00:00:00'), Timestamp('2016-10-08 00:00:00'), Timestamp('2016-10-09 00:00:00'), Timestamp('2016-10-15 00:00:00'), Timestamp('2016-10-16 00:00:00'), Timestamp('2016-10-22 00:00:00'), Timestamp('2016-10-23 00:00:00'), Timestamp('2016-10-29 00:00:00'), Timestamp('2016-10-30 00:00:00'), Timestamp('2016-11-05 00:00:00'), Timestamp('2016-11-06 00:00:00'), Timestamp('2016-11-12 00:00:00'), Timestamp('2016-11-13 00:00:00'), Timestamp('2016-11-19 00:00:00'), Timestamp('2016-11-20 00:00:00'), Timestamp('2016-11-26 00:00:00'), Timestamp('2016-11-27 00:00:00'), Timestamp('2016-12-03 00:00:00'), Timestamp('2016-12-04 00:00:00'), Timestamp('2016-12-10 00:00:00'), Timestamp('2016-12-11 00:00:00'), Timestamp('2016-12-17 00:00:00'), Timestamp('2016-12-18 00:00:00'), Timestamp('2016-12-24 00:00:00'), Timestamp('2016-12-25 00:00:00'), Timestamp('2016-12-31 00:00:00'), Timestamp('2017-01-01 00:00:00'), Timestamp('2017-01-07 00:00:00'), Timestamp('2017-01-08 00:00:00'), Timestamp('2017-01-14 00:00:00'), Timestamp('2017-01-15 00:00:00'), Timestamp('2017-01-21 00:00:00'), Timestamp('2017-01-22 00:00:00'), Timestamp('2017-01-28 00:00:00'), Timestamp('2017-01-29 00:00:00'), Timestamp('2017-02-04 00:00:00'), Timestamp('2017-02-05 00:00:00'), Timestamp('2017-02-11 00:00:00'), Timestamp('2017-02-12 00:00:00'), Timestamp('2017-02-18 00:00:00'), Timestamp('2017-02-19 00:00:00'), Timestamp('2017-02-25 00:00:00'), Timestamp('2017-02-26 00:00:00'), Timestamp('2017-03-04 00:00:00'), Timestamp('2017-03-05 00:00:00'), Timestamp('2017-03-11 00:00:00'), Timestamp('2017-03-12 00:00:00'), Timestamp('2017-03-18 00:00:00'), Timestamp('2017-03-19 00:00:00'), Timestamp('2017-03-25 00:00:00'), Timestamp('2017-03-26 00:00:00'), Timestamp('2017-04-01 00:00:00'), Timestamp('2017-04-02 00:00:00'), Timestamp('2017-04-08 00:00:00'), Timestamp('2017-04-09 00:00:00'), Timestamp('2017-04-15 00:00:00'), Timestamp('2017-04-16 00:00:00'), Timestamp('2017-04-22 00:00:00'), Timestamp('2017-04-23 00:00:00'), Timestamp('2017-04-29 00:00:00'), Timestamp('2017-04-30 00:00:00'), Timestamp('2017-05-06 00:00:00'), Timestamp('2017-05-07 00:00:00'), Timestamp('2017-05-13 00:00:00'), Timestamp('2017-05-14 00:00:00'), Timestamp('2017-05-20 00:00:00'), Timestamp('2017-05-21 00:00:00'), Timestamp('2017-05-27 00:00:00'), Timestamp('2017-05-28 00:00:00'), Timestamp('2017-06-03 00:00:00'), Timestamp('2017-06-04 00:00:00'), Timestamp('2017-06-10 00:00:00'), Timestamp('2017-06-11 00:00:00'), Timestamp('2017-06-17 00:00:00'), Timestamp('2017-06-18 00:00:00'), Timestamp('2017-06-24 00:00:00'), Timestamp('2017-06-25 00:00:00'), Timestamp('2017-07-01 00:00:00'), Timestamp('2017-07-02 00:00:00'), Timestamp('2017-07-08 00:00:00'), Timestamp('2017-07-09 00:00:00'), Timestamp('2017-07-15 00:00:00'), Timestamp('2017-07-16 00:00:00'), Timestamp('2017-07-22 00:00:00'), Timestamp('2017-07-23 00:00:00'), Timestamp('2017-07-29 00:00:00'), Timestamp('2017-07-30 00:00:00'), Timestamp('2017-08-05 00:00:00'), Timestamp('2017-08-06 00:00:00'), Timestamp('2017-08-12 00:00:00'), Timestamp('2017-08-13 00:00:00'), Timestamp('2017-08-19 00:00:00'), Timestamp('2017-08-20 00:00:00'), Timestamp('2017-08-26 00:00:00'), Timestamp('2017-08-27 00:00:00'), Timestamp('2017-09-02 00:00:00'), Timestamp('2017-09-03 00:00:00'), Timestamp('2017-09-09 00:00:00'), Timestamp('2017-09-10 00:00:00'), Timestamp('2017-09-16 00:00:00'), Timestamp('2017-09-17 00:00:00'), Timestamp('2017-09-23 00:00:00'), Timestamp('2017-09-24 00:00:00'), Timestamp('2017-09-30 00:00:00'), Timestamp('2017-10-01 00:00:00'), Timestamp('2017-10-07 00:00:00'), Timestamp('2017-10-08 00:00:00'), Timestamp('2017-10-14 00:00:00'), Timestamp('2017-10-15 00:00:00'), Timestamp('2017-10-21 00:00:00'), Timestamp('2017-10-22 00:00:00'), Timestamp('2017-10-28 00:00:00'), Timestamp('2017-10-29 00:00:00'), Timestamp('2017-11-04 00:00:00'), Timestamp('2017-11-05 00:00:00'), Timestamp('2017-11-11 00:00:00'), Timestamp('2017-11-12 00:00:00'), Timestamp('2017-11-18 00:00:00'), Timestamp('2017-11-19 00:00:00'), Timestamp('2017-11-25 00:00:00'), Timestamp('2017-11-26 00:00:00'), Timestamp('2017-12-02 00:00:00'), Timestamp('2017-12-03 00:00:00'), Timestamp('2017-12-09 00:00:00'), Timestamp('2017-12-10 00:00:00'), Timestamp('2017-12-16 00:00:00'), Timestamp('2017-12-17 00:00:00'), Timestamp('2017-12-23 00:00:00'), Timestamp('2017-12-24 00:00:00'), Timestamp('2017-12-30 00:00:00'), Timestamp('2017-12-31 00:00:00'), Timestamp('2018-01-06 00:00:00'), Timestamp('2018-01-07 00:00:00'), Timestamp('2018-01-13 00:00:00'), Timestamp('2018-01-14 00:00:00'), Timestamp('2018-01-20 00:00:00'), Timestamp('2018-01-21 00:00:00'), Timestamp('2018-01-27 00:00:00'), Timestamp('2018-01-28 00:00:00'), Timestamp('2018-02-03 00:00:00'), Timestamp('2018-02-04 00:00:00'), Timestamp('2018-02-10 00:00:00'), Timestamp('2018-02-11 00:00:00'), Timestamp('2018-02-17 00:00:00'), Timestamp('2018-02-18 00:00:00'), Timestamp('2018-02-24 00:00:00'), Timestamp('2018-02-25 00:00:00'), Timestamp('2018-03-03 00:00:00'), Timestamp('2018-03-04 00:00:00'), Timestamp('2018-03-10 00:00:00'), Timestamp('2018-03-11 00:00:00'), Timestamp('2018-03-17 00:00:00'), Timestamp('2018-03-18 00:00:00'), Timestamp('2018-03-24 00:00:00'), Timestamp('2018-03-25 00:00:00'), Timestamp('2018-03-31 00:00:00'), Timestamp('2018-04-01 00:00:00'), Timestamp('2018-04-07 00:00:00'), Timestamp('2018-04-08 00:00:00'), Timestamp('2018-04-14 00:00:00'), Timestamp('2018-04-15 00:00:00'), Timestamp('2018-04-21 00:00:00'), Timestamp('2018-04-22 00:00:00'), Timestamp('2018-04-28 00:00:00'), Timestamp('2018-04-29 00:00:00'), Timestamp('2018-05-05 00:00:00'), Timestamp('2018-05-06 00:00:00'), Timestamp('2018-05-12 00:00:00'), Timestamp('2018-05-13 00:00:00'), Timestamp('2018-05-19 00:00:00'), Timestamp('2018-05-20 00:00:00'), Timestamp('2018-05-26 00:00:00'), Timestamp('2018-05-27 00:00:00'), Timestamp('2018-06-02 00:00:00'), Timestamp('2018-06-03 00:00:00'), Timestamp('2018-06-09 00:00:00'), Timestamp('2018-06-10 00:00:00'), Timestamp('2018-06-16 00:00:00'), Timestamp('2018-06-17 00:00:00'), Timestamp('2018-06-23 00:00:00'), Timestamp('2018-06-24 00:00:00'), Timestamp('2018-06-30 00:00:00'), Timestamp('2018-07-01 00:00:00'), Timestamp('2018-07-07 00:00:00'), Timestamp('2018-07-08 00:00:00'), Timestamp('2018-07-14 00:00:00'), Timestamp('2018-07-15 00:00:00'), Timestamp('2018-07-21 00:00:00'), Timestamp('2018-07-22 00:00:00'), Timestamp('2018-07-28 00:00:00'), Timestamp('2018-07-29 00:00:00'), Timestamp('2018-08-04 00:00:00'), Timestamp('2018-08-05 00:00:00'), Timestamp('2018-08-11 00:00:00'), Timestamp('2018-08-12 00:00:00'), Timestamp('2018-08-18 00:00:00'), Timestamp('2018-08-19 00:00:00'), Timestamp('2018-08-25 00:00:00'), Timestamp('2018-08-26 00:00:00'), Timestamp('2018-09-01 00:00:00'), Timestamp('2018-09-02 00:00:00'), Timestamp('2018-09-08 00:00:00'), Timestamp('2018-09-09 00:00:00'), Timestamp('2018-09-15 00:00:00'), Timestamp('2018-09-16 00:00:00'), Timestamp('2018-09-22 00:00:00'), Timestamp('2018-09-23 00:00:00'), Timestamp('2018-09-29 00:00:00'), Timestamp('2018-09-30 00:00:00'), Timestamp('2018-10-06 00:00:00'), Timestamp('2018-10-07 00:00:00'), Timestamp('2018-10-13 00:00:00'), Timestamp('2018-10-14 00:00:00'), Timestamp('2018-10-20 00:00:00'), Timestamp('2018-10-21 00:00:00'), Timestamp('2018-10-27 00:00:00'), Timestamp('2018-10-28 00:00:00'), Timestamp('2018-11-03 00:00:00'), Timestamp('2018-11-04 00:00:00'), Timestamp('2018-11-10 00:00:00'), Timestamp('2018-11-11 00:00:00'), Timestamp('2018-11-17 00:00:00'), Timestamp('2018-11-18 00:00:00'), Timestamp('2018-11-24 00:00:00'), Timestamp('2018-11-25 00:00:00'), Timestamp('2018-12-01 00:00:00'), Timestamp('2018-12-02 00:00:00'), Timestamp('2018-12-08 00:00:00'), Timestamp('2018-12-09 00:00:00'), Timestamp('2018-12-15 00:00:00'), Timestamp('2018-12-16 00:00:00'), Timestamp('2018-12-22 00:00:00'), Timestamp('2018-12-23 00:00:00'), Timestamp('2018-12-29 00:00:00'), Timestamp('2018-12-30 00:00:00'), Timestamp('2019-01-05 00:00:00'), Timestamp('2019-01-06 00:00:00'), Timestamp('2019-01-12 00:00:00'), Timestamp('2019-01-13 00:00:00'), Timestamp('2019-01-19 00:00:00'), Timestamp('2019-01-20 00:00:00'), Timestamp('2019-01-26 00:00:00'), Timestamp('2019-01-27 00:00:00'), Timestamp('2019-02-02 00:00:00'), Timestamp('2019-02-03 00:00:00'), Timestamp('2019-02-09 00:00:00'), Timestamp('2019-02-10 00:00:00'), Timestamp('2019-02-16 00:00:00'), Timestamp('2019-02-17 00:00:00'), Timestamp('2019-02-23 00:00:00'), Timestamp('2019-02-24 00:00:00'), Timestamp('2019-03-02 00:00:00'), Timestamp('2019-03-03 00:00:00'), Timestamp('2019-03-09 00:00:00'), Timestamp('2019-03-10 00:00:00'), Timestamp('2019-03-16 00:00:00'), Timestamp('2019-03-17 00:00:00'), Timestamp('2019-03-23 00:00:00'), Timestamp('2019-03-24 00:00:00'), Timestamp('2019-03-30 00:00:00'), Timestamp('2019-03-31 00:00:00'), Timestamp('2019-04-06 00:00:00'), Timestamp('2019-04-07 00:00:00'), Timestamp('2019-04-13 00:00:00'), Timestamp('2019-04-14 00:00:00'), Timestamp('2019-04-20 00:00:00'), Timestamp('2019-04-21 00:00:00'), Timestamp('2019-04-27 00:00:00'), Timestamp('2019-04-28 00:00:00'), Timestamp('2019-05-04 00:00:00'), Timestamp('2019-05-05 00:00:00'), Timestamp('2019-05-11 00:00:00'), Timestamp('2019-05-12 00:00:00'), Timestamp('2019-05-18 00:00:00'), Timestamp('2019-05-19 00:00:00'), Timestamp('2019-05-25 00:00:00'), Timestamp('2019-05-26 00:00:00'), Timestamp('2019-06-01 00:00:00'), Timestamp('2019-06-02 00:00:00'), Timestamp('2019-06-08 00:00:00'), Timestamp('2019-06-09 00:00:00'), Timestamp('2019-06-15 00:00:00'), Timestamp('2019-06-16 00:00:00'), Timestamp('2019-06-22 00:00:00'), Timestamp('2019-06-23 00:00:00'), Timestamp('2019-06-29 00:00:00'), Timestamp('2019-06-30 00:00:00'), Timestamp('2019-07-06 00:00:00'), Timestamp('2019-07-07 00:00:00'), Timestamp('2019-07-13 00:00:00'), Timestamp('2019-07-14 00:00:00'), Timestamp('2019-07-20 00:00:00'), Timestamp('2019-07-21 00:00:00'), Timestamp('2019-07-27 00:00:00'), Timestamp('2019-07-28 00:00:00'), Timestamp('2019-08-03 00:00:00'), Timestamp('2019-08-04 00:00:00'), Timestamp('2019-08-10 00:00:00'), Timestamp('2019-08-11 00:00:00'), Timestamp('2019-08-17 00:00:00'), Timestamp('2019-08-18 00:00:00'), Timestamp('2019-08-24 00:00:00'), Timestamp('2019-08-25 00:00:00'), Timestamp('2019-08-31 00:00:00'), Timestamp('2019-09-01 00:00:00'), Timestamp('2019-09-07 00:00:00'), Timestamp('2019-09-08 00:00:00'), Timestamp('2019-09-14 00:00:00'), Timestamp('2019-09-15 00:00:00'), Timestamp('2019-09-21 00:00:00'), Timestamp('2019-09-22 00:00:00'), Timestamp('2019-09-28 00:00:00'), Timestamp('2019-09-29 00:00:00'), Timestamp('2019-10-05 00:00:00'), Timestamp('2019-10-06 00:00:00'), Timestamp('2019-10-12 00:00:00'), Timestamp('2019-10-13 00:00:00'), Timestamp('2019-10-19 00:00:00'), Timestamp('2019-10-20 00:00:00'), Timestamp('2019-10-26 00:00:00'), Timestamp('2019-10-27 00:00:00'), Timestamp('2019-11-02 00:00:00'), Timestamp('2019-11-03 00:00:00'), Timestamp('2019-11-09 00:00:00'), Timestamp('2019-11-10 00:00:00'), Timestamp('2019-11-16 00:00:00'), Timestamp('2019-11-17 00:00:00'), Timestamp('2019-11-23 00:00:00'), Timestamp('2019-11-24 00:00:00'), Timestamp('2019-11-30 00:00:00'), Timestamp('2019-12-01 00:00:00'), Timestamp('2019-12-07 00:00:00'), Timestamp('2019-12-08 00:00:00'), Timestamp('2019-12-14 00:00:00'), Timestamp('2019-12-15 00:00:00'), Timestamp('2019-12-21 00:00:00'), Timestamp('2019-12-22 00:00:00'), Timestamp('2019-12-28 00:00:00'), Timestamp('2019-12-29 00:00:00'), Timestamp('2020-01-04 00:00:00'), Timestamp('2020-01-05 00:00:00'), Timestamp('2020-01-11 00:00:00'), Timestamp('2020-01-12 00:00:00'), Timestamp('2020-01-18 00:00:00'), Timestamp('2020-01-19 00:00:00'), Timestamp('2020-01-25 00:00:00'), Timestamp('2020-01-26 00:00:00'), Timestamp('2020-02-01 00:00:00'), Timestamp('2020-02-02 00:00:00'), Timestamp('2020-02-08 00:00:00'), Timestamp('2020-02-09 00:00:00'), Timestamp('2020-02-15 00:00:00'), Timestamp('2020-02-16 00:00:00'), Timestamp('2020-02-22 00:00:00'), Timestamp('2020-02-23 00:00:00'), Timestamp('2020-02-29 00:00:00'), Timestamp('2020-03-01 00:00:00'), Timestamp('2020-03-07 00:00:00'), Timestamp('2020-03-08 00:00:00'), Timestamp('2020-03-14 00:00:00'), Timestamp('2020-03-15 00:00:00'), Timestamp('2020-03-21 00:00:00'), Timestamp('2020-03-22 00:00:00'), Timestamp('2020-03-28 00:00:00'), Timestamp('2020-03-29 00:00:00'), Timestamp('2020-04-04 00:00:00'), Timestamp('2020-04-05 00:00:00'), Timestamp('2020-04-11 00:00:00'), Timestamp('2020-04-12 00:00:00'), Timestamp('2020-04-18 00:00:00'), Timestamp('2020-04-19 00:00:00'), Timestamp('2020-04-25 00:00:00'), Timestamp('2020-04-26 00:00:00'), Timestamp('2020-05-02 00:00:00'), Timestamp('2020-05-03 00:00:00'), Timestamp('2020-05-09 00:00:00'), Timestamp('2020-05-10 00:00:00'), Timestamp('2020-05-16 00:00:00'), Timestamp('2020-05-17 00:00:00'), Timestamp('2020-05-23 00:00:00'), Timestamp('2020-05-24 00:00:00'), Timestamp('2020-05-30 00:00:00'), Timestamp('2020-05-31 00:00:00'), Timestamp('2020-06-06 00:00:00'), Timestamp('2020-06-07 00:00:00'), Timestamp('2020-06-13 00:00:00'), Timestamp('2020-06-14 00:00:00'), Timestamp('2020-06-20 00:00:00'), Timestamp('2020-06-21 00:00:00'), Timestamp('2020-06-27 00:00:00'), Timestamp('2020-06-28 00:00:00'), Timestamp('2020-07-04 00:00:00'), Timestamp('2020-07-05 00:00:00'), Timestamp('2020-07-11 00:00:00'), Timestamp('2020-07-12 00:00:00'), Timestamp('2020-07-18 00:00:00'), Timestamp('2020-07-19 00:00:00'), Timestamp('2020-07-25 00:00:00'), Timestamp('2020-07-26 00:00:00'), Timestamp('2020-08-01 00:00:00'), Timestamp('2020-08-02 00:00:00'), Timestamp('2020-08-08 00:00:00'), Timestamp('2020-08-09 00:00:00'), Timestamp('2020-08-15 00:00:00'), Timestamp('2020-08-16 00:00:00'), Timestamp('2020-08-22 00:00:00'), Timestamp('2020-08-23 00:00:00'), Timestamp('2020-08-29 00:00:00'), Timestamp('2020-08-30 00:00:00'), Timestamp('2020-09-05 00:00:00'), Timestamp('2020-09-06 00:00:00'), Timestamp('2020-09-12 00:00:00'), Timestamp('2020-09-13 00:00:00'), Timestamp('2020-09-19 00:00:00'), Timestamp('2020-09-20 00:00:00'), Timestamp('2020-09-26 00:00:00'), Timestamp('2020-09-27 00:00:00'), Timestamp('2020-10-03 00:00:00'), Timestamp('2020-10-04 00:00:00'), Timestamp('2020-10-10 00:00:00'), Timestamp('2020-10-11 00:00:00'), Timestamp('2020-10-17 00:00:00'), Timestamp('2020-10-18 00:00:00'), Timestamp('2020-10-24 00:00:00'), Timestamp('2020-10-25 00:00:00'), Timestamp('2020-10-31 00:00:00'), Timestamp('2020-11-01 00:00:00'), Timestamp('2020-11-07 00:00:00'), Timestamp('2020-11-08 00:00:00'), Timestamp('2020-11-14 00:00:00'), Timestamp('2020-11-15 00:00:00'), Timestamp('2020-11-21 00:00:00'), Timestamp('2020-11-22 00:00:00'), Timestamp('2020-11-28 00:00:00'), Timestamp('2020-11-29 00:00:00'), Timestamp('2020-12-05 00:00:00'), Timestamp('2020-12-06 00:00:00'), Timestamp('2020-12-12 00:00:00'), Timestamp('2020-12-13 00:00:00'), Timestamp('2020-12-19 00:00:00'), Timestamp('2020-12-20 00:00:00'), Timestamp('2020-12-26 00:00:00'), Timestamp('2020-12-27 00:00:00'), Timestamp('2021-01-02 00:00:00'), Timestamp('2021-01-03 00:00:00'), Timestamp('2021-01-09 00:00:00'), Timestamp('2021-01-10 00:00:00'), Timestamp('2021-01-16 00:00:00'), Timestamp('2021-01-17 00:00:00'), Timestamp('2021-01-23 00:00:00'), Timestamp('2021-01-24 00:00:00'), Timestamp('2021-01-30 00:00:00'), Timestamp('2021-01-31 00:00:00'), Timestamp('2021-02-06 00:00:00'), Timestamp('2021-02-07 00:00:00'), Timestamp('2021-02-13 00:00:00'), Timestamp('2021-02-14 00:00:00'), Timestamp('2021-02-20 00:00:00'), Timestamp('2021-02-21 00:00:00'), Timestamp('2021-02-27 00:00:00'), Timestamp('2021-02-28 00:00:00'), Timestamp('2021-03-06 00:00:00'), Timestamp('2021-03-07 00:00:00'), Timestamp('2021-03-13 00:00:00'), Timestamp('2021-03-14 00:00:00'), Timestamp('2021-03-20 00:00:00'), Timestamp('2021-03-21 00:00:00'), Timestamp('2021-03-27 00:00:00'), Timestamp('2021-03-28 00:00:00'), Timestamp('2021-04-03 00:00:00'), Timestamp('2021-04-04 00:00:00'), Timestamp('2021-04-10 00:00:00'), Timestamp('2021-04-11 00:00:00'), Timestamp('2021-04-17 00:00:00'), Timestamp('2021-04-18 00:00:00'), Timestamp('2021-04-24 00:00:00'), Timestamp('2021-04-25 00:00:00'), Timestamp('2021-05-01 00:00:00'), Timestamp('2021-05-02 00:00:00'), Timestamp('2021-05-08 00:00:00'), Timestamp('2021-05-09 00:00:00'), Timestamp('2021-05-15 00:00:00'), Timestamp('2021-05-16 00:00:00'), Timestamp('2021-05-22 00:00:00'), Timestamp('2021-05-23 00:00:00'), Timestamp('2021-05-29 00:00:00'), Timestamp('2021-05-30 00:00:00'), Timestamp('2021-06-05 00:00:00'), Timestamp('2021-06-06 00:00:00'), Timestamp('2021-06-12 00:00:00'), Timestamp('2021-06-13 00:00:00'), Timestamp('2021-06-19 00:00:00'), Timestamp('2021-06-20 00:00:00'), Timestamp('2021-06-26 00:00:00'), Timestamp('2021-06-27 00:00:00'), Timestamp('2021-07-03 00:00:00'), Timestamp('2021-07-04 00:00:00'), Timestamp('2021-07-10 00:00:00'), Timestamp('2021-07-11 00:00:00'), Timestamp('2021-07-17 00:00:00'), Timestamp('2021-07-18 00:00:00'), Timestamp('2021-07-24 00:00:00'), Timestamp('2021-07-25 00:00:00'), Timestamp('2021-07-31 00:00:00'), Timestamp('2021-08-01 00:00:00'), Timestamp('2021-08-07 00:00:00'), Timestamp('2021-08-08 00:00:00'), Timestamp('2021-08-14 00:00:00'), Timestamp('2021-08-15 00:00:00'), Timestamp('2021-08-21 00:00:00'), Timestamp('2021-08-22 00:00:00'), Timestamp('2021-08-28 00:00:00'), Timestamp('2021-08-29 00:00:00'), Timestamp('2021-09-04 00:00:00'), Timestamp('2021-09-05 00:00:00'), Timestamp('2021-09-11 00:00:00'), Timestamp('2021-09-12 00:00:00'), Timestamp('2021-09-18 00:00:00'), Timestamp('2021-09-19 00:00:00'), Timestamp('2021-09-25 00:00:00'), Timestamp('2021-09-26 00:00:00'), Timestamp('2021-10-02 00:00:00'), Timestamp('2021-10-03 00:00:00'), Timestamp('2021-10-09 00:00:00'), Timestamp('2021-10-10 00:00:00'), Timestamp('2021-10-16 00:00:00'), Timestamp('2021-10-17 00:00:00'), Timestamp('2021-10-23 00:00:00'), Timestamp('2021-10-24 00:00:00'), Timestamp('2021-10-30 00:00:00'), Timestamp('2021-10-31 00:00:00'), Timestamp('2021-11-06 00:00:00'), Timestamp('2021-11-07 00:00:00'), Timestamp('2021-11-13 00:00:00'), Timestamp('2021-11-14 00:00:00'), Timestamp('2021-11-20 00:00:00'), Timestamp('2021-11-21 00:00:00'), Timestamp('2021-11-27 00:00:00'), Timestamp('2021-11-28 00:00:00'), Timestamp('2021-12-04 00:00:00'), Timestamp('2021-12-05 00:00:00'), Timestamp('2021-12-11 00:00:00'), Timestamp('2021-12-12 00:00:00'), Timestamp('2021-12-18 00:00:00'), Timestamp('2021-12-19 00:00:00'), Timestamp('2021-12-25 00:00:00'), Timestamp('2021-12-26 00:00:00'), Timestamp('2022-01-01 00:00:00'), Timestamp('2022-01-02 00:00:00'), Timestamp('2022-01-08 00:00:00'), Timestamp('2022-01-09 00:00:00'), Timestamp('2022-01-15 00:00:00'), Timestamp('2022-01-16 00:00:00'), Timestamp('2022-01-22 00:00:00'), Timestamp('2022-01-23 00:00:00'), Timestamp('2022-01-29 00:00:00'), Timestamp('2022-01-30 00:00:00'), Timestamp('2022-02-05 00:00:00'), Timestamp('2022-02-06 00:00:00'), Timestamp('2022-02-12 00:00:00'), Timestamp('2022-02-13 00:00:00'), Timestamp('2022-02-19 00:00:00'), Timestamp('2022-02-20 00:00:00'), Timestamp('2022-02-26 00:00:00'), Timestamp('2022-02-27 00:00:00'), Timestamp('2022-03-05 00:00:00'), Timestamp('2022-03-06 00:00:00'), Timestamp('2022-03-12 00:00:00'), Timestamp('2022-03-13 00:00:00'), Timestamp('2022-03-19 00:00:00'), Timestamp('2022-03-20 00:00:00'), Timestamp('2022-03-26 00:00:00'), Timestamp('2022-03-27 00:00:00'), Timestamp('2022-04-02 00:00:00'), Timestamp('2022-04-03 00:00:00'), Timestamp('2022-04-09 00:00:00'), Timestamp('2022-04-10 00:00:00'), Timestamp('2022-04-16 00:00:00'), Timestamp('2022-04-17 00:00:00'), Timestamp('2022-04-23 00:00:00'), Timestamp('2022-04-24 00:00:00'), Timestamp('2022-04-30 00:00:00'), Timestamp('2022-05-01 00:00:00'), Timestamp('2022-05-07 00:00:00'), Timestamp('2022-05-08 00:00:00'), Timestamp('2022-05-14 00:00:00'), Timestamp('2022-05-15 00:00:00'), Timestamp('2022-05-21 00:00:00'), Timestamp('2022-05-22 00:00:00'), Timestamp('2022-05-28 00:00:00'), Timestamp('2022-05-29 00:00:00'), Timestamp('2022-06-04 00:00:00'), Timestamp('2022-06-05 00:00:00'), Timestamp('2022-06-11 00:00:00'), Timestamp('2022-06-12 00:00:00'), Timestamp('2022-06-18 00:00:00'), Timestamp('2022-06-19 00:00:00'), Timestamp('2022-06-25 00:00:00'), Timestamp('2022-06-26 00:00:00'), Timestamp('2022-07-02 00:00:00'), Timestamp('2022-07-03 00:00:00'), Timestamp('2022-07-09 00:00:00'), Timestamp('2022-07-10 00:00:00'), Timestamp('2022-07-16 00:00:00'), Timestamp('2022-07-17 00:00:00'), Timestamp('2022-07-23 00:00:00'), Timestamp('2022-07-24 00:00:00'), Timestamp('2022-07-30 00:00:00'), Timestamp('2022-07-31 00:00:00'), Timestamp('2022-08-06 00:00:00'), Timestamp('2022-08-07 00:00:00'), Timestamp('2022-08-13 00:00:00'), Timestamp('2022-08-14 00:00:00'), Timestamp('2022-08-20 00:00:00'), Timestamp('2022-08-21 00:00:00'), Timestamp('2022-08-27 00:00:00'), Timestamp('2022-08-28 00:00:00'), Timestamp('2022-09-03 00:00:00'), Timestamp('2022-09-04 00:00:00'), Timestamp('2022-09-10 00:00:00'), Timestamp('2022-09-11 00:00:00'), Timestamp('2022-09-17 00:00:00'), Timestamp('2022-09-18 00:00:00'), Timestamp('2022-09-24 00:00:00'), Timestamp('2022-09-25 00:00:00'), Timestamp('2022-10-01 00:00:00'), Timestamp('2022-10-02 00:00:00'), Timestamp('2022-10-08 00:00:00'), Timestamp('2022-10-09 00:00:00'), Timestamp('2022-10-15 00:00:00'), Timestamp('2022-10-16 00:00:00'), Timestamp('2022-10-22 00:00:00'), Timestamp('2022-10-23 00:00:00'), Timestamp('2022-10-29 00:00:00'), Timestamp('2022-10-30 00:00:00'), Timestamp('2022-11-05 00:00:00'), Timestamp('2022-11-06 00:00:00'), Timestamp('2022-11-12 00:00:00'), Timestamp('2022-11-13 00:00:00'), Timestamp('2022-11-19 00:00:00'), Timestamp('2022-11-20 00:00:00'), Timestamp('2022-11-26 00:00:00'), Timestamp('2022-11-27 00:00:00'), Timestamp('2022-12-03 00:00:00'), Timestamp('2022-12-04 00:00:00'), Timestamp('2022-12-10 00:00:00'), Timestamp('2022-12-11 00:00:00')]\n"
          ]
        }
      ],
      "source": [
        "# Filtering missing dates for weekends\n",
        "weekend_dates = [date for date in missing_dates if date.weekday() >= 5]  # 5 = Saturday, 6 = Sunday\n",
        "\n",
        "print(\"Weekend missing dates:\")\n",
        "print(weekend_dates)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b2085c6",
      "metadata": {
        "id": "3b2085c6",
        "outputId": "98e4406b-f03b-4ac8-df0a-d2d39a0979a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of weekend missing dates: 5526\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of weekend missing dates: {len(weekend_dates)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a470f29",
      "metadata": {
        "id": "0a470f29"
      },
      "source": [
        "The dataset has 5982 missing dates, most of which are weekends (5526 days), aligning with the natural gaps in stock market trading. This makes the dataset appropriate for the task, as it realistically reflects real-world market conditions without requiring additional adjustments."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94b71e74",
      "metadata": {
        "id": "94b71e74"
      },
      "source": [
        "### Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af3aa8e5",
      "metadata": {
        "id": "af3aa8e5",
        "outputId": "eec9df85-6bf1-43cc-8d1d-343b5ecf88e1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1970-01-02</td>\n",
              "      <td>0.688495</td>\n",
              "      <td>0.689779</td>\n",
              "      <td>0.683357</td>\n",
              "      <td>0.683357</td>\n",
              "      <td>1109377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1970-01-05</td>\n",
              "      <td>0.683357</td>\n",
              "      <td>0.688495</td>\n",
              "      <td>0.662805</td>\n",
              "      <td>0.662805</td>\n",
              "      <td>1440243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1970-01-06</td>\n",
              "      <td>0.655098</td>\n",
              "      <td>0.655098</td>\n",
              "      <td>0.646106</td>\n",
              "      <td>0.649960</td>\n",
              "      <td>3503294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1970-01-07</td>\n",
              "      <td>0.652529</td>\n",
              "      <td>0.670512</td>\n",
              "      <td>0.652529</td>\n",
              "      <td>0.666658</td>\n",
              "      <td>5741510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1970-01-08</td>\n",
              "      <td>0.673081</td>\n",
              "      <td>0.692348</td>\n",
              "      <td>0.673081</td>\n",
              "      <td>0.683357</td>\n",
              "      <td>2316067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13351</th>\n",
              "      <td>2022-12-06</td>\n",
              "      <td>95.730003</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>91.980003</td>\n",
              "      <td>92.290001</td>\n",
              "      <td>11492400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13352</th>\n",
              "      <td>2022-12-07</td>\n",
              "      <td>92.660004</td>\n",
              "      <td>92.790001</td>\n",
              "      <td>91.260002</td>\n",
              "      <td>92.150002</td>\n",
              "      <td>10353400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13353</th>\n",
              "      <td>2022-12-08</td>\n",
              "      <td>92.500000</td>\n",
              "      <td>93.430000</td>\n",
              "      <td>91.669998</td>\n",
              "      <td>92.550003</td>\n",
              "      <td>9351200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13354</th>\n",
              "      <td>2022-12-09</td>\n",
              "      <td>92.370003</td>\n",
              "      <td>94.910004</td>\n",
              "      <td>91.900002</td>\n",
              "      <td>93.379997</td>\n",
              "      <td>10101500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13355</th>\n",
              "      <td>2022-12-12</td>\n",
              "      <td>93.730003</td>\n",
              "      <td>94.330002</td>\n",
              "      <td>92.610001</td>\n",
              "      <td>94.099998</td>\n",
              "      <td>3185773</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13356 rows  6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Date       Open       High        Low      Close    Volume\n",
              "0     1970-01-02   0.688495   0.689779   0.683357   0.683357   1109377\n",
              "1     1970-01-05   0.683357   0.688495   0.662805   0.662805   1440243\n",
              "2     1970-01-06   0.655098   0.655098   0.646106   0.649960   3503294\n",
              "3     1970-01-07   0.652529   0.670512   0.652529   0.666658   5741510\n",
              "4     1970-01-08   0.673081   0.692348   0.673081   0.683357   2316067\n",
              "...          ...        ...        ...        ...        ...       ...\n",
              "13351 2022-12-06  95.730003  96.000000  91.980003  92.290001  11492400\n",
              "13352 2022-12-07  92.660004  92.790001  91.260002  92.150002  10353400\n",
              "13353 2022-12-08  92.500000  93.430000  91.669998  92.550003   9351200\n",
              "13354 2022-12-09  92.370003  94.910004  91.900002  93.379997  10101500\n",
              "13355 2022-12-12  93.730003  94.330002  92.610001  94.099998   3185773\n",
              "\n",
              "[13356 rows x 6 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Selecting relevant features\n",
        "features = ['Date','Open', 'High', 'Low', 'Close', 'Volume']\n",
        "target = 'Close'\n",
        "\n",
        "# Ensuring the selected columns exist\n",
        "data = data[features]\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6e69960",
      "metadata": {
        "id": "b6e69960"
      },
      "source": [
        "To retain only the most relevant columns for stock price prediction, I excluded the \"Adjusted Close\" because it is likely redundant as it mirrors the \"Close\" column very closely."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "p0otmeaa-5Im"
      },
      "id": "p0otmeaa-5Im"
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting relevant features\n",
        "features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "target = 'Close'\n",
        "\n",
        "# Splitting into features and target\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "# Train-test-validation split\n",
        "train_size = int(0.7 * len(X))  # 70% for training\n",
        "val_size = int(0.15 * len(X))   # 15% for validation\n",
        "test_size = len(X) - train_size - val_size  # Remaining 15% for testing\n",
        "\n",
        "X_train = X.iloc[:train_size]\n",
        "y_train = y.iloc[:train_size]\n",
        "X_val = X.iloc[train_size:train_size+val_size]\n",
        "y_val = y.iloc[train_size:train_size+val_size]\n",
        "X_test = X.iloc[train_size+val_size:]\n",
        "y_test = y.iloc[train_size+val_size:]"
      ],
      "metadata": {
        "id": "Y7PrvmTX-4hq"
      },
      "id": "Y7PrvmTX-4hq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is split into training (70%), validation (15%), and test (15%) sets sequentially to preserve the chronological order, crucial for time-series tasks. The training set, comprising the first 70% of the data, is used to train the model. The validation set is obtained as the next 15% of the data following the training set and is used to fine-tune hyperparameters and evaluate performance during training. The test set, consisting of the final 15%, assesses the model's generalization on unseen data. This approach avoids data leakage by ensuring the model doesn't use future data to inform predictions, while the proportions provide a balance between training capacity and robust evaluation."
      ],
      "metadata": {
        "id": "wBZLG-y2_GS0"
      },
      "id": "wBZLG-y2_GS0"
    },
    {
      "cell_type": "markdown",
      "id": "676f5e6d",
      "metadata": {
        "id": "676f5e6d"
      },
      "source": [
        "### Feature Normalization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying MinMaxScaler to the training set only and transform validation and test sets using the same scaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)  # Fit and transform on the training set\n",
        "X_val = scaler.transform(X_val)          # Transform validation set\n",
        "X_test = scaler.transform(X_test)        # Transform test set\n",
        "\n",
        "# Converting scaled data back to DataFrame for consistency\n",
        "X_train = pd.DataFrame(X_train, columns=features)\n",
        "X_val = pd.DataFrame(X_val, columns=features)\n",
        "X_test = pd.DataFrame(X_test, columns=features)"
      ],
      "metadata": {
        "id": "OcA04ohx_aj1"
      },
      "id": "OcA04ohx_aj1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "81524403",
      "metadata": {
        "id": "81524403"
      },
      "source": [
        "Feature normalization was applied to the numeric features (Open, High, Low, Close, Volume) to scale their values to a range of [0, 1] using the MinMaxScaler. The scaler was fit on the training set only, ensuring no data leakage, and the same scaling parameters were applied to the validation and test sets. This approach ensures consistency across all datasets while preserving the integrity of the test set during evaluation. By normalizing the features, the training process becomes more stable and efficient for machine learning models like RNNs, GRUs, and LSTMs. Scaling prevents features with larger ranges (e.g., Volume) from dominating those with smaller ranges (e.g., Close), ensuring that all features contribute equally to the learning process. This step is critical for achieving accurate and reliable predictions in time-series forecasting tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4b3d72f",
      "metadata": {
        "id": "d4b3d72f"
      },
      "source": [
        "### Creating Sliding Window Sequences for Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating sliding window sequences\n",
        "sequence_length = 30\n",
        "def create_sequences(X, y, sequence_length):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X) - sequence_length):\n",
        "        X_seq.append(X[i:i + sequence_length].values)\n",
        "        y_seq.append(y.iloc[i + sequence_length])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "X_train, y_train = create_sequences(X_train, y_train, sequence_length)\n",
        "X_val, y_val = create_sequences(X_val, y_val, sequence_length)\n",
        "X_test, y_test = create_sequences(X_test, y_test, sequence_length)"
      ],
      "metadata": {
        "id": "GY5WXs_e_o77"
      },
      "id": "GY5WXs_e_o77",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9557e9d9",
      "metadata": {
        "id": "9557e9d9"
      },
      "source": [
        "This part prepares the sliding window sequences for training, validation, and testing. For each data point, it collects the previous 30 days of data (sequence_length) as input features (X) and the corresponding next day's closing price as the target variable (y). The function create_sequences ensures the sequences are stored as NumPy arrays to be compatible with machine learning models.\n",
        "\n",
        "The sequence length of 30 days is chosen because it aligns with real-world financial practices, where trends over the past month (approximately 30 trading days) are considered crucial for making predictions. This window size effectively captures short-term market behaviors, such as trends, cycles, and volatility, which are commonly observed in stock markets. It strikes a practical balance between including sufficient historical context for meaningful predictions and avoiding excessive input size that might introduce noise or computational inefficiencies. This preprocessing step ensures the data is well-structured for sequential models like RNNs, GRUs, and LSTMs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcc6e810",
      "metadata": {
        "id": "dcc6e810"
      },
      "source": [
        "## Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eede1e04",
      "metadata": {
        "id": "eede1e04"
      },
      "source": [
        "### Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e22d86",
      "metadata": {
        "id": "c1e22d86"
      },
      "outputs": [],
      "source": [
        "# Define custom RMSE metric\n",
        "def rmse(y_true, y_pred):\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
        "\n",
        "\n",
        "# Function to calculate MDA (Mean Directional Accuracy)\n",
        "def mda(y_true, y_pred, t=12):\n",
        "    # Convert y_true and y_pred to float32 to ensure consistent data type\n",
        "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # Calculate the direction of change (sign of the difference)\n",
        "    direction_true = tf.sign(y_true[1:] - y_true[:-1])  # Direction of actual values\n",
        "    direction_pred = tf.sign(y_pred[1:] - y_pred[:-1])  # Direction of predicted values\n",
        "\n",
        "    # Compare the directions and calculate the mean accuracy\n",
        "    # Cast boolean output of tf.equal to float32 to avoid TypeError\n",
        "    return tf.reduce_mean(tf.cast(tf.equal(direction_true, direction_pred), dtype=tf.float32))\n",
        "\n",
        "def accuracy(y_true, y_pred, threshold=0.02):\n",
        "    # Calculate Mean Absolute Percentage Error (MAPE)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "    return 100 - mape  # Return accuracy as 100 - MAPE\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    acc = accuracy(y_test, y_pred)\n",
        "    test_rmse = rmse(y_test, y_pred)\n",
        "    test_mda = mda(y_test, y_pred)\n",
        "    test_mae = mae(y_test, y_pred)\n",
        "\n",
        "    return acc, test_rmse, test_mda, test_mae"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "334d19ed",
      "metadata": {
        "id": "334d19ed"
      },
      "source": [
        "I used RMSE, MDA, and accuracy to evaluate model performance.\n",
        "\n",
        "RMSE: Measures the magnitude of the errors in predicted stock prices, and penalizes large deviations, which is crucial for minimizing potential financial losses due to large errors.\n",
        "\n",
        "MDA: Measures directional prediction accuracy (whether the predicted movement direction matches the actual one).\n",
        "This is key in trading, where predicting the right trend (up or down) is more important than predicting the exact value.\n",
        "\n",
        "Accuracy: Used for directional prediction. It measures how often the model correctly predicts whether the stock price will go up or down, which is an essential part of trend-based trading strategies.\n",
        "\n",
        "MAE (Mean Absolute Error): Measures the average magnitude of errors in predictions without considering their direction. Unlike RMSE, it treats all errors equally, regardless of whether they are large or small, making it a simpler measure to understand the general performance of the model. MAE provides a straightforward metric for evaluating how close the model's predictions are to the actual values in terms of absolute difference.\n",
        "\n",
        "Together, RMSE, MDA, Accuracy, and MAE provide a well-rounded evaluation of the models' performance. These metrics capture both magnitude (how close the predicted prices are) and direction (whether the model predicts the correct price trend), allowing for a comprehensive assessment of model accuracy, reliability, and practical utility in stock market prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfdcd4f6",
      "metadata": {
        "id": "dfdcd4f6"
      },
      "source": [
        "## Model Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cff4ea91",
      "metadata": {
        "id": "cff4ea91"
      },
      "source": [
        "### Vanilla RNN (Baseline Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91946fe3",
      "metadata": {
        "id": "91946fe3"
      },
      "outputs": [],
      "source": [
        "def build_simple_rnn(input_shape, rnn_units, dropout_rate, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(rnn_units, return_sequences=True, input_shape=input_shape, activation='tanh'))  # tanh activation\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(SimpleRNN(rnn_units, activation='tanh'))  # Stacked RNN layer with tanh\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1))  # Output layer\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=[rmse, mda, 'mae'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "110d660a",
      "metadata": {
        "id": "110d660a"
      },
      "source": [
        "### GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bee1aaa9",
      "metadata": {
        "id": "bee1aaa9"
      },
      "outputs": [],
      "source": [
        "def build_gru(input_shape, rnn_units, dropout_rate, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(GRU(rnn_units, return_sequences=True, input_shape=input_shape, activation='tanh'))  # tanh activation\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(GRU(rnn_units, activation='tanh'))  # Stacked GRU layer with tanh\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1))  # Output layer\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=[rmse, mda, 'mae'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8e8226e",
      "metadata": {
        "id": "f8e8226e"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "606e105c",
      "metadata": {
        "id": "606e105c"
      },
      "outputs": [],
      "source": [
        "def build_lstm(input_shape, rnn_units, dropout_rate, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(rnn_units, return_sequences=True, input_shape=input_shape, activation='tanh'))  # tanh activation\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(LSTM(rnn_units, activation='tanh'))  # Stacked LSTM layer with tanh\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1))  # Output layer\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=[rmse, mda, 'mae'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb23f571",
      "metadata": {
        "id": "eb23f571"
      },
      "source": [
        "I built 3 types of models: SimpleRNN, GRU, and LSTM. These are all variants of recurrent neural networks (RNNs) designed to process sequential data. The architectures are composed of several layers: the first layer in each model is a recurrent layer (SimpleRNN, GRU, or LSTM), with the return_sequences=True argument ensuring that the output of each timestep is passed on to the next layer, enabling the model to capture temporal dependencies effectively. A second stacked recurrent layer further enhances the model's ability to learn hierarchical temporal patterns, making it better suited for complex sequential relationships. Each recurrent layer uses the tanh activation function, which scales outputs to the range [-1, 1], stabilizing gradients during training and enabling smooth learning. Dropout layers are applied after each recurrent layer to prevent overfitting by randomly setting a fraction of the input units to 0 during training. Finally, the Dense output layer, with a linear activation function, predicts the closing price. This architecture is particularly suitable for time-series forecasting tasks, such as stock price prediction, as it effectively captures both short-term and long-term temporal patterns in sequential data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6b6fb10",
      "metadata": {
        "id": "b6b6fb10"
      },
      "source": [
        "### Hyper-parameter Tuning and Training of each model on best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a93e2cf2",
      "metadata": {
        "id": "a93e2cf2"
      },
      "outputs": [],
      "source": [
        "def get_callbacks():\n",
        "    callbacks = [\n",
        "        ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, verbose=1),  # Updated file extension\n",
        "        EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
        "    ]\n",
        "    return callbacks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2475ed36",
      "metadata": {
        "scrolled": true,
        "id": "2475ed36"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning function with logging\n",
        "def tune_model(build_model_fn, X_train, y_train, X_val, y_val, param_grid):\n",
        "    best_model = None\n",
        "    best_val_loss = float('inf')\n",
        "    best_params = None\n",
        "    results = []  # Log results\n",
        "\n",
        "    for params in ParameterGrid(param_grid):\n",
        "        print(f\"Training with parameters: {params}\")\n",
        "        print(\"-\" * 100)  # Add dashed line before training starts\n",
        "\n",
        "        # Extract parameters for the model\n",
        "        model_params = {k: params[k] for k in ['rnn_units', 'dropout_rate', 'learning_rate']}\n",
        "        batch_size = params['batch_size']\n",
        "\n",
        "        # Build the model\n",
        "        model = build_model_fn(input_shape=(X_train.shape[1], X_train.shape[2]), **model_params)\n",
        "\n",
        "        # Train the model\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=50,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=get_callbacks(),\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Get validation loss and metrics from history\n",
        "        val_loss = min(history.history['val_loss'])\n",
        "        val_rmse = min(history.history['val_rmse'])\n",
        "        val_mda = max(history.history['val_mda'])\n",
        "        val_mae = min(history.history['val_mae'])\n",
        "\n",
        "        # Log results with 2 decimal precision\n",
        "        results.append({**params,\n",
        "                        'val_loss': f\"{val_loss:.5f}\",\n",
        "                        'val_rmse': f\"{val_rmse:.5f}\",\n",
        "                        'val_mda': f\"{val_mda:.5f}\",\n",
        "                        'val_mae': f\"{val_mae:.5f}\"})\n",
        "\n",
        "        # Print out results with 2 decimal precision\n",
        "        print(f\"Validation Loss: {val_loss:.5f}, RMSE: {val_rmse:.5f}, MDA: {val_mda:.5f}, MAE: {val_mae:.5f}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Update the best model if validation loss improves\n",
        "        if val_loss < best_val_loss:\n",
        "            best_model = model\n",
        "            best_val_loss = val_loss\n",
        "            best_params = params\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    return best_model, best_params, best_val_loss, results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4f86059",
      "metadata": {
        "id": "f4f86059"
      },
      "source": [
        "### Hyper-parameter Grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4188d4f9",
      "metadata": {
        "id": "4188d4f9"
      },
      "outputs": [],
      "source": [
        "# Defining hyperparameter search space\n",
        "param_grid = {\n",
        "    'rnn_units': [32, 64, 128],\n",
        "    'dropout_rate': [0.2, 0.3],\n",
        "    'learning_rate': [0.001, 0.01],\n",
        "    'batch_size': [16, 32]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9672d9fe",
      "metadata": {
        "id": "9672d9fe"
      },
      "source": [
        "### Tuning Vanilla RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "440396ca",
      "metadata": {
        "scrolled": false,
        "id": "440396ca",
        "outputId": "f8a85082-2cc5-4fba-90c9-018ba0f73556"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning SimpleRNN...\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sadafahmedsara/anaconda3/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:205: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m568/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0170 - mae: 0.0736 - mda: 0.4994 - rmse: 0.1304\n",
            "Epoch 1: val_loss improved from inf to 0.00156, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0167 - mae: 0.0728 - mda: 0.4994 - rmse: 0.1295 - val_loss: 0.0016 - val_mae: 0.0296 - val_mda: 0.5007 - val_rmse: 0.0312 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m565/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8983e-04 - mae: 0.0184 - mda: 0.5038 - rmse: 0.0800\n",
            "Epoch 2: val_loss improved from 0.00156 to 0.00047, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.8440e-04 - mae: 0.0184 - mda: 0.5039 - rmse: 0.0800 - val_loss: 4.6543e-04 - val_mae: 0.0141 - val_mda: 0.4995 - val_rmse: 0.0157 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m556/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6130e-04 - mae: 0.0126 - mda: 0.5053 - rmse: 0.0765\n",
            "Epoch 3: val_loss improved from 0.00047 to 0.00030, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5949e-04 - mae: 0.0126 - mda: 0.5053 - rmse: 0.0766 - val_loss: 3.0084e-04 - val_mae: 0.0115 - val_mda: 0.5023 - val_rmse: 0.0133 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3637e-04 - mae: 0.0105 - mda: 0.5065 - rmse: 0.0773\n",
            "Epoch 4: val_loss did not improve from 0.00030\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3613e-04 - mae: 0.0105 - mda: 0.5065 - rmse: 0.0773 - val_loss: 4.7453e-04 - val_mae: 0.0161 - val_mda: 0.5047 - val_rmse: 0.0177 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6697e-04 - mae: 0.0089 - mda: 0.5064 - rmse: 0.0773\n",
            "Epoch 5: val_loss improved from 0.00030 to 0.00014, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6697e-04 - mae: 0.0089 - mda: 0.5064 - rmse: 0.0773 - val_loss: 1.3989e-04 - val_mae: 0.0082 - val_mda: 0.5029 - val_rmse: 0.0102 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5031e-04 - mae: 0.0084 - mda: 0.5059 - rmse: 0.0765\n",
            "Epoch 6: val_loss did not improve from 0.00014\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5022e-04 - mae: 0.0084 - mda: 0.5059 - rmse: 0.0765 - val_loss: 2.0830e-04 - val_mae: 0.0113 - val_mda: 0.5026 - val_rmse: 0.0131 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2245e-04 - mae: 0.0077 - mda: 0.5064 - rmse: 0.0760\n",
            "Epoch 7: val_loss improved from 0.00014 to 0.00005, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2236e-04 - mae: 0.0077 - mda: 0.5064 - rmse: 0.0761 - val_loss: 4.6825e-05 - val_mae: 0.0050 - val_mda: 0.5038 - val_rmse: 0.0076 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0274e-04 - mae: 0.0072 - mda: 0.5070 - rmse: 0.0758\n",
            "Epoch 8: val_loss improved from 0.00005 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0271e-04 - mae: 0.0072 - mda: 0.5070 - rmse: 0.0758 - val_loss: 4.2472e-05 - val_mae: 0.0051 - val_mda: 0.5076 - val_rmse: 0.0076 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m561/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.5951e-05 - mae: 0.0067 - mda: 0.5064 - rmse: 0.0764\n",
            "Epoch 9: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.5931e-05 - mae: 0.0067 - mda: 0.5065 - rmse: 0.0764 - val_loss: 5.8589e-05 - val_mae: 0.0055 - val_mda: 0.5071 - val_rmse: 0.0080 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m562/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9050e-05 - mae: 0.0064 - mda: 0.5070 - rmse: 0.0764\n",
            "Epoch 10: val_loss improved from 0.00004 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.9003e-05 - mae: 0.0064 - mda: 0.5070 - rmse: 0.0764 - val_loss: 3.9410e-05 - val_mae: 0.0048 - val_mda: 0.5095 - val_rmse: 0.0074 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.0195e-05 - mae: 0.0064 - mda: 0.5073 - rmse: 0.0763\n",
            "Epoch 11: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.0169e-05 - mae: 0.0064 - mda: 0.5073 - rmse: 0.0763 - val_loss: 2.4216e-04 - val_mae: 0.0119 - val_mda: 0.5113 - val_rmse: 0.0136 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m556/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7210e-05 - mae: 0.0060 - mda: 0.5071 - rmse: 0.0765\n",
            "Epoch 12: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.7151e-05 - mae: 0.0060 - mda: 0.5072 - rmse: 0.0765 - val_loss: 1.9408e-04 - val_mae: 0.0107 - val_mda: 0.5127 - val_rmse: 0.0125 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m564/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7906e-05 - mae: 0.0059 - mda: 0.5073 - rmse: 0.0770\n",
            "Epoch 13: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.7797e-05 - mae: 0.0059 - mda: 0.5073 - rmse: 0.0770 - val_loss: 2.0484e-04 - val_mae: 0.0104 - val_mda: 0.5141 - val_rmse: 0.0123 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m565/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9595e-05 - mae: 0.0056 - mda: 0.5080 - rmse: 0.0761\n",
            "Epoch 14: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.9661e-05 - mae: 0.0056 - mda: 0.5080 - rmse: 0.0761 - val_loss: 1.6061e-04 - val_mae: 0.0094 - val_mda: 0.5144 - val_rmse: 0.0114 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m558/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.6881e-05 - mae: 0.0054 - mda: 0.5081 - rmse: 0.0759\n",
            "Epoch 15: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.6940e-05 - mae: 0.0054 - mda: 0.5081 - rmse: 0.0759 - val_loss: 1.4397e-04 - val_mae: 0.0095 - val_mda: 0.5198 - val_rmse: 0.0114 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7874e-05 - mae: 0.0050 - mda: 0.5083 - rmse: 0.0769\n",
            "Epoch 16: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.7869e-05 - mae: 0.0050 - mda: 0.5083 - rmse: 0.0769 - val_loss: 6.0360e-05 - val_mae: 0.0051 - val_mda: 0.5197 - val_rmse: 0.0077 - learning_rate: 5.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m564/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5201e-05 - mae: 0.0049 - mda: 0.5076 - rmse: 0.0760\n",
            "Epoch 17: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.5253e-05 - mae: 0.0049 - mda: 0.5076 - rmse: 0.0761 - val_loss: 4.1931e-05 - val_mae: 0.0044 - val_mda: 0.5200 - val_rmse: 0.0071 - learning_rate: 5.0000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m560/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7815e-05 - mae: 0.0049 - mda: 0.5064 - rmse: 0.0754\n",
            "Epoch 18: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.7925e-05 - mae: 0.0049 - mda: 0.5064 - rmse: 0.0755 - val_loss: 1.1689e-04 - val_mae: 0.0075 - val_mda: 0.5192 - val_rmse: 0.0098 - learning_rate: 5.0000e-04\n",
            "Epoch 19/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m560/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4281e-05 - mae: 0.0048 - mda: 0.5068 - rmse: 0.0767\n",
            "Epoch 19: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.4307e-05 - mae: 0.0048 - mda: 0.5068 - rmse: 0.0767 - val_loss: 6.5632e-05 - val_mae: 0.0051 - val_mda: 0.5207 - val_rmse: 0.0078 - learning_rate: 5.0000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m561/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9862e-05 - mae: 0.0051 - mda: 0.5086 - rmse: 0.0774\n",
            "Epoch 20: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.9725e-05 - mae: 0.0051 - mda: 0.5086 - rmse: 0.0774 - val_loss: 5.5917e-04 - val_mae: 0.0191 - val_mda: 0.5208 - val_rmse: 0.0203 - learning_rate: 5.0000e-04\n",
            "Epoch 20: early stopping\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "Validation Loss: 0.00004, RMSE: 0.00715, MDA: 0.52076, MAE: 0.00436\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0214 - mae: 0.0759 - mda: 0.5011 - rmse: 0.1342\n",
            "Epoch 1: val_loss improved from inf to 0.00106, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0214 - mae: 0.0759 - mda: 0.5011 - rmse: 0.1341 - val_loss: 0.0011 - val_mae: 0.0205 - val_mda: 0.5007 - val_rmse: 0.0224 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2859e-04 - mae: 0.0182 - mda: 0.5046 - rmse: 0.0797\n",
            "Epoch 2: val_loss improved from 0.00106 to 0.00090, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7.2824e-04 - mae: 0.0182 - mda: 0.5046 - rmse: 0.0797 - val_loss: 9.0408e-04 - val_mae: 0.0220 - val_mda: 0.5002 - val_rmse: 0.0234 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4520e-04 - mae: 0.0125 - mda: 0.5053 - rmse: 0.0776\n",
            "Epoch 3: val_loss improved from 0.00090 to 0.00016, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.4514e-04 - mae: 0.0125 - mda: 0.5053 - rmse: 0.0776 - val_loss: 1.6480e-04 - val_mae: 0.0085 - val_mda: 0.5067 - val_rmse: 0.0106 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2215e-04 - mae: 0.0102 - mda: 0.5066 - rmse: 0.0779\n",
            "Epoch 4: val_loss did not improve from 0.00016\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.2199e-04 - mae: 0.0101 - mda: 0.5066 - rmse: 0.0779 - val_loss: 1.7192e-04 - val_mae: 0.0091 - val_mda: 0.5047 - val_rmse: 0.0111 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6808e-04 - mae: 0.0089 - mda: 0.5050 - rmse: 0.0777\n",
            "Epoch 5: val_loss improved from 0.00016 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.6790e-04 - mae: 0.0089 - mda: 0.5050 - rmse: 0.0777 - val_loss: 3.8893e-05 - val_mae: 0.0049 - val_mda: 0.5053 - val_rmse: 0.0075 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2854e-04 - mae: 0.0078 - mda: 0.5057 - rmse: 0.0772\n",
            "Epoch 6: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.2840e-04 - mae: 0.0078 - mda: 0.5057 - rmse: 0.0772 - val_loss: 8.5976e-05 - val_mae: 0.0065 - val_mda: 0.5049 - val_rmse: 0.0088 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.5246e-05 - mae: 0.0069 - mda: 0.5083 - rmse: 0.0765\n",
            "Epoch 7: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 9.5286e-05 - mae: 0.0069 - mda: 0.5083 - rmse: 0.0765 - val_loss: 7.7424e-05 - val_mae: 0.0064 - val_mda: 0.5045 - val_rmse: 0.0088 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.7198e-05 - mae: 0.0066 - mda: 0.5073 - rmse: 0.0761\n",
            "Epoch 8: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 8.7171e-05 - mae: 0.0066 - mda: 0.5073 - rmse: 0.0761 - val_loss: 8.4112e-05 - val_mae: 0.0066 - val_mda: 0.5091 - val_rmse: 0.0090 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m564/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1476e-05 - mae: 0.0060 - mda: 0.5069 - rmse: 0.0773\n",
            "Epoch 9: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 7.1373e-05 - mae: 0.0060 - mda: 0.5069 - rmse: 0.0773 - val_loss: 1.3837e-04 - val_mae: 0.0089 - val_mda: 0.5115 - val_rmse: 0.0110 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m572/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3099e-05 - mae: 0.0061 - mda: 0.5086 - rmse: 0.0770\n",
            "Epoch 10: val_loss improved from 0.00004 to 0.00003, saving model to best_model.keras\n",
            "\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 7.2980e-05 - mae: 0.0060 - mda: 0.5085 - rmse: 0.0770 - val_loss: 2.5136e-05 - val_mae: 0.0039 - val_mda: 0.5087 - val_rmse: 0.0067 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m565/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3032e-05 - mae: 0.0051 - mda: 0.5094 - rmse: 0.0760\n",
            "Epoch 11: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.2944e-05 - mae: 0.0051 - mda: 0.5093 - rmse: 0.0761 - val_loss: 3.1302e-05 - val_mae: 0.0040 - val_mda: 0.5119 - val_rmse: 0.0069 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7456e-05 - mae: 0.0049 - mda: 0.5075 - rmse: 0.0765\n",
            "Epoch 12: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 4.7458e-05 - mae: 0.0049 - mda: 0.5075 - rmse: 0.0765 - val_loss: 3.3562e-05 - val_mae: 0.0043 - val_mda: 0.5147 - val_rmse: 0.0071 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6906e-05 - mae: 0.0050 - mda: 0.5077 - rmse: 0.0772\n",
            "Epoch 13: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 4.6903e-05 - mae: 0.0050 - mda: 0.5077 - rmse: 0.0772 - val_loss: 5.5971e-05 - val_mae: 0.0058 - val_mda: 0.5146 - val_rmse: 0.0083 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3769e-05 - mae: 0.0047 - mda: 0.5081 - rmse: 0.0768\n",
            "Epoch 14: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 4.3775e-05 - mae: 0.0047 - mda: 0.5081 - rmse: 0.0768 - val_loss: 1.3342e-04 - val_mae: 0.0090 - val_mda: 0.5121 - val_rmse: 0.0110 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m568/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3733e-05 - mae: 0.0048 - mda: 0.5066 - rmse: 0.0767\n",
            "Epoch 15: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 4.3701e-05 - mae: 0.0048 - mda: 0.5067 - rmse: 0.0767 - val_loss: 4.3756e-05 - val_mae: 0.0054 - val_mda: 0.5157 - val_rmse: 0.0081 - learning_rate: 5.0000e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3446e-05 - mae: 0.0042 - mda: 0.5091 - rmse: 0.0767\n",
            "Epoch 16: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.3441e-05 - mae: 0.0042 - mda: 0.5091 - rmse: 0.0767 - val_loss: 5.8225e-05 - val_mae: 0.0058 - val_mda: 0.5161 - val_rmse: 0.0083 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m565/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4127e-05 - mae: 0.0042 - mda: 0.5089 - rmse: 0.0776\n",
            "Epoch 17: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.4112e-05 - mae: 0.0042 - mda: 0.5088 - rmse: 0.0775 - val_loss: 2.6646e-05 - val_mae: 0.0041 - val_mda: 0.5151 - val_rmse: 0.0069 - learning_rate: 2.5000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3249e-05 - mae: 0.0041 - mda: 0.5083 - rmse: 0.0764\n",
            "Epoch 18: val_loss improved from 0.00003 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.3247e-05 - mae: 0.0041 - mda: 0.5083 - rmse: 0.0764 - val_loss: 2.3286e-05 - val_mae: 0.0035 - val_mda: 0.5158 - val_rmse: 0.0065 - learning_rate: 2.5000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1623e-05 - mae: 0.0041 - mda: 0.5078 - rmse: 0.0765\n",
            "Epoch 19: val_loss improved from 0.00002 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.1627e-05 - mae: 0.0041 - mda: 0.5078 - rmse: 0.0765 - val_loss: 1.8705e-05 - val_mae: 0.0033 - val_mda: 0.5187 - val_rmse: 0.0064 - learning_rate: 2.5000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m567/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3221e-05 - mae: 0.0042 - mda: 0.5103 - rmse: 0.0771\n",
            "Epoch 20: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.3200e-05 - mae: 0.0042 - mda: 0.5103 - rmse: 0.0771 - val_loss: 2.0765e-05 - val_mae: 0.0033 - val_mda: 0.5178 - val_rmse: 0.0064 - learning_rate: 2.5000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m573/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9670e-05 - mae: 0.0039 - mda: 0.5085 - rmse: 0.0773\n",
            "Epoch 21: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.9652e-05 - mae: 0.0039 - mda: 0.5085 - rmse: 0.0772 - val_loss: 4.9430e-05 - val_mae: 0.0051 - val_mda: 0.5187 - val_rmse: 0.0078 - learning_rate: 1.2500e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m565/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6776e-05 - mae: 0.0037 - mda: 0.5091 - rmse: 0.0762\n",
            "Epoch 22: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.6820e-05 - mae: 0.0037 - mda: 0.5091 - rmse: 0.0762 - val_loss: 1.9441e-05 - val_mae: 0.0033 - val_mda: 0.5199 - val_rmse: 0.0064 - learning_rate: 1.2500e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m568/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8363e-05 - mae: 0.0038 - mda: 0.5088 - rmse: 0.0768\n",
            "Epoch 23: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.8327e-05 - mae: 0.0038 - mda: 0.5088 - rmse: 0.0768 - val_loss: 6.2104e-05 - val_mae: 0.0057 - val_mda: 0.5189 - val_rmse: 0.0083 - learning_rate: 1.2500e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m565/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7244e-05 - mae: 0.0037 - mda: 0.5072 - rmse: 0.0767\n",
            "Epoch 24: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.7249e-05 - mae: 0.0037 - mda: 0.5072 - rmse: 0.0767 - val_loss: 7.9097e-05 - val_mae: 0.0070 - val_mda: 0.5195 - val_rmse: 0.0094 - learning_rate: 1.2500e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m563/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7775e-05 - mae: 0.0038 - mda: 0.5081 - rmse: 0.0768\n",
            "Epoch 25: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.7757e-05 - mae: 0.0038 - mda: 0.5081 - rmse: 0.0768 - val_loss: 7.6367e-05 - val_mae: 0.0063 - val_mda: 0.5177 - val_rmse: 0.0087 - learning_rate: 1.2500e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6372e-05 - mae: 0.0036 - mda: 0.5091 - rmse: 0.0767\n",
            "Epoch 26: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.6369e-05 - mae: 0.0036 - mda: 0.5091 - rmse: 0.0767 - val_loss: 1.9506e-05 - val_mae: 0.0032 - val_mda: 0.5190 - val_rmse: 0.0064 - learning_rate: 6.2500e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2662e-05 - mae: 0.0034 - mda: 0.5069 - rmse: 0.0775\n",
            "Epoch 27: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.2671e-05 - mae: 0.0034 - mda: 0.5069 - rmse: 0.0775 - val_loss: 6.2392e-05 - val_mae: 0.0059 - val_mda: 0.5199 - val_rmse: 0.0084 - learning_rate: 6.2500e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m573/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5808e-05 - mae: 0.0035 - mda: 0.5085 - rmse: 0.0761\n",
            "Epoch 28: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.5796e-05 - mae: 0.0035 - mda: 0.5085 - rmse: 0.0761 - val_loss: 9.1813e-05 - val_mae: 0.0072 - val_mda: 0.5190 - val_rmse: 0.0095 - learning_rate: 6.2500e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m564/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4085e-05 - mae: 0.0035 - mda: 0.5080 - rmse: 0.0768\n",
            "Epoch 29: val_loss improved from 0.00002 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.4062e-05 - mae: 0.0035 - mda: 0.5080 - rmse: 0.0768 - val_loss: 1.8348e-05 - val_mae: 0.0032 - val_mda: 0.5185 - val_rmse: 0.0064 - learning_rate: 6.2500e-05\n",
            "Epoch 30/50\n",
            "\u001b[1m568/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4318e-05 - mae: 0.0035 - mda: 0.5079 - rmse: 0.0768\n",
            "Epoch 30: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 30: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.4314e-05 - mae: 0.0035 - mda: 0.5079 - rmse: 0.0768 - val_loss: 3.0168e-05 - val_mae: 0.0038 - val_mda: 0.5196 - val_rmse: 0.0068 - learning_rate: 6.2500e-05\n",
            "Epoch 31/50\n",
            "\u001b[1m565/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4049e-05 - mae: 0.0034 - mda: 0.5088 - rmse: 0.0775\n",
            "Epoch 31: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.4020e-05 - mae: 0.0034 - mda: 0.5089 - rmse: 0.0775 - val_loss: 3.7157e-05 - val_mae: 0.0042 - val_mda: 0.5190 - val_rmse: 0.0071 - learning_rate: 3.1250e-05\n",
            "Epoch 32/50\n",
            "\u001b[1m566/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1270e-05 - mae: 0.0033 - mda: 0.5090 - rmse: 0.0764\n",
            "Epoch 32: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.1291e-05 - mae: 0.0033 - mda: 0.5090 - rmse: 0.0764 - val_loss: 3.2478e-05 - val_mae: 0.0039 - val_mda: 0.5212 - val_rmse: 0.0069 - learning_rate: 3.1250e-05\n",
            "Epoch 33/50\n",
            "\u001b[1m568/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2728e-05 - mae: 0.0034 - mda: 0.5084 - rmse: 0.0766\n",
            "Epoch 33: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.2715e-05 - mae: 0.0034 - mda: 0.5084 - rmse: 0.0766 - val_loss: 5.2564e-05 - val_mae: 0.0051 - val_mda: 0.5204 - val_rmse: 0.0078 - learning_rate: 3.1250e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34/50\n",
            "\u001b[1m568/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1611e-05 - mae: 0.0033 - mda: 0.5078 - rmse: 0.0762\n",
            "Epoch 34: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.1638e-05 - mae: 0.0033 - mda: 0.5077 - rmse: 0.0762 - val_loss: 3.0807e-05 - val_mae: 0.0038 - val_mda: 0.5191 - val_rmse: 0.0068 - learning_rate: 3.1250e-05\n",
            "Epoch 35/50\n",
            "\u001b[1m568/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1465e-05 - mae: 0.0033 - mda: 0.5077 - rmse: 0.0763\n",
            "Epoch 35: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 35: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.1472e-05 - mae: 0.0033 - mda: 0.5077 - rmse: 0.0763 - val_loss: 5.0587e-05 - val_mae: 0.0050 - val_mda: 0.5205 - val_rmse: 0.0077 - learning_rate: 3.1250e-05\n",
            "Epoch 36/50\n",
            "\u001b[1m568/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2717e-05 - mae: 0.0033 - mda: 0.5077 - rmse: 0.0771\n",
            "Epoch 36: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.2701e-05 - mae: 0.0033 - mda: 0.5077 - rmse: 0.0771 - val_loss: 2.5451e-05 - val_mae: 0.0035 - val_mda: 0.5201 - val_rmse: 0.0066 - learning_rate: 1.5625e-05\n",
            "Epoch 37/50\n",
            "\u001b[1m567/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3370e-05 - mae: 0.0034 - mda: 0.5093 - rmse: 0.0770\n",
            "Epoch 37: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.3326e-05 - mae: 0.0034 - mda: 0.5093 - rmse: 0.0770 - val_loss: 3.0357e-05 - val_mae: 0.0038 - val_mda: 0.5200 - val_rmse: 0.0068 - learning_rate: 1.5625e-05\n",
            "Epoch 38/50\n",
            "\u001b[1m567/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2544e-05 - mae: 0.0033 - mda: 0.5075 - rmse: 0.0767\n",
            "Epoch 38: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.2517e-05 - mae: 0.0033 - mda: 0.5075 - rmse: 0.0767 - val_loss: 3.6763e-05 - val_mae: 0.0041 - val_mda: 0.5196 - val_rmse: 0.0071 - learning_rate: 1.5625e-05\n",
            "Epoch 39/50\n",
            "\u001b[1m563/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2598e-05 - mae: 0.0033 - mda: 0.5092 - rmse: 0.0768\n",
            "Epoch 39: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.2568e-05 - mae: 0.0033 - mda: 0.5092 - rmse: 0.0768 - val_loss: 4.7196e-05 - val_mae: 0.0047 - val_mda: 0.5190 - val_rmse: 0.0075 - learning_rate: 1.5625e-05\n",
            "Epoch 39: early stopping\n",
            "Restoring model weights from the end of the best epoch: 29.\n",
            "Validation Loss: 0.00002, RMSE: 0.00635, MDA: 0.52119, MAE: 0.00319\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0175 - mae: 0.0696 - mda: 0.5021 - rmse: 0.1264\n",
            "Epoch 1: val_loss improved from inf to 0.00076, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0173 - mae: 0.0691 - mda: 0.5021 - rmse: 0.1258 - val_loss: 7.6391e-04 - val_mae: 0.0203 - val_mda: 0.5032 - val_rmse: 0.0221 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3340e-04 - mae: 0.0153 - mda: 0.5040 - rmse: 0.0789\n",
            "Epoch 2: val_loss improved from 0.00076 to 0.00009, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 5.3208e-04 - mae: 0.0153 - mda: 0.5040 - rmse: 0.0789 - val_loss: 8.5470e-05 - val_mae: 0.0067 - val_mda: 0.5029 - val_rmse: 0.0089 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6215e-04 - mae: 0.0106 - mda: 0.5069 - rmse: 0.0774\n",
            "Epoch 3: val_loss did not improve from 0.00009\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.6191e-04 - mae: 0.0106 - mda: 0.5069 - rmse: 0.0774 - val_loss: 1.8405e-04 - val_mae: 0.0104 - val_mda: 0.5045 - val_rmse: 0.0123 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7672e-04 - mae: 0.0087 - mda: 0.5064 - rmse: 0.0772\n",
            "Epoch 4: val_loss did not improve from 0.00009\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 1.7654e-04 - mae: 0.0087 - mda: 0.5064 - rmse: 0.0772 - val_loss: 8.8875e-05 - val_mae: 0.0073 - val_mda: 0.5102 - val_rmse: 0.0097 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m573/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1207e-04 - mae: 0.0074 - mda: 0.5056 - rmse: 0.0770\n",
            "Epoch 5: val_loss improved from 0.00009 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 1.1204e-04 - mae: 0.0074 - mda: 0.5056 - rmse: 0.0770 - val_loss: 2.5989e-05 - val_mae: 0.0038 - val_mda: 0.5087 - val_rmse: 0.0068 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.5201e-05 - mae: 0.0067 - mda: 0.5073 - rmse: 0.0770\n",
            "Epoch 6: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 9.5179e-05 - mae: 0.0067 - mda: 0.5073 - rmse: 0.0770 - val_loss: 9.5801e-05 - val_mae: 0.0077 - val_mda: 0.5137 - val_rmse: 0.0099 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.4082e-05 - mae: 0.0059 - mda: 0.5078 - rmse: 0.0771\n",
            "Epoch 7: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 7.4075e-05 - mae: 0.0059 - mda: 0.5078 - rmse: 0.0771 - val_loss: 3.5765e-05 - val_mae: 0.0047 - val_mda: 0.5087 - val_rmse: 0.0075 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.5666e-05 - mae: 0.0052 - mda: 0.5069 - rmse: 0.0769\n",
            "Epoch 8: val_loss improved from 0.00003 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 5.5625e-05 - mae: 0.0052 - mda: 0.5069 - rmse: 0.0769 - val_loss: 1.7944e-05 - val_mae: 0.0032 - val_mda: 0.5109 - val_rmse: 0.0064 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m572/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0799e-05 - mae: 0.0049 - mda: 0.5069 - rmse: 0.0768\n",
            "Epoch 9: val_loss improved from 0.00002 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 5.0802e-05 - mae: 0.0049 - mda: 0.5069 - rmse: 0.0768 - val_loss: 1.6153e-05 - val_mae: 0.0030 - val_mda: 0.5123 - val_rmse: 0.0063 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.9457e-05 - mae: 0.0048 - mda: 0.5084 - rmse: 0.0771\n",
            "Epoch 10: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 4.9408e-05 - mae: 0.0048 - mda: 0.5084 - rmse: 0.0771 - val_loss: 3.0932e-05 - val_mae: 0.0041 - val_mda: 0.5076 - val_rmse: 0.0072 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.3346e-05 - mae: 0.0046 - mda: 0.5083 - rmse: 0.0770\n",
            "Epoch 11: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 4.3318e-05 - mae: 0.0046 - mda: 0.5083 - rmse: 0.0770 - val_loss: 2.1456e-05 - val_mae: 0.0036 - val_mda: 0.5151 - val_rmse: 0.0069 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.9761e-05 - mae: 0.0045 - mda: 0.5073 - rmse: 0.0765\n",
            "Epoch 12: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 3.9778e-05 - mae: 0.0045 - mda: 0.5073 - rmse: 0.0765 - val_loss: 3.9643e-05 - val_mae: 0.0053 - val_mda: 0.5132 - val_rmse: 0.0082 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.3114e-05 - mae: 0.0041 - mda: 0.5076 - rmse: 0.0766\n",
            "Epoch 13: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 3.3107e-05 - mae: 0.0041 - mda: 0.5076 - rmse: 0.0766 - val_loss: 2.1130e-05 - val_mae: 0.0034 - val_mda: 0.5169 - val_rmse: 0.0066 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.0415e-05 - mae: 0.0038 - mda: 0.5088 - rmse: 0.0775\n",
            "Epoch 14: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 3.0406e-05 - mae: 0.0038 - mda: 0.5088 - rmse: 0.0775 - val_loss: 7.3150e-05 - val_mae: 0.0072 - val_mda: 0.5155 - val_rmse: 0.0096 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.9650e-05 - mae: 0.0039 - mda: 0.5084 - rmse: 0.0761\n",
            "Epoch 15: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.9668e-05 - mae: 0.0039 - mda: 0.5084 - rmse: 0.0761 - val_loss: 4.3034e-05 - val_mae: 0.0057 - val_mda: 0.5159 - val_rmse: 0.0084 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1077e-05 - mae: 0.0040 - mda: 0.5069 - rmse: 0.0767\n",
            "Epoch 16: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 3.1058e-05 - mae: 0.0040 - mda: 0.5069 - rmse: 0.0767 - val_loss: 2.4071e-05 - val_mae: 0.0037 - val_mda: 0.5154 - val_rmse: 0.0068 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7041e-05 - mae: 0.0037 - mda: 0.5071 - rmse: 0.0769\n",
            "Epoch 17: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.7034e-05 - mae: 0.0037 - mda: 0.5072 - rmse: 0.0769 - val_loss: 2.4158e-05 - val_mae: 0.0038 - val_mda: 0.5160 - val_rmse: 0.0069 - learning_rate: 2.5000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m573/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1795e-05 - mae: 0.0033 - mda: 0.5084 - rmse: 0.0762\n",
            "Epoch 18: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.1800e-05 - mae: 0.0033 - mda: 0.5084 - rmse: 0.0762 - val_loss: 1.6700e-05 - val_mae: 0.0030 - val_mda: 0.5165 - val_rmse: 0.0063 - learning_rate: 1.2500e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0873e-05 - mae: 0.0033 - mda: 0.5077 - rmse: 0.0777\n",
            "Epoch 19: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.0887e-05 - mae: 0.0033 - mda: 0.5077 - rmse: 0.0777 - val_loss: 2.7729e-05 - val_mae: 0.0038 - val_mda: 0.5165 - val_rmse: 0.0069 - learning_rate: 1.2500e-04\n",
            "Epoch 19: early stopping\n",
            "Restoring model weights from the end of the best epoch: 9.\n",
            "Validation Loss: 0.00002, RMSE: 0.00626, MDA: 0.51691, MAE: 0.00299\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0260 - mae: 0.0714 - mda: 0.5018 - rmse: 0.1260\n",
            "Epoch 1: val_loss improved from inf to 0.00020, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0259 - mae: 0.0713 - mda: 0.5018 - rmse: 0.1259 - val_loss: 1.9850e-04 - val_mae: 0.0105 - val_mda: 0.5038 - val_rmse: 0.0126 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6768e-04 - mae: 0.0124 - mda: 0.5040 - rmse: 0.0774\n",
            "Epoch 2: val_loss did not improve from 0.00020\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6764e-04 - mae: 0.0124 - mda: 0.5040 - rmse: 0.0774 - val_loss: 2.4434e-04 - val_mae: 0.0107 - val_mda: 0.5089 - val_rmse: 0.0127 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m558/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6503e-04 - mae: 0.0097 - mda: 0.5060 - rmse: 0.0766\n",
            "Epoch 3: val_loss did not improve from 0.00020\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6422e-04 - mae: 0.0097 - mda: 0.5060 - rmse: 0.0766 - val_loss: 5.9112e-04 - val_mae: 0.0180 - val_mda: 0.5095 - val_rmse: 0.0195 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1723e-04 - mae: 0.0080 - mda: 0.5059 - rmse: 0.0784\n",
            "Epoch 4: val_loss did not improve from 0.00020\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1720e-04 - mae: 0.0080 - mda: 0.5059 - rmse: 0.0784 - val_loss: 2.6878e-04 - val_mae: 0.0110 - val_mda: 0.5113 - val_rmse: 0.0130 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m560/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.8757e-05 - mae: 0.0073 - mda: 0.5066 - rmse: 0.0766\n",
            "Epoch 5: val_loss did not improve from 0.00020\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.8492e-05 - mae: 0.0073 - mda: 0.5067 - rmse: 0.0766 - val_loss: 2.0115e-04 - val_mae: 0.0088 - val_mda: 0.5136 - val_rmse: 0.0109 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m556/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8658e-05 - mae: 0.0065 - mda: 0.5064 - rmse: 0.0770\n",
            "Epoch 6: val_loss did not improve from 0.00020\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.8504e-05 - mae: 0.0065 - mda: 0.5065 - rmse: 0.0770 - val_loss: 4.7280e-04 - val_mae: 0.0154 - val_mda: 0.5118 - val_rmse: 0.0170 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6557e-05 - mae: 0.0059 - mda: 0.5070 - rmse: 0.0768\n",
            "Epoch 7: val_loss did not improve from 0.00020\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.6546e-05 - mae: 0.0059 - mda: 0.5070 - rmse: 0.0768 - val_loss: 3.2123e-04 - val_mae: 0.0111 - val_mda: 0.5167 - val_rmse: 0.0131 - learning_rate: 0.0050\n",
            "Epoch 8/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8026e-05 - mae: 0.0055 - mda: 0.5092 - rmse: 0.0752\n",
            "Epoch 8: val_loss improved from 0.00020 to 0.00013, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.8083e-05 - mae: 0.0055 - mda: 0.5092 - rmse: 0.0752 - val_loss: 1.2779e-04 - val_mae: 0.0079 - val_mda: 0.5157 - val_rmse: 0.0101 - learning_rate: 0.0050\n",
            "Epoch 9/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5004e-05 - mae: 0.0059 - mda: 0.5076 - rmse: 0.0760\n",
            "Epoch 9: val_loss did not improve from 0.00013\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.4997e-05 - mae: 0.0059 - mda: 0.5076 - rmse: 0.0760 - val_loss: 3.1359e-04 - val_mae: 0.0110 - val_mda: 0.5174 - val_rmse: 0.0130 - learning_rate: 0.0050\n",
            "Epoch 10/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m559/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0389e-05 - mae: 0.0058 - mda: 0.5082 - rmse: 0.0770\n",
            "Epoch 10: val_loss did not improve from 0.00013\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.0275e-05 - mae: 0.0057 - mda: 0.5082 - rmse: 0.0770 - val_loss: 2.5792e-04 - val_mae: 0.0097 - val_mda: 0.5180 - val_rmse: 0.0119 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m558/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1736e-05 - mae: 0.0058 - mda: 0.5070 - rmse: 0.0779\n",
            "Epoch 11: val_loss did not improve from 0.00013\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.1631e-05 - mae: 0.0058 - mda: 0.5070 - rmse: 0.0778 - val_loss: 5.9870e-04 - val_mae: 0.0187 - val_mda: 0.5178 - val_rmse: 0.0200 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m560/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5672e-05 - mae: 0.0054 - mda: 0.5058 - rmse: 0.0765\n",
            "Epoch 12: val_loss did not improve from 0.00013\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5527e-05 - mae: 0.0054 - mda: 0.5058 - rmse: 0.0765 - val_loss: 2.9216e-04 - val_mae: 0.0107 - val_mda: 0.5162 - val_rmse: 0.0128 - learning_rate: 0.0025\n",
            "Epoch 13/50\n",
            "\u001b[1m558/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0594e-05 - mae: 0.0051 - mda: 0.5073 - rmse: 0.0770\n",
            "Epoch 13: val_loss did not improve from 0.00013\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.0668e-05 - mae: 0.0051 - mda: 0.5073 - rmse: 0.0770 - val_loss: 4.1191e-04 - val_mae: 0.0120 - val_mda: 0.5198 - val_rmse: 0.0140 - learning_rate: 0.0025\n",
            "Epoch 14/50\n",
            "\u001b[1m560/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2754e-05 - mae: 0.0052 - mda: 0.5090 - rmse: 0.0771\n",
            "Epoch 14: val_loss did not improve from 0.00013\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.2873e-05 - mae: 0.0052 - mda: 0.5089 - rmse: 0.0771 - val_loss: 1.4424e-04 - val_mae: 0.0089 - val_mda: 0.5177 - val_rmse: 0.0110 - learning_rate: 0.0025\n",
            "Epoch 15/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2737e-05 - mae: 0.0053 - mda: 0.5069 - rmse: 0.0759\n",
            "Epoch 15: val_loss did not improve from 0.00013\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.2739e-05 - mae: 0.0053 - mda: 0.5069 - rmse: 0.0759 - val_loss: 5.8534e-04 - val_mae: 0.0144 - val_mda: 0.5191 - val_rmse: 0.0163 - learning_rate: 0.0025\n",
            "Epoch 16/50\n",
            "\u001b[1m560/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8367e-05 - mae: 0.0055 - mda: 0.5067 - rmse: 0.0763\n",
            "Epoch 16: val_loss did not improve from 0.00013\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.8244e-05 - mae: 0.0055 - mda: 0.5067 - rmse: 0.0763 - val_loss: 3.2657e-04 - val_mae: 0.0108 - val_mda: 0.5184 - val_rmse: 0.0130 - learning_rate: 0.0025\n",
            "Epoch 17/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6212e-05 - mae: 0.0048 - mda: 0.5075 - rmse: 0.0764\n",
            "Epoch 17: val_loss did not improve from 0.00013\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.6216e-05 - mae: 0.0048 - mda: 0.5075 - rmse: 0.0764 - val_loss: 5.8543e-04 - val_mae: 0.0158 - val_mda: 0.5220 - val_rmse: 0.0175 - learning_rate: 0.0012\n",
            "Epoch 18/50\n",
            "\u001b[1m560/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5562e-05 - mae: 0.0049 - mda: 0.5073 - rmse: 0.0770\n",
            "Epoch 18: val_loss did not improve from 0.00013\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.5575e-05 - mae: 0.0049 - mda: 0.5073 - rmse: 0.0770 - val_loss: 0.0019 - val_mae: 0.0327 - val_mda: 0.5196 - val_rmse: 0.0336 - learning_rate: 0.0012\n",
            "Epoch 18: early stopping\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "Validation Loss: 0.00013, RMSE: 0.01012, MDA: 0.52198, MAE: 0.00792\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1335 - mae: 0.2191 - mda: 0.5002 - rmse: 0.2829\n",
            "Epoch 1: val_loss improved from inf to 0.01066, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1311 - mae: 0.2159 - mda: 0.5003 - rmse: 0.2794 - val_loss: 0.0107 - val_mae: 0.0630 - val_mda: 0.5022 - val_rmse: 0.0639 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m564/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4673e-04 - mae: 0.0163 - mda: 0.5044 - rmse: 0.0765\n",
            "Epoch 2: val_loss did not improve from 0.01066\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 4.4864e-04 - mae: 0.0163 - mda: 0.5043 - rmse: 0.0765 - val_loss: 0.0154 - val_mae: 0.0855 - val_mda: 0.4999 - val_rmse: 0.0861 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.3274e-04 - mae: 0.0182 - mda: 0.5035 - rmse: 0.0766\n",
            "Epoch 3: val_loss improved from 0.01066 to 0.00899, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.3276e-04 - mae: 0.0182 - mda: 0.5035 - rmse: 0.0767 - val_loss: 0.0090 - val_mae: 0.0651 - val_mda: 0.5000 - val_rmse: 0.0657 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m565/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5832e-04 - mae: 0.0185 - mda: 0.5045 - rmse: 0.0771\n",
            "Epoch 4: val_loss did not improve from 0.00899\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.5786e-04 - mae: 0.0185 - mda: 0.5045 - rmse: 0.0771 - val_loss: 0.0098 - val_mae: 0.0676 - val_mda: 0.4984 - val_rmse: 0.0681 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m567/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6312e-04 - mae: 0.0199 - mda: 0.5049 - rmse: 0.0773\n",
            "Epoch 5: val_loss improved from 0.00899 to 0.00831, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.6119e-04 - mae: 0.0199 - mda: 0.5049 - rmse: 0.0773 - val_loss: 0.0083 - val_mae: 0.0539 - val_mda: 0.5035 - val_rmse: 0.0549 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m563/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.2348e-04 - mae: 0.0194 - mda: 0.5039 - rmse: 0.0776\n",
            "Epoch 6: val_loss did not improve from 0.00831\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.2320e-04 - mae: 0.0194 - mda: 0.5040 - rmse: 0.0776 - val_loss: 0.0084 - val_mae: 0.0541 - val_mda: 0.5030 - val_rmse: 0.0551 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m567/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3866e-04 - mae: 0.0142 - mda: 0.5064 - rmse: 0.0764\n",
            "Epoch 7: val_loss improved from 0.00831 to 0.00634, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.3701e-04 - mae: 0.0142 - mda: 0.5064 - rmse: 0.0764 - val_loss: 0.0063 - val_mae: 0.0464 - val_mda: 0.5085 - val_rmse: 0.0474 - learning_rate: 0.0100\n",
            "Epoch 8/50\n",
            "\u001b[1m566/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8466e-04 - mae: 0.0131 - mda: 0.5065 - rmse: 0.0760\n",
            "Epoch 8: val_loss improved from 0.00634 to 0.00419, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.8425e-04 - mae: 0.0131 - mda: 0.5064 - rmse: 0.0761 - val_loss: 0.0042 - val_mae: 0.0352 - val_mda: 0.5102 - val_rmse: 0.0364 - learning_rate: 0.0100\n",
            "Epoch 9/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m566/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3596e-04 - mae: 0.0089 - mda: 0.5062 - rmse: 0.0757\n",
            "Epoch 9: val_loss did not improve from 0.00419\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.3582e-04 - mae: 0.0089 - mda: 0.5062 - rmse: 0.0757 - val_loss: 0.0057 - val_mae: 0.0467 - val_mda: 0.5151 - val_rmse: 0.0477 - learning_rate: 0.0100\n",
            "Epoch 10/50\n",
            "\u001b[1m566/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2152e-04 - mae: 0.0083 - mda: 0.5077 - rmse: 0.0764\n",
            "Epoch 10: val_loss improved from 0.00419 to 0.00209, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.2132e-04 - mae: 0.0083 - mda: 0.5077 - rmse: 0.0764 - val_loss: 0.0021 - val_mae: 0.0245 - val_mda: 0.5209 - val_rmse: 0.0261 - learning_rate: 0.0100\n",
            "Epoch 11/50\n",
            "\u001b[1m567/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.4685e-05 - mae: 0.0064 - mda: 0.5061 - rmse: 0.0769\n",
            "Epoch 11: val_loss did not improve from 0.00209\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 7.4648e-05 - mae: 0.0064 - mda: 0.5061 - rmse: 0.0769 - val_loss: 0.0024 - val_mae: 0.0279 - val_mda: 0.5179 - val_rmse: 0.0294 - learning_rate: 0.0100\n",
            "Epoch 12/50\n",
            "\u001b[1m566/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.0786e-05 - mae: 0.0066 - mda: 0.5065 - rmse: 0.0761\n",
            "Epoch 12: val_loss improved from 0.00209 to 0.00137, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 8.0404e-05 - mae: 0.0066 - mda: 0.5065 - rmse: 0.0762 - val_loss: 0.0014 - val_mae: 0.0220 - val_mda: 0.5148 - val_rmse: 0.0236 - learning_rate: 0.0100\n",
            "Epoch 13/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7417e-05 - mae: 0.0061 - mda: 0.5059 - rmse: 0.0767\n",
            "Epoch 13: val_loss improved from 0.00137 to 0.00127, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.7343e-05 - mae: 0.0061 - mda: 0.5059 - rmse: 0.0767 - val_loss: 0.0013 - val_mae: 0.0257 - val_mda: 0.5160 - val_rmse: 0.0269 - learning_rate: 0.0100\n",
            "Epoch 14/50\n",
            "\u001b[1m567/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5677e-05 - mae: 0.0055 - mda: 0.5094 - rmse: 0.0774\n",
            "Epoch 14: val_loss improved from 0.00127 to 0.00052, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.5564e-05 - mae: 0.0055 - mda: 0.5094 - rmse: 0.0774 - val_loss: 5.1873e-04 - val_mae: 0.0153 - val_mda: 0.5166 - val_rmse: 0.0171 - learning_rate: 0.0100\n",
            "Epoch 15/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0503e-05 - mae: 0.0057 - mda: 0.5078 - rmse: 0.0758\n",
            "Epoch 15: val_loss improved from 0.00052 to 0.00014, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.0660e-05 - mae: 0.0057 - mda: 0.5078 - rmse: 0.0758 - val_loss: 1.4052e-04 - val_mae: 0.0074 - val_mda: 0.5145 - val_rmse: 0.0100 - learning_rate: 0.0100\n",
            "Epoch 16/50\n",
            "\u001b[1m567/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1066e-05 - mae: 0.0053 - mda: 0.5080 - rmse: 0.0767\n",
            "Epoch 16: val_loss did not improve from 0.00014\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.1084e-05 - mae: 0.0053 - mda: 0.5080 - rmse: 0.0767 - val_loss: 6.4903e-04 - val_mae: 0.0170 - val_mda: 0.5161 - val_rmse: 0.0188 - learning_rate: 0.0100\n",
            "Epoch 17/50\n",
            "\u001b[1m564/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4235e-05 - mae: 0.0059 - mda: 0.5079 - rmse: 0.0771\n",
            "Epoch 17: val_loss did not improve from 0.00014\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.4052e-05 - mae: 0.0058 - mda: 0.5079 - rmse: 0.0771 - val_loss: 2.3469e-04 - val_mae: 0.0144 - val_mda: 0.5184 - val_rmse: 0.0162 - learning_rate: 0.0100\n",
            "Epoch 18/50\n",
            "\u001b[1m567/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1295e-05 - mae: 0.0063 - mda: 0.5057 - rmse: 0.0775\n",
            "Epoch 18: val_loss did not improve from 0.00014\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 7.1199e-05 - mae: 0.0063 - mda: 0.5057 - rmse: 0.0775 - val_loss: 3.9429e-04 - val_mae: 0.0171 - val_mda: 0.5134 - val_rmse: 0.0186 - learning_rate: 0.0100\n",
            "Epoch 19/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1091e-05 - mae: 0.0067 - mda: 0.5080 - rmse: 0.0767\n",
            "Epoch 19: val_loss did not improve from 0.00014\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 8.1098e-05 - mae: 0.0067 - mda: 0.5080 - rmse: 0.0766 - val_loss: 2.4303e-04 - val_mae: 0.0142 - val_mda: 0.5143 - val_rmse: 0.0161 - learning_rate: 0.0100\n",
            "Epoch 20/50\n",
            "\u001b[1m567/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8114e-05 - mae: 0.0061 - mda: 0.5084 - rmse: 0.0762\n",
            "Epoch 20: val_loss did not improve from 0.00014\n",
            "\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.8472e-05 - mae: 0.0061 - mda: 0.5084 - rmse: 0.0762 - val_loss: 2.8329e-04 - val_mae: 0.0122 - val_mda: 0.5066 - val_rmse: 0.0144 - learning_rate: 0.0100\n",
            "Epoch 21/50\n",
            "\u001b[1m563/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8806e-05 - mae: 0.0052 - mda: 0.5083 - rmse: 0.0760\n",
            "Epoch 21: val_loss did not improve from 0.00014\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 4.8641e-05 - mae: 0.0052 - mda: 0.5083 - rmse: 0.0760 - val_loss: 1.9049e-04 - val_mae: 0.0130 - val_mda: 0.5150 - val_rmse: 0.0150 - learning_rate: 0.0050\n",
            "Epoch 22/50\n",
            "\u001b[1m564/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3328e-05 - mae: 0.0053 - mda: 0.5075 - rmse: 0.0763\n",
            "Epoch 22: val_loss improved from 0.00014 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.3723e-05 - mae: 0.0053 - mda: 0.5074 - rmse: 0.0763 - val_loss: 1.8236e-05 - val_mae: 0.0032 - val_mda: 0.5117 - val_rmse: 0.0071 - learning_rate: 0.0050\n",
            "Epoch 23/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2659e-05 - mae: 0.0061 - mda: 0.5083 - rmse: 0.0757\n",
            "Epoch 23: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 7.2559e-05 - mae: 0.0061 - mda: 0.5083 - rmse: 0.0757 - val_loss: 3.4746e-04 - val_mae: 0.0136 - val_mda: 0.5114 - val_rmse: 0.0156 - learning_rate: 0.0050\n",
            "Epoch 24/50\n",
            "\u001b[1m564/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0596e-05 - mae: 0.0053 - mda: 0.5070 - rmse: 0.0768\n",
            "Epoch 24: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.0835e-05 - mae: 0.0053 - mda: 0.5071 - rmse: 0.0768 - val_loss: 4.2044e-04 - val_mae: 0.0190 - val_mda: 0.5073 - val_rmse: 0.0208 - learning_rate: 0.0050\n",
            "Epoch 25/50\n",
            "\u001b[1m566/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0028 - mae: 0.0148 - mda: 0.5076 - rmse: 0.0813\n",
            "Epoch 25: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0030 - mae: 0.0155 - mda: 0.5075 - rmse: 0.0815 - val_loss: 0.0204 - val_mae: 0.1075 - val_mda: 0.5160 - val_rmse: 0.1080 - learning_rate: 0.0050\n",
            "Epoch 26/50\n",
            "\u001b[1m563/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0047 - mae: 0.0560 - mda: 0.5002 - rmse: 0.0693\n",
            "Epoch 26: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0047 - mae: 0.0560 - mda: 0.5002 - rmse: 0.0693 - val_loss: 0.0282 - val_mae: 0.1374 - val_mda: 0.4942 - val_rmse: 0.1378 - learning_rate: 0.0050\n",
            "Epoch 27/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m566/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0048 - mae: 0.0566 - mda: 0.4998 - rmse: 0.0677\n",
            "Epoch 27: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0048 - mae: 0.0566 - mda: 0.4998 - rmse: 0.0677 - val_loss: 0.0371 - val_mae: 0.1667 - val_mda: 0.4962 - val_rmse: 0.1671 - learning_rate: 0.0050\n",
            "Epoch 28/50\n",
            "\u001b[1m568/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - mae: 0.0519 - mda: 0.4988 - rmse: 0.0612\n",
            "Epoch 28: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0038 - mae: 0.0519 - mda: 0.4989 - rmse: 0.0612 - val_loss: 0.0417 - val_mae: 0.1827 - val_mda: 0.5093 - val_rmse: 0.1830 - learning_rate: 0.0025\n",
            "Epoch 29/50\n",
            "\u001b[1m567/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0020 - mae: 0.0341 - mda: 0.5023 - rmse: 0.0693\n",
            "Epoch 29: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0020 - mae: 0.0338 - mda: 0.5023 - rmse: 0.0694 - val_loss: 0.0151 - val_mae: 0.0919 - val_mda: 0.5239 - val_rmse: 0.0924 - learning_rate: 0.0025\n",
            "Epoch 30/50\n",
            "\u001b[1m567/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1606e-04 - mae: 0.0205 - mda: 0.5028 - rmse: 0.0760\n",
            "Epoch 30: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 7.1096e-04 - mae: 0.0204 - mda: 0.5028 - rmse: 0.0760 - val_loss: 0.0124 - val_mae: 0.0715 - val_mda: 0.5217 - val_rmse: 0.0722 - learning_rate: 0.0025\n",
            "Epoch 31/50\n",
            "\u001b[1m567/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0239e-04 - mae: 0.0174 - mda: 0.5042 - rmse: 0.0760\n",
            "Epoch 31: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.0234e-04 - mae: 0.0174 - mda: 0.5042 - rmse: 0.0760 - val_loss: 0.0139 - val_mae: 0.0785 - val_mda: 0.5230 - val_rmse: 0.0791 - learning_rate: 0.0025\n",
            "Epoch 32/50\n",
            "\u001b[1m567/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1034e-04 - mae: 0.0176 - mda: 0.5037 - rmse: 0.0771\n",
            "Epoch 32: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.0819e-04 - mae: 0.0176 - mda: 0.5038 - rmse: 0.0771 - val_loss: 0.0123 - val_mae: 0.0701 - val_mda: 0.5229 - val_rmse: 0.0708 - learning_rate: 0.0025\n",
            "Epoch 32: early stopping\n",
            "Restoring model weights from the end of the best epoch: 22.\n",
            "Validation Loss: 0.00002, RMSE: 0.00709, MDA: 0.52392, MAE: 0.00320\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2470 - mae: 0.3021 - mda: 0.4995 - rmse: 0.3602\n",
            "Epoch 1: val_loss improved from inf to 0.03739, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.2461 - mae: 0.3013 - mda: 0.4995 - rmse: 0.3593 - val_loss: 0.0374 - val_mae: 0.1675 - val_mda: 0.5016 - val_rmse: 0.1679 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0054 - mae: 0.0598 - mda: 0.5006 - rmse: 0.0715\n",
            "Epoch 2: val_loss improved from 0.03739 to 0.02632, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0055 - mae: 0.0598 - mda: 0.5006 - rmse: 0.0716 - val_loss: 0.0263 - val_mae: 0.1305 - val_mda: 0.4949 - val_rmse: 0.1309 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0078 - mae: 0.0710 - mda: 0.4997 - rmse: 0.0849\n",
            "Epoch 3: val_loss did not improve from 0.02632\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0078 - mae: 0.0709 - mda: 0.4997 - rmse: 0.0848 - val_loss: 0.0445 - val_mae: 0.1879 - val_mda: 0.5009 - val_rmse: 0.1882 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0063 - mae: 0.0635 - mda: 0.5013 - rmse: 0.0775\n",
            "Epoch 4: val_loss improved from 0.02632 to 0.02599, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0063 - mae: 0.0635 - mda: 0.5012 - rmse: 0.0775 - val_loss: 0.0260 - val_mae: 0.1297 - val_mda: 0.4991 - val_rmse: 0.1301 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0094 - mae: 0.0785 - mda: 0.5009 - rmse: 0.0932\n",
            "Epoch 5: val_loss did not improve from 0.02599\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0094 - mae: 0.0784 - mda: 0.5010 - rmse: 0.0931 - val_loss: 0.0284 - val_mae: 0.1389 - val_mda: 0.4952 - val_rmse: 0.1393 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0081 - mae: 0.0723 - mda: 0.5003 - rmse: 0.0879\n",
            "Epoch 6: val_loss did not improve from 0.02599\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0081 - mae: 0.0723 - mda: 0.5003 - rmse: 0.0879 - val_loss: 0.0342 - val_mae: 0.1634 - val_mda: 0.5158 - val_rmse: 0.1637 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0067 - mae: 0.0652 - mda: 0.4992 - rmse: 0.0873\n",
            "Epoch 7: val_loss improved from 0.02599 to 0.01575, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0067 - mae: 0.0651 - mda: 0.4992 - rmse: 0.0873 - val_loss: 0.0158 - val_mae: 0.0906 - val_mda: 0.5192 - val_rmse: 0.0912 - learning_rate: 0.0100\n",
            "Epoch 8/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0029 - mae: 0.0380 - mda: 0.5028 - rmse: 0.0844\n",
            "Epoch 8: val_loss improved from 0.01575 to 0.01145, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0029 - mae: 0.0380 - mda: 0.5028 - rmse: 0.0844 - val_loss: 0.0114 - val_mae: 0.0673 - val_mda: 0.5223 - val_rmse: 0.0681 - learning_rate: 0.0100\n",
            "Epoch 9/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0014 - mae: 0.0287 - mda: 0.5049 - rmse: 0.0816\n",
            "Epoch 9: val_loss improved from 0.01145 to 0.00794, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0014 - mae: 0.0288 - mda: 0.5049 - rmse: 0.0816 - val_loss: 0.0079 - val_mae: 0.0648 - val_mda: 0.5197 - val_rmse: 0.0653 - learning_rate: 0.0100\n",
            "Epoch 10/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0140 - mae: 0.0883 - mda: 0.4994 - rmse: 0.1168\n",
            "Epoch 10: val_loss did not improve from 0.00794\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0140 - mae: 0.0884 - mda: 0.4994 - rmse: 0.1168 - val_loss: 0.0377 - val_mae: 0.1693 - val_mda: 0.5081 - val_rmse: 0.1697 - learning_rate: 0.0100\n",
            "Epoch 11/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0082 - mae: 0.0725 - mda: 0.5006 - rmse: 0.0885\n",
            "Epoch 11: val_loss did not improve from 0.00794\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0082 - mae: 0.0724 - mda: 0.5006 - rmse: 0.0885 - val_loss: 0.0144 - val_mae: 0.0862 - val_mda: 0.5219 - val_rmse: 0.0868 - learning_rate: 0.0100\n",
            "Epoch 12/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.6147e-04 - mae: 0.0240 - mda: 0.5045 - rmse: 0.0782\n",
            "Epoch 12: val_loss did not improve from 0.00794\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 9.6804e-04 - mae: 0.0241 - mda: 0.5045 - rmse: 0.0782 - val_loss: 0.0123 - val_mae: 0.0700 - val_mda: 0.5182 - val_rmse: 0.0709 - learning_rate: 0.0100\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0011 - mae: 0.0262 - mda: 0.5049 - rmse: 0.0812\n",
            "Epoch 13: val_loss did not improve from 0.00794\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0011 - mae: 0.0262 - mda: 0.5049 - rmse: 0.0812 - val_loss: 0.0522 - val_mae: 0.1530 - val_mda: 0.4744 - val_rmse: 0.1536 - learning_rate: 0.0100\n",
            "Epoch 14/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0239 - mae: 0.1178 - mda: 0.5012 - rmse: 0.1359\n",
            "Epoch 14: val_loss did not improve from 0.00794\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0238 - mae: 0.1175 - mda: 0.5012 - rmse: 0.1355 - val_loss: 0.0363 - val_mae: 0.1657 - val_mda: 0.5142 - val_rmse: 0.1660 - learning_rate: 0.0100\n",
            "Epoch 15/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0021 - mae: 0.0349 - mda: 0.5017 - rmse: 0.0755\n",
            "Epoch 15: val_loss did not improve from 0.00794\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0021 - mae: 0.0348 - mda: 0.5017 - rmse: 0.0755 - val_loss: 0.0148 - val_mae: 0.0820 - val_mda: 0.5098 - val_rmse: 0.0826 - learning_rate: 0.0050\n",
            "Epoch 16/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.5509e-04 - mae: 0.0245 - mda: 0.5033 - rmse: 0.0775\n",
            "Epoch 16: val_loss did not improve from 0.00794\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 9.5380e-04 - mae: 0.0244 - mda: 0.5033 - rmse: 0.0775 - val_loss: 0.0163 - val_mae: 0.0879 - val_mda: 0.5160 - val_rmse: 0.0887 - learning_rate: 0.0050\n",
            "Epoch 17/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.7282e-04 - mae: 0.0217 - mda: 0.5035 - rmse: 0.0786\n",
            "Epoch 17: val_loss did not improve from 0.00794\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 7.7694e-04 - mae: 0.0218 - mda: 0.5035 - rmse: 0.0786 - val_loss: 0.0200 - val_mae: 0.1061 - val_mda: 0.5183 - val_rmse: 0.1067 - learning_rate: 0.0050\n",
            "Epoch 18/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0014 - mae: 0.0291 - mda: 0.5063 - rmse: 0.0801\n",
            "Epoch 18: val_loss did not improve from 0.00794\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0014 - mae: 0.0291 - mda: 0.5063 - rmse: 0.0801 - val_loss: 0.0155 - val_mae: 0.0888 - val_mda: 0.5195 - val_rmse: 0.0894 - learning_rate: 0.0050\n",
            "Epoch 19/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0011 - mae: 0.0272 - mda: 0.5037 - rmse: 0.0810\n",
            "Epoch 19: val_loss did not improve from 0.00794\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0011 - mae: 0.0272 - mda: 0.5037 - rmse: 0.0810 - val_loss: 0.0081 - val_mae: 0.0567 - val_mda: 0.5177 - val_rmse: 0.0575 - learning_rate: 0.0050\n",
            "Epoch 19: early stopping\n",
            "Restoring model weights from the end of the best epoch: 9.\n",
            "Validation Loss: 0.00794, RMSE: 0.05754, MDA: 0.52232, MAE: 0.05666\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0193 - mae: 0.0853 - mda: 0.5011 - rmse: 0.1413\n",
            "Epoch 1: val_loss improved from inf to 0.00049, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0193 - mae: 0.0852 - mda: 0.5011 - rmse: 0.1412 - val_loss: 4.8731e-04 - val_mae: 0.0170 - val_mda: 0.5027 - val_rmse: 0.0195 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m563/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0011 - mae: 0.0224 - mda: 0.5036 - rmse: 0.0818\n",
            "Epoch 2: val_loss improved from 0.00049 to 0.00022, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - mae: 0.0223 - mda: 0.5036 - rmse: 0.0817 - val_loss: 2.1844e-04 - val_mae: 0.0108 - val_mda: 0.5080 - val_rmse: 0.0132 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m564/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8653e-04 - mae: 0.0150 - mda: 0.5049 - rmse: 0.0778\n",
            "Epoch 3: val_loss did not improve from 0.00022\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.8407e-04 - mae: 0.0149 - mda: 0.5049 - rmse: 0.0778 - val_loss: 2.7578e-04 - val_mae: 0.0124 - val_mda: 0.5082 - val_rmse: 0.0144 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m568/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9769e-04 - mae: 0.0118 - mda: 0.5057 - rmse: 0.0770\n",
            "Epoch 4: val_loss improved from 0.00022 to 0.00018, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.9693e-04 - mae: 0.0118 - mda: 0.5057 - rmse: 0.0770 - val_loss: 1.8229e-04 - val_mae: 0.0098 - val_mda: 0.5095 - val_rmse: 0.0118 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m563/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3159e-04 - mae: 0.0104 - mda: 0.5063 - rmse: 0.0767\n",
            "Epoch 5: val_loss did not improve from 0.00018\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3111e-04 - mae: 0.0104 - mda: 0.5063 - rmse: 0.0767 - val_loss: 2.0287e-04 - val_mae: 0.0106 - val_mda: 0.5115 - val_rmse: 0.0125 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m564/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8350e-04 - mae: 0.0094 - mda: 0.5068 - rmse: 0.0770\n",
            "Epoch 6: val_loss improved from 0.00018 to 0.00017, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.8310e-04 - mae: 0.0094 - mda: 0.5067 - rmse: 0.0770 - val_loss: 1.7060e-04 - val_mae: 0.0091 - val_mda: 0.5135 - val_rmse: 0.0112 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m567/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4535e-04 - mae: 0.0086 - mda: 0.5078 - rmse: 0.0758\n",
            "Epoch 7: val_loss did not improve from 0.00017\n",
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4510e-04 - mae: 0.0086 - mda: 0.5078 - rmse: 0.0758 - val_loss: 5.1029e-04 - val_mae: 0.0157 - val_mda: 0.5162 - val_rmse: 0.0172 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m563/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3347e-04 - mae: 0.0082 - mda: 0.5056 - rmse: 0.0771\n",
            "Epoch 8: val_loss improved from 0.00017 to 0.00013, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3307e-04 - mae: 0.0081 - mda: 0.5057 - rmse: 0.0771 - val_loss: 1.2639e-04 - val_mae: 0.0071 - val_mda: 0.5150 - val_rmse: 0.0092 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m563/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0044e-04 - mae: 0.0072 - mda: 0.5072 - rmse: 0.0763\n",
            "Epoch 9: val_loss did not improve from 0.00013\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0067e-04 - mae: 0.0072 - mda: 0.5073 - rmse: 0.0763 - val_loss: 1.5961e-04 - val_mae: 0.0078 - val_mda: 0.5182 - val_rmse: 0.0098 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m567/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.8669e-05 - mae: 0.0072 - mda: 0.5092 - rmse: 0.0764\n",
            "Epoch 10: val_loss did not improve from 0.00013\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.8650e-05 - mae: 0.0072 - mda: 0.5092 - rmse: 0.0764 - val_loss: 1.8671e-04 - val_mae: 0.0084 - val_mda: 0.5185 - val_rmse: 0.0103 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m558/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.9700e-05 - mae: 0.0069 - mda: 0.5062 - rmse: 0.0771\n",
            "Epoch 11: val_loss did not improve from 0.00013\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.9761e-05 - mae: 0.0069 - mda: 0.5062 - rmse: 0.0771 - val_loss: 5.1041e-04 - val_mae: 0.0149 - val_mda: 0.5179 - val_rmse: 0.0164 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m568/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.5298e-05 - mae: 0.0067 - mda: 0.5084 - rmse: 0.0766\n",
            "Epoch 12: val_loss did not improve from 0.00013\n",
            "\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.5349e-05 - mae: 0.0067 - mda: 0.5084 - rmse: 0.0766 - val_loss: 2.8811e-04 - val_mae: 0.0102 - val_mda: 0.5183 - val_rmse: 0.0119 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m573/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8919e-05 - mae: 0.0063 - mda: 0.5077 - rmse: 0.0762\n",
            "Epoch 13: val_loss did not improve from 0.00013\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.8926e-05 - mae: 0.0063 - mda: 0.5077 - rmse: 0.0762 - val_loss: 4.8218e-04 - val_mae: 0.0131 - val_mda: 0.5197 - val_rmse: 0.0147 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m568/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.0534e-05 - mae: 0.0064 - mda: 0.5066 - rmse: 0.0765\n",
            "Epoch 14: val_loss did not improve from 0.00013\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.0526e-05 - mae: 0.0064 - mda: 0.5067 - rmse: 0.0765 - val_loss: 3.6927e-04 - val_mae: 0.0113 - val_mda: 0.5197 - val_rmse: 0.0130 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m564/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5526e-05 - mae: 0.0062 - mda: 0.5085 - rmse: 0.0768\n",
            "Epoch 15: val_loss did not improve from 0.00013\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.5589e-05 - mae: 0.0062 - mda: 0.5085 - rmse: 0.0768 - val_loss: 5.9737e-04 - val_mae: 0.0142 - val_mda: 0.5208 - val_rmse: 0.0157 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m567/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5948e-05 - mae: 0.0062 - mda: 0.5084 - rmse: 0.0770\n",
            "Epoch 16: val_loss did not improve from 0.00013\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.5902e-05 - mae: 0.0062 - mda: 0.5084 - rmse: 0.0770 - val_loss: 6.4073e-04 - val_mae: 0.0149 - val_mda: 0.5213 - val_rmse: 0.0163 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m568/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3430e-05 - mae: 0.0061 - mda: 0.5060 - rmse: 0.0770\n",
            "Epoch 17: val_loss did not improve from 0.00013\n",
            "\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.3400e-05 - mae: 0.0061 - mda: 0.5060 - rmse: 0.0770 - val_loss: 7.9711e-04 - val_mae: 0.0172 - val_mda: 0.5180 - val_rmse: 0.0187 - learning_rate: 2.5000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9299e-05 - mae: 0.0059 - mda: 0.5061 - rmse: 0.0763\n",
            "Epoch 18: val_loss did not improve from 0.00013\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.9306e-05 - mae: 0.0059 - mda: 0.5061 - rmse: 0.0763 - val_loss: 5.9170e-04 - val_mae: 0.0139 - val_mda: 0.5193 - val_rmse: 0.0155 - learning_rate: 1.2500e-04\n",
            "Epoch 18: early stopping\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "Validation Loss: 0.00013, RMSE: 0.00919, MDA: 0.52131, MAE: 0.00711\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m566/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0149 - mae: 0.0686 - mda: 0.5021 - rmse: 0.1266\n",
            "Epoch 1: val_loss improved from inf to 0.00073, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0146 - mae: 0.0677 - mda: 0.5021 - rmse: 0.1257 - val_loss: 7.3458e-04 - val_mae: 0.0174 - val_mda: 0.4963 - val_rmse: 0.0191 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4860e-04 - mae: 0.0173 - mda: 0.5050 - rmse: 0.0800\n",
            "Epoch 2: val_loss improved from 0.00073 to 0.00037, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.4761e-04 - mae: 0.0173 - mda: 0.5050 - rmse: 0.0800 - val_loss: 3.7087e-04 - val_mae: 0.0146 - val_mda: 0.5015 - val_rmse: 0.0162 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4128e-04 - mae: 0.0124 - mda: 0.5079 - rmse: 0.0781\n",
            "Epoch 3: val_loss improved from 0.00037 to 0.00020, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.4083e-04 - mae: 0.0124 - mda: 0.5078 - rmse: 0.0781 - val_loss: 2.0249e-04 - val_mae: 0.0106 - val_mda: 0.5060 - val_rmse: 0.0124 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2022e-04 - mae: 0.0102 - mda: 0.5064 - rmse: 0.0768\n",
            "Epoch 4: val_loss improved from 0.00020 to 0.00009, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.1989e-04 - mae: 0.0102 - mda: 0.5064 - rmse: 0.0768 - val_loss: 9.1740e-05 - val_mae: 0.0067 - val_mda: 0.5073 - val_rmse: 0.0089 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4494e-04 - mae: 0.0085 - mda: 0.5063 - rmse: 0.0758\n",
            "Epoch 5: val_loss did not improve from 0.00009\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.4489e-04 - mae: 0.0085 - mda: 0.5063 - rmse: 0.0758 - val_loss: 1.7877e-04 - val_mae: 0.0096 - val_mda: 0.5086 - val_rmse: 0.0113 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2863e-04 - mae: 0.0079 - mda: 0.5055 - rmse: 0.0756\n",
            "Epoch 6: val_loss improved from 0.00009 to 0.00006, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.2857e-04 - mae: 0.0079 - mda: 0.5055 - rmse: 0.0756 - val_loss: 6.4509e-05 - val_mae: 0.0062 - val_mda: 0.5086 - val_rmse: 0.0085 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.7677e-05 - mae: 0.0072 - mda: 0.5068 - rmse: 0.0766\n",
            "Epoch 7: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 9.7622e-05 - mae: 0.0072 - mda: 0.5068 - rmse: 0.0766 - val_loss: 1.5648e-04 - val_mae: 0.0096 - val_mda: 0.5126 - val_rmse: 0.0114 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.6187e-05 - mae: 0.0067 - mda: 0.5071 - rmse: 0.0767\n",
            "Epoch 8: val_loss improved from 0.00006 to 0.00005, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 8.6206e-05 - mae: 0.0067 - mda: 0.5071 - rmse: 0.0767 - val_loss: 5.4987e-05 - val_mae: 0.0054 - val_mda: 0.5142 - val_rmse: 0.0078 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2471e-05 - mae: 0.0061 - mda: 0.5069 - rmse: 0.0763\n",
            "Epoch 9: val_loss did not improve from 0.00005\n",
            "\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 7.2462e-05 - mae: 0.0061 - mda: 0.5069 - rmse: 0.0763 - val_loss: 9.3368e-05 - val_mae: 0.0071 - val_mda: 0.5155 - val_rmse: 0.0092 - learning_rate: 0.0010\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "\u001b[1m573/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2011e-05 - mae: 0.0056 - mda: 0.5082 - rmse: 0.0762\n",
            "Epoch 10: val_loss improved from 0.00005 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.1987e-05 - mae: 0.0056 - mda: 0.5082 - rmse: 0.0762 - val_loss: 4.1708e-05 - val_mae: 0.0046 - val_mda: 0.5178 - val_rmse: 0.0071 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.7775e-05 - mae: 0.0054 - mda: 0.5083 - rmse: 0.0767\n",
            "Epoch 11: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.7767e-05 - mae: 0.0054 - mda: 0.5083 - rmse: 0.0767 - val_loss: 1.6691e-04 - val_mae: 0.0091 - val_mda: 0.5152 - val_rmse: 0.0110 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.7880e-05 - mae: 0.0054 - mda: 0.5084 - rmse: 0.0779\n",
            "Epoch 12: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.7845e-05 - mae: 0.0054 - mda: 0.5084 - rmse: 0.0779 - val_loss: 2.8198e-04 - val_mae: 0.0128 - val_mda: 0.5200 - val_rmse: 0.0143 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9784e-05 - mae: 0.0051 - mda: 0.5070 - rmse: 0.0764\n",
            "Epoch 13: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 4.9788e-05 - mae: 0.0051 - mda: 0.5070 - rmse: 0.0764 - val_loss: 3.2359e-04 - val_mae: 0.0136 - val_mda: 0.5212 - val_rmse: 0.0151 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.7438e-05 - mae: 0.0055 - mda: 0.5087 - rmse: 0.0766\n",
            "Epoch 14: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.7394e-05 - mae: 0.0055 - mda: 0.5087 - rmse: 0.0766 - val_loss: 1.2826e-04 - val_mae: 0.0075 - val_mda: 0.5153 - val_rmse: 0.0096 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4907e-05 - mae: 0.0048 - mda: 0.5064 - rmse: 0.0769\n",
            "Epoch 15: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 4.4918e-05 - mae: 0.0048 - mda: 0.5064 - rmse: 0.0769 - val_loss: 5.5279e-05 - val_mae: 0.0054 - val_mda: 0.5211 - val_rmse: 0.0078 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1727e-05 - mae: 0.0046 - mda: 0.5086 - rmse: 0.0760\n",
            "Epoch 16: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 4.1717e-05 - mae: 0.0046 - mda: 0.5086 - rmse: 0.0760 - val_loss: 1.0809e-04 - val_mae: 0.0068 - val_mda: 0.5220 - val_rmse: 0.0090 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4007e-05 - mae: 0.0047 - mda: 0.5075 - rmse: 0.0770\n",
            "Epoch 17: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 4.4024e-05 - mae: 0.0047 - mda: 0.5075 - rmse: 0.0770 - val_loss: 1.2418e-04 - val_mae: 0.0070 - val_mda: 0.5222 - val_rmse: 0.0092 - learning_rate: 2.5000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3052e-05 - mae: 0.0047 - mda: 0.5094 - rmse: 0.0769\n",
            "Epoch 18: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 4.3060e-05 - mae: 0.0047 - mda: 0.5094 - rmse: 0.0769 - val_loss: 4.6268e-04 - val_mae: 0.0168 - val_mda: 0.5211 - val_rmse: 0.0182 - learning_rate: 2.5000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3431e-05 - mae: 0.0048 - mda: 0.5088 - rmse: 0.0759\n",
            "Epoch 19: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 4.3414e-05 - mae: 0.0048 - mda: 0.5088 - rmse: 0.0759 - val_loss: 2.6863e-04 - val_mae: 0.0114 - val_mda: 0.5238 - val_rmse: 0.0132 - learning_rate: 2.5000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8500e-05 - mae: 0.0044 - mda: 0.5078 - rmse: 0.0761\n",
            "Epoch 20: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.8501e-05 - mae: 0.0044 - mda: 0.5078 - rmse: 0.0761 - val_loss: 2.2064e-04 - val_mae: 0.0097 - val_mda: 0.5234 - val_rmse: 0.0117 - learning_rate: 1.2500e-04\n",
            "Epoch 20: early stopping\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "Validation Loss: 0.00004, RMSE: 0.00712, MDA: 0.52379, MAE: 0.00457\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0188 - mae: 0.0752 - mda: 0.5015 - rmse: 0.1319\n",
            "Epoch 1: val_loss improved from inf to 0.00116, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0187 - mae: 0.0747 - mda: 0.5015 - rmse: 0.1315 - val_loss: 0.0012 - val_mae: 0.0283 - val_mda: 0.5009 - val_rmse: 0.0297 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.7456e-04 - mae: 0.0164 - mda: 0.5046 - rmse: 0.0796\n",
            "Epoch 2: val_loss improved from 0.00116 to 0.00006, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 5.7268e-04 - mae: 0.0163 - mda: 0.5046 - rmse: 0.0796 - val_loss: 6.3918e-05 - val_mae: 0.0061 - val_mda: 0.5035 - val_rmse: 0.0086 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9490e-04 - mae: 0.0117 - mda: 0.5060 - rmse: 0.0790\n",
            "Epoch 3: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.9456e-04 - mae: 0.0117 - mda: 0.5060 - rmse: 0.0790 - val_loss: 9.2609e-05 - val_mae: 0.0074 - val_mda: 0.5014 - val_rmse: 0.0096 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8862e-04 - mae: 0.0094 - mda: 0.5075 - rmse: 0.0775\n",
            "Epoch 4: val_loss improved from 0.00006 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 1.8858e-04 - mae: 0.0094 - mda: 0.5075 - rmse: 0.0775 - val_loss: 3.1199e-05 - val_mae: 0.0043 - val_mda: 0.5043 - val_rmse: 0.0069 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m573/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4843e-04 - mae: 0.0080 - mda: 0.5080 - rmse: 0.0773\n",
            "Epoch 5: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 1.4815e-04 - mae: 0.0080 - mda: 0.5080 - rmse: 0.0773 - val_loss: 4.3034e-05 - val_mae: 0.0050 - val_mda: 0.5099 - val_rmse: 0.0074 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m573/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.4792e-05 - mae: 0.0069 - mda: 0.5065 - rmse: 0.0772\n",
            "Epoch 6: val_loss improved from 0.00003 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 9.4767e-05 - mae: 0.0069 - mda: 0.5065 - rmse: 0.0772 - val_loss: 2.6366e-05 - val_mae: 0.0039 - val_mda: 0.5057 - val_rmse: 0.0065 - learning_rate: 0.0010\n",
            "Epoch 7/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.2230e-05 - mae: 0.0067 - mda: 0.5079 - rmse: 0.0772\n",
            "Epoch 7: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 9.2109e-05 - mae: 0.0067 - mda: 0.5079 - rmse: 0.0772 - val_loss: 1.5780e-04 - val_mae: 0.0105 - val_mda: 0.5099 - val_rmse: 0.0123 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.5866e-05 - mae: 0.0058 - mda: 0.5060 - rmse: 0.0777\n",
            "Epoch 8: val_loss improved from 0.00003 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 6.5825e-05 - mae: 0.0058 - mda: 0.5060 - rmse: 0.0777 - val_loss: 2.4753e-05 - val_mae: 0.0037 - val_mda: 0.5128 - val_rmse: 0.0064 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.0163e-05 - mae: 0.0053 - mda: 0.5076 - rmse: 0.0763\n",
            "Epoch 9: val_loss improved from 0.00002 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 6.0136e-05 - mae: 0.0053 - mda: 0.5076 - rmse: 0.0763 - val_loss: 2.0011e-05 - val_mae: 0.0034 - val_mda: 0.5133 - val_rmse: 0.0061 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.6951e-05 - mae: 0.0053 - mda: 0.5080 - rmse: 0.0760\n",
            "Epoch 10: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 5.6910e-05 - mae: 0.0053 - mda: 0.5080 - rmse: 0.0760 - val_loss: 6.8599e-05 - val_mae: 0.0064 - val_mda: 0.5124 - val_rmse: 0.0086 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.1203e-05 - mae: 0.0052 - mda: 0.5080 - rmse: 0.0765\n",
            "Epoch 11: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 5.1225e-05 - mae: 0.0052 - mda: 0.5080 - rmse: 0.0765 - val_loss: 4.5047e-05 - val_mae: 0.0052 - val_mda: 0.5134 - val_rmse: 0.0078 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.2482e-05 - mae: 0.0051 - mda: 0.5082 - rmse: 0.0775\n",
            "Epoch 12: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 5.2447e-05 - mae: 0.0051 - mda: 0.5082 - rmse: 0.0775 - val_loss: 2.1389e-05 - val_mae: 0.0035 - val_mda: 0.5144 - val_rmse: 0.0063 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1262e-05 - mae: 0.0046 - mda: 0.5071 - rmse: 0.0775\n",
            "Epoch 13: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 4.1259e-05 - mae: 0.0046 - mda: 0.5071 - rmse: 0.0775 - val_loss: 3.1450e-05 - val_mae: 0.0046 - val_mda: 0.5159 - val_rmse: 0.0072 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.9743e-05 - mae: 0.0045 - mda: 0.5076 - rmse: 0.0760\n",
            "Epoch 14: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 3.9737e-05 - mae: 0.0045 - mda: 0.5076 - rmse: 0.0760 - val_loss: 1.2433e-04 - val_mae: 0.0087 - val_mda: 0.5187 - val_rmse: 0.0107 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.8821e-05 - mae: 0.0044 - mda: 0.5078 - rmse: 0.0764\n",
            "Epoch 15: val_loss improved from 0.00002 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 3.8805e-05 - mae: 0.0044 - mda: 0.5078 - rmse: 0.0764 - val_loss: 1.6838e-05 - val_mae: 0.0031 - val_mda: 0.5175 - val_rmse: 0.0060 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.6348e-05 - mae: 0.0043 - mda: 0.5086 - rmse: 0.0763\n",
            "Epoch 16: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 3.6365e-05 - mae: 0.0043 - mda: 0.5086 - rmse: 0.0763 - val_loss: 2.0498e-05 - val_mae: 0.0034 - val_mda: 0.5186 - val_rmse: 0.0062 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.6933e-05 - mae: 0.0043 - mda: 0.5076 - rmse: 0.0771\n",
            "Epoch 17: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 3.6926e-05 - mae: 0.0043 - mda: 0.5076 - rmse: 0.0771 - val_loss: 4.9449e-05 - val_mae: 0.0060 - val_mda: 0.5160 - val_rmse: 0.0085 - learning_rate: 2.5000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m573/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.2518e-05 - mae: 0.0041 - mda: 0.5076 - rmse: 0.0765\n",
            "Epoch 18: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 3.2492e-05 - mae: 0.0040 - mda: 0.5076 - rmse: 0.0765 - val_loss: 3.0011e-05 - val_mae: 0.0039 - val_mda: 0.5190 - val_rmse: 0.0067 - learning_rate: 1.2500e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m573/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7775e-05 - mae: 0.0038 - mda: 0.5093 - rmse: 0.0765\n",
            "Epoch 19: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.7789e-05 - mae: 0.0038 - mda: 0.5093 - rmse: 0.0765 - val_loss: 7.9318e-05 - val_mae: 0.0072 - val_mda: 0.5202 - val_rmse: 0.0094 - learning_rate: 1.2500e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6944e-05 - mae: 0.0037 - mda: 0.5089 - rmse: 0.0764\n",
            "Epoch 20: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.6969e-05 - mae: 0.0037 - mda: 0.5088 - rmse: 0.0764 - val_loss: 3.4481e-05 - val_mae: 0.0042 - val_mda: 0.5178 - val_rmse: 0.0070 - learning_rate: 1.2500e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.8554e-05 - mae: 0.0038 - mda: 0.5078 - rmse: 0.0767\n",
            "Epoch 21: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.8582e-05 - mae: 0.0038 - mda: 0.5078 - rmse: 0.0767 - val_loss: 1.0434e-04 - val_mae: 0.0081 - val_mda: 0.5208 - val_rmse: 0.0102 - learning_rate: 1.2500e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6586e-05 - mae: 0.0037 - mda: 0.5070 - rmse: 0.0756\n",
            "Epoch 22: val_loss improved from 0.00002 to 0.00002, saving model to best_model.keras\n",
            "\n",
            "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.6613e-05 - mae: 0.0037 - mda: 0.5070 - rmse: 0.0756 - val_loss: 1.6668e-05 - val_mae: 0.0030 - val_mda: 0.5189 - val_rmse: 0.0061 - learning_rate: 1.2500e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.5075e-05 - mae: 0.0036 - mda: 0.5068 - rmse: 0.0764\n",
            "Epoch 23: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.5077e-05 - mae: 0.0036 - mda: 0.5068 - rmse: 0.0764 - val_loss: 6.1418e-05 - val_mae: 0.0058 - val_mda: 0.5219 - val_rmse: 0.0082 - learning_rate: 6.2500e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m572/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.3784e-05 - mae: 0.0035 - mda: 0.5076 - rmse: 0.0767\n",
            "Epoch 24: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.3794e-05 - mae: 0.0035 - mda: 0.5076 - rmse: 0.0767 - val_loss: 2.7846e-05 - val_mae: 0.0038 - val_mda: 0.5203 - val_rmse: 0.0066 - learning_rate: 6.2500e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1971e-05 - mae: 0.0033 - mda: 0.5070 - rmse: 0.0760\n",
            "Epoch 25: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.1991e-05 - mae: 0.0033 - mda: 0.5070 - rmse: 0.0760 - val_loss: 4.2053e-05 - val_mae: 0.0047 - val_mda: 0.5220 - val_rmse: 0.0073 - learning_rate: 6.2500e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3383e-05 - mae: 0.0034 - mda: 0.5084 - rmse: 0.0757\n",
            "Epoch 26: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.3382e-05 - mae: 0.0034 - mda: 0.5084 - rmse: 0.0757 - val_loss: 2.1956e-05 - val_mae: 0.0034 - val_mda: 0.5224 - val_rmse: 0.0063 - learning_rate: 6.2500e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.3377e-05 - mae: 0.0034 - mda: 0.5087 - rmse: 0.0764\n",
            "Epoch 27: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.3378e-05 - mae: 0.0034 - mda: 0.5087 - rmse: 0.0764 - val_loss: 6.8905e-05 - val_mae: 0.0063 - val_mda: 0.5203 - val_rmse: 0.0087 - learning_rate: 6.2500e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.2578e-05 - mae: 0.0033 - mda: 0.5068 - rmse: 0.0766\n",
            "Epoch 28: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.2568e-05 - mae: 0.0033 - mda: 0.5068 - rmse: 0.0766 - val_loss: 1.9270e-05 - val_mae: 0.0032 - val_mda: 0.5192 - val_rmse: 0.0062 - learning_rate: 3.1250e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.2082e-05 - mae: 0.0033 - mda: 0.5077 - rmse: 0.0769\n",
            "Epoch 29: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.2080e-05 - mae: 0.0033 - mda: 0.5077 - rmse: 0.0769 - val_loss: 2.0343e-05 - val_mae: 0.0033 - val_mda: 0.5213 - val_rmse: 0.0063 - learning_rate: 3.1250e-05\n",
            "Epoch 30/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.2021e-05 - mae: 0.0033 - mda: 0.5085 - rmse: 0.0761\n",
            "Epoch 30: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.2017e-05 - mae: 0.0033 - mda: 0.5085 - rmse: 0.0762 - val_loss: 1.9318e-05 - val_mae: 0.0034 - val_mda: 0.5212 - val_rmse: 0.0064 - learning_rate: 3.1250e-05\n",
            "Epoch 31/50\n",
            "\u001b[1m573/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.0737e-05 - mae: 0.0032 - mda: 0.5092 - rmse: 0.0755\n",
            "Epoch 31: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.0758e-05 - mae: 0.0032 - mda: 0.5092 - rmse: 0.0755 - val_loss: 7.4303e-05 - val_mae: 0.0064 - val_mda: 0.5206 - val_rmse: 0.0087 - learning_rate: 3.1250e-05\n",
            "Epoch 32/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0354e-05 - mae: 0.0032 - mda: 0.5080 - rmse: 0.0759\n",
            "Epoch 32: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2.0355e-05 - mae: 0.0032 - mda: 0.5080 - rmse: 0.0759 - val_loss: 3.0577e-05 - val_mae: 0.0039 - val_mda: 0.5215 - val_rmse: 0.0067 - learning_rate: 3.1250e-05\n",
            "Epoch 32: early stopping\n",
            "Restoring model weights from the end of the best epoch: 22.\n",
            "Validation Loss: 0.00002, RMSE: 0.00601, MDA: 0.52236, MAE: 0.00303\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m560/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0187 - mae: 0.0655 - mda: 0.5026 - rmse: 0.1205\n",
            "Epoch 1: val_loss improved from inf to 0.00129, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0182 - mae: 0.0640 - mda: 0.5026 - rmse: 0.1192 - val_loss: 0.0013 - val_mae: 0.0282 - val_mda: 0.5016 - val_rmse: 0.0292 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m563/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1185e-04 - mae: 0.0106 - mda: 0.5046 - rmse: 0.0757\n",
            "Epoch 2: val_loss improved from 0.00129 to 0.00018, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1114e-04 - mae: 0.0106 - mda: 0.5047 - rmse: 0.0757 - val_loss: 1.7752e-04 - val_mae: 0.0090 - val_mda: 0.5065 - val_rmse: 0.0111 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3656e-04 - mae: 0.0086 - mda: 0.5066 - rmse: 0.0760\n",
            "Epoch 3: val_loss did not improve from 0.00018\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3649e-04 - mae: 0.0086 - mda: 0.5066 - rmse: 0.0760 - val_loss: 9.4537e-04 - val_mae: 0.0221 - val_mda: 0.5095 - val_rmse: 0.0234 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m558/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1196e-04 - mae: 0.0078 - mda: 0.5077 - rmse: 0.0758\n",
            "Epoch 4: val_loss did not improve from 0.00018\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1210e-04 - mae: 0.0078 - mda: 0.5077 - rmse: 0.0758 - val_loss: 7.7372e-04 - val_mae: 0.0189 - val_mda: 0.5086 - val_rmse: 0.0203 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0121e-04 - mae: 0.0074 - mda: 0.5082 - rmse: 0.0774\n",
            "Epoch 5: val_loss did not improve from 0.00018\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0123e-04 - mae: 0.0074 - mda: 0.5082 - rmse: 0.0774 - val_loss: 9.4302e-04 - val_mae: 0.0175 - val_mda: 0.5109 - val_rmse: 0.0192 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m558/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0536e-04 - mae: 0.0076 - mda: 0.5069 - rmse: 0.0766\n",
            "Epoch 6: val_loss did not improve from 0.00018\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0514e-04 - mae: 0.0076 - mda: 0.5069 - rmse: 0.0766 - val_loss: 8.2767e-04 - val_mae: 0.0165 - val_mda: 0.5105 - val_rmse: 0.0182 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.8240e-05 - mae: 0.0073 - mda: 0.5060 - rmse: 0.0769\n",
            "Epoch 7: val_loss did not improve from 0.00018\n",
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.8228e-05 - mae: 0.0073 - mda: 0.5060 - rmse: 0.0769 - val_loss: 0.0014 - val_mae: 0.0229 - val_mda: 0.5158 - val_rmse: 0.0244 - learning_rate: 0.0100\n",
            "Epoch 8/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6526e-05 - mae: 0.0067 - mda: 0.5057 - rmse: 0.0764\n",
            "Epoch 8: val_loss did not improve from 0.00018\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.6537e-05 - mae: 0.0067 - mda: 0.5057 - rmse: 0.0764 - val_loss: 0.0019 - val_mae: 0.0306 - val_mda: 0.5149 - val_rmse: 0.0317 - learning_rate: 0.0050\n",
            "Epoch 9/50\n",
            "\u001b[1m566/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.7372e-05 - mae: 0.0068 - mda: 0.5077 - rmse: 0.0757\n",
            "Epoch 9: val_loss did not improve from 0.00018\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.7322e-05 - mae: 0.0068 - mda: 0.5077 - rmse: 0.0757 - val_loss: 0.0021 - val_mae: 0.0339 - val_mda: 0.5167 - val_rmse: 0.0348 - learning_rate: 0.0050\n",
            "Epoch 10/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.0146e-05 - mae: 0.0069 - mda: 0.5069 - rmse: 0.0769\n",
            "Epoch 10: val_loss did not improve from 0.00018\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.0141e-05 - mae: 0.0069 - mda: 0.5069 - rmse: 0.0769 - val_loss: 5.9371e-04 - val_mae: 0.0132 - val_mda: 0.5172 - val_rmse: 0.0154 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m564/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0016e-04 - mae: 0.0073 - mda: 0.5079 - rmse: 0.0771\n",
            "Epoch 11: val_loss did not improve from 0.00018\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0020e-04 - mae: 0.0073 - mda: 0.5079 - rmse: 0.0771 - val_loss: 9.1037e-04 - val_mae: 0.0195 - val_mda: 0.5184 - val_rmse: 0.0210 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.5354e-05 - mae: 0.0071 - mda: 0.5060 - rmse: 0.0761\n",
            "Epoch 12: val_loss did not improve from 0.00018\n",
            "\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.5422e-05 - mae: 0.0071 - mda: 0.5060 - rmse: 0.0761 - val_loss: 2.3215e-04 - val_mae: 0.0143 - val_mda: 0.5177 - val_rmse: 0.0160 - learning_rate: 0.0050\n",
            "Epoch 12: early stopping\n",
            "Restoring model weights from the end of the best epoch: 2.\n",
            "Validation Loss: 0.00018, RMSE: 0.01107, MDA: 0.51838, MAE: 0.00903\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m568/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2272 - mae: 0.2687 - mda: 0.5007 - rmse: 0.3371\n",
            "Epoch 1: val_loss improved from inf to 0.01302, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2227 - mae: 0.2643 - mda: 0.5007 - rmse: 0.3323 - val_loss: 0.0130 - val_mae: 0.0751 - val_mda: 0.5025 - val_rmse: 0.0758 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m567/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.8275e-04 - mae: 0.0190 - mda: 0.5059 - rmse: 0.0771\n",
            "Epoch 2: val_loss did not improve from 0.01302\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.8035e-04 - mae: 0.0190 - mda: 0.5058 - rmse: 0.0771 - val_loss: 0.0143 - val_mae: 0.0826 - val_mda: 0.5028 - val_rmse: 0.0832 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5335e-04 - mae: 0.0148 - mda: 0.5046 - rmse: 0.0757\n",
            "Epoch 3: val_loss improved from 0.01302 to 0.01114, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.5356e-04 - mae: 0.0148 - mda: 0.5046 - rmse: 0.0757 - val_loss: 0.0111 - val_mae: 0.0659 - val_mda: 0.5014 - val_rmse: 0.0667 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3868e-04 - mae: 0.0165 - mda: 0.5040 - rmse: 0.0775\n",
            "Epoch 4: val_loss improved from 0.01114 to 0.00915, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 4.3888e-04 - mae: 0.0166 - mda: 0.5040 - rmse: 0.0775 - val_loss: 0.0092 - val_mae: 0.0583 - val_mda: 0.5124 - val_rmse: 0.0591 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1155e-04 - mae: 0.0208 - mda: 0.5045 - rmse: 0.0788\n",
            "Epoch 5: val_loss did not improve from 0.00915\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 7.1193e-04 - mae: 0.0208 - mda: 0.5045 - rmse: 0.0788 - val_loss: 0.0185 - val_mae: 0.1044 - val_mda: 0.5058 - val_rmse: 0.1049 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m566/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0012 - mae: 0.0280 - mda: 0.5025 - rmse: 0.0801\n",
            "Epoch 6: val_loss did not improve from 0.00915\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0012 - mae: 0.0280 - mda: 0.5026 - rmse: 0.0801 - val_loss: 0.0100 - val_mae: 0.0604 - val_mda: 0.5042 - val_rmse: 0.0612 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.6154e-04 - mae: 0.0239 - mda: 0.5043 - rmse: 0.0808\n",
            "Epoch 7: val_loss did not improve from 0.00915\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 9.6413e-04 - mae: 0.0239 - mda: 0.5043 - rmse: 0.0808 - val_loss: 0.0129 - val_mae: 0.0697 - val_mda: 0.4973 - val_rmse: 0.0705 - learning_rate: 0.0100\n",
            "Epoch 8/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0011 - mae: 0.0249 - mda: 0.5042 - rmse: 0.0801\n",
            "Epoch 8: val_loss did not improve from 0.00915\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0011 - mae: 0.0250 - mda: 0.5042 - rmse: 0.0802 - val_loss: 0.0115 - val_mae: 0.0680 - val_mda: 0.5008 - val_rmse: 0.0686 - learning_rate: 0.0100\n",
            "Epoch 9/50\n",
            "\u001b[1m573/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0013 - mae: 0.0287 - mda: 0.5028 - rmse: 0.0817\n",
            "Epoch 9: val_loss did not improve from 0.00915\n",
            "\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0013 - mae: 0.0287 - mda: 0.5028 - rmse: 0.0817 - val_loss: 0.0181 - val_mae: 0.0848 - val_mda: 0.5069 - val_rmse: 0.0855 - learning_rate: 0.0100\n",
            "Epoch 10/50\n",
            "\u001b[1m565/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9905e-04 - mae: 0.0156 - mda: 0.5050 - rmse: 0.0762\n",
            "Epoch 10: val_loss did not improve from 0.00915\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.9722e-04 - mae: 0.0156 - mda: 0.5050 - rmse: 0.0762 - val_loss: 0.0154 - val_mae: 0.0705 - val_mda: 0.5107 - val_rmse: 0.0714 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5524e-04 - mae: 0.0125 - mda: 0.5047 - rmse: 0.0778\n",
            "Epoch 11: val_loss did not improve from 0.00915\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.5541e-04 - mae: 0.0125 - mda: 0.5047 - rmse: 0.0778 - val_loss: 0.0159 - val_mae: 0.0773 - val_mda: 0.5090 - val_rmse: 0.0781 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2756e-04 - mae: 0.0143 - mda: 0.5048 - rmse: 0.0773\n",
            "Epoch 12: val_loss did not improve from 0.00915\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.2764e-04 - mae: 0.0143 - mda: 0.5048 - rmse: 0.0773 - val_loss: 0.0106 - val_mae: 0.0739 - val_mda: 0.5043 - val_rmse: 0.0744 - learning_rate: 0.0050\n",
            "Epoch 13/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2539e-04 - mae: 0.0141 - mda: 0.5044 - rmse: 0.0761\n",
            "Epoch 13: val_loss did not improve from 0.00915\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.2539e-04 - mae: 0.0141 - mda: 0.5044 - rmse: 0.0761 - val_loss: 0.0113 - val_mae: 0.0655 - val_mda: 0.5091 - val_rmse: 0.0664 - learning_rate: 0.0050\n",
            "Epoch 14/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0252e-04 - mae: 0.0137 - mda: 0.5039 - rmse: 0.0778\n",
            "Epoch 14: val_loss did not improve from 0.00915\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.0269e-04 - mae: 0.0137 - mda: 0.5039 - rmse: 0.0778 - val_loss: 0.0109 - val_mae: 0.0637 - val_mda: 0.5132 - val_rmse: 0.0646 - learning_rate: 0.0050\n",
            "Epoch 14: early stopping\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "Validation Loss: 0.00915, RMSE: 0.05909, MDA: 0.51318, MAE: 0.05831\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m572/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6380 - mae: 0.4528 - mda: 0.5003 - rmse: 0.5391\n",
            "Epoch 1: val_loss improved from inf to 0.01347, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.6286 - mae: 0.4472 - mda: 0.5003 - rmse: 0.5330 - val_loss: 0.0135 - val_mae: 0.0831 - val_mda: 0.4997 - val_rmse: 0.0836 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3768e-04 - mae: 0.0180 - mda: 0.5020 - rmse: 0.0754\n",
            "Epoch 2: val_loss improved from 0.01347 to 0.01313, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 5.3777e-04 - mae: 0.0180 - mda: 0.5020 - rmse: 0.0754 - val_loss: 0.0131 - val_mae: 0.0753 - val_mda: 0.5013 - val_rmse: 0.0759 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0901e-04 - mae: 0.0176 - mda: 0.5034 - rmse: 0.0763\n",
            "Epoch 3: val_loss improved from 0.01313 to 0.01240, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 5.0930e-04 - mae: 0.0176 - mda: 0.5034 - rmse: 0.0763 - val_loss: 0.0124 - val_mae: 0.0695 - val_mda: 0.4992 - val_rmse: 0.0702 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.9465e-04 - mae: 0.0191 - mda: 0.5048 - rmse: 0.0775\n",
            "Epoch 4: val_loss did not improve from 0.01240\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 5.9630e-04 - mae: 0.0191 - mda: 0.5048 - rmse: 0.0775 - val_loss: 0.0145 - val_mae: 0.0803 - val_mda: 0.4941 - val_rmse: 0.0810 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0013 - mae: 0.0278 - mda: 0.5033 - rmse: 0.0813\n",
            "Epoch 5: val_loss improved from 0.01240 to 0.01188, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0013 - mae: 0.0279 - mda: 0.5033 - rmse: 0.0813 - val_loss: 0.0119 - val_mae: 0.0670 - val_mda: 0.4975 - val_rmse: 0.0678 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0048 - mae: 0.0533 - mda: 0.5027 - rmse: 0.0976\n",
            "Epoch 6: val_loss did not improve from 0.01188\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0049 - mae: 0.0535 - mda: 0.5027 - rmse: 0.0978 - val_loss: 0.0188 - val_mae: 0.1084 - val_mda: 0.4991 - val_rmse: 0.1089 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m573/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0017 - mae: 0.0310 - mda: 0.5034 - rmse: 0.0826\n",
            "Epoch 7: val_loss did not improve from 0.01188\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0017 - mae: 0.0312 - mda: 0.5034 - rmse: 0.0828 - val_loss: 0.0134 - val_mae: 0.0734 - val_mda: 0.4965 - val_rmse: 0.0740 - learning_rate: 0.0100\n",
            "Epoch 8/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0166 - mae: 0.0619 - mda: 0.5006 - rmse: 0.1055\n",
            "Epoch 8: val_loss did not improve from 0.01188\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0228 - mae: 0.0642 - mda: 0.5006 - rmse: 0.1078 - val_loss: 0.3306 - val_mae: 0.5611 - val_mda: 0.4956 - val_rmse: 0.5630 - learning_rate: 0.0100\n",
            "Epoch 9/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1532 - mae: 0.5475 - mda: 0.4986 - rmse: 0.6480\n",
            "Epoch 9: val_loss did not improve from 0.01188\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 1.1464 - mae: 0.5451 - mda: 0.4986 - rmse: 0.6451 - val_loss: 0.0398 - val_mae: 0.1747 - val_mda: 0.0096 - val_rmse: 0.1750 - learning_rate: 0.0100\n",
            "Epoch 10/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0052 - mae: 0.0576 - mda: 0.5003 - rmse: 0.0712\n",
            "Epoch 10: val_loss did not improve from 0.01188\n",
            "\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0052 - mae: 0.0576 - mda: 0.5003 - rmse: 0.0712 - val_loss: 0.0291 - val_mae: 0.1406 - val_mda: 0.0096 - val_rmse: 0.1410 - learning_rate: 0.0100\n",
            "Epoch 11/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0049 - mae: 0.0567 - mda: 0.4996 - rmse: 0.0689\n",
            "Epoch 11: val_loss did not improve from 0.01188\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0049 - mae: 0.0567 - mda: 0.4996 - rmse: 0.0689 - val_loss: 0.0383 - val_mae: 0.1703 - val_mda: 0.0096 - val_rmse: 0.1706 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0050 - mae: 0.0572 - mda: 0.5003 - rmse: 0.0695\n",
            "Epoch 12: val_loss did not improve from 0.01188\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0050 - mae: 0.0572 - mda: 0.5003 - rmse: 0.0695 - val_loss: 0.0394 - val_mae: 0.1733 - val_mda: 0.0096 - val_rmse: 0.1736 - learning_rate: 0.0050\n",
            "Epoch 13/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0050 - mae: 0.0573 - mda: 0.4987 - rmse: 0.0696\n",
            "Epoch 13: val_loss did not improve from 0.01188\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0050 - mae: 0.0573 - mda: 0.4987 - rmse: 0.0697 - val_loss: 0.0408 - val_mae: 0.1774 - val_mda: 0.0096 - val_rmse: 0.1777 - learning_rate: 0.0050\n",
            "Epoch 14/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0052 - mae: 0.0585 - mda: 0.5010 - rmse: 0.0706\n",
            "Epoch 14: val_loss did not improve from 0.01188\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0052 - mae: 0.0585 - mda: 0.5010 - rmse: 0.0706 - val_loss: 0.0455 - val_mae: 0.1901 - val_mda: 0.0096 - val_rmse: 0.1904 - learning_rate: 0.0050\n",
            "Epoch 15/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0059 - mae: 0.0620 - mda: 0.4996 - rmse: 0.0754\n",
            "Epoch 15: val_loss did not improve from 0.01188\n",
            "\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0059 - mae: 0.0620 - mda: 0.4996 - rmse: 0.0754 - val_loss: 0.0450 - val_mae: 0.1888 - val_mda: 0.0096 - val_rmse: 0.1891 - learning_rate: 0.0050\n",
            "Epoch 15: early stopping\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "Validation Loss: 0.01188, RMSE: 0.06778, MDA: 0.50134, MAE: 0.06696\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0372 - mae: 0.1123 - mda: 0.5002 - rmse: 0.1713\n",
            "Epoch 1: val_loss improved from inf to 0.00142, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0368 - mae: 0.1115 - mda: 0.5002 - rmse: 0.1704 - val_loss: 0.0014 - val_mae: 0.0261 - val_mda: 0.4950 - val_rmse: 0.0301 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m277/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0018 - mae: 0.0287 - mda: 0.5004 - rmse: 0.0872\n",
            "Epoch 2: val_loss improved from 0.00142 to 0.00100, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0018 - mae: 0.0285 - mda: 0.5004 - rmse: 0.0871 - val_loss: 9.9750e-04 - val_mae: 0.0213 - val_mda: 0.4974 - val_rmse: 0.0247 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m272/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.8037e-04 - mae: 0.0198 - mda: 0.5004 - rmse: 0.0823\n",
            "Epoch 3: val_loss improved from 0.00100 to 0.00036, saving model to best_model.keras\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.7385e-04 - mae: 0.0197 - mda: 0.5005 - rmse: 0.0823 - val_loss: 3.6135e-04 - val_mae: 0.0128 - val_mda: 0.4978 - val_rmse: 0.0166 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m275/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5922e-04 - mae: 0.0156 - mda: 0.5012 - rmse: 0.0800\n",
            "Epoch 4: val_loss did not improve from 0.00036\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 5.5577e-04 - mae: 0.0155 - mda: 0.5013 - rmse: 0.0801 - val_loss: 4.4022e-04 - val_mae: 0.0146 - val_mda: 0.4987 - val_rmse: 0.0181 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m280/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0287e-04 - mae: 0.0134 - mda: 0.5020 - rmse: 0.0803\n",
            "Epoch 5: val_loss improved from 0.00036 to 0.00024, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.0169e-04 - mae: 0.0133 - mda: 0.5020 - rmse: 0.0803 - val_loss: 2.4147e-04 - val_mae: 0.0099 - val_mda: 0.4993 - val_rmse: 0.0139 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m279/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8408e-04 - mae: 0.0115 - mda: 0.5019 - rmse: 0.0786\n",
            "Epoch 6: val_loss did not improve from 0.00024\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.8431e-04 - mae: 0.0115 - mda: 0.5019 - rmse: 0.0786 - val_loss: 3.9833e-04 - val_mae: 0.0143 - val_mda: 0.5011 - val_rmse: 0.0176 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m275/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5808e-04 - mae: 0.0106 - mda: 0.5028 - rmse: 0.0788\n",
            "Epoch 7: val_loss did not improve from 0.00024\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.5752e-04 - mae: 0.0106 - mda: 0.5028 - rmse: 0.0788 - val_loss: 3.1817e-04 - val_mae: 0.0128 - val_mda: 0.5016 - val_rmse: 0.0162 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m274/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1006e-04 - mae: 0.0096 - mda: 0.5021 - rmse: 0.0782\n",
            "Epoch 8: val_loss improved from 0.00024 to 0.00015, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.0951e-04 - mae: 0.0096 - mda: 0.5021 - rmse: 0.0782 - val_loss: 1.5430e-04 - val_mae: 0.0081 - val_mda: 0.5038 - val_rmse: 0.0122 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m281/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7102e-04 - mae: 0.0092 - mda: 0.5034 - rmse: 0.0786\n",
            "Epoch 9: val_loss did not improve from 0.00015\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.7085e-04 - mae: 0.0092 - mda: 0.5034 - rmse: 0.0786 - val_loss: 3.9521e-04 - val_mae: 0.0154 - val_mda: 0.5036 - val_rmse: 0.0184 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m279/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5277e-04 - mae: 0.0085 - mda: 0.5020 - rmse: 0.0785\n",
            "Epoch 10: val_loss did not improve from 0.00015\n",
            "\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.5265e-04 - mae: 0.0085 - mda: 0.5020 - rmse: 0.0785 - val_loss: 1.9370e-04 - val_mae: 0.0093 - val_mda: 0.5030 - val_rmse: 0.0132 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m280/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4853e-04 - mae: 0.0082 - mda: 0.5014 - rmse: 0.0776\n",
            "Epoch 11: val_loss improved from 0.00015 to 0.00009, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.4837e-04 - mae: 0.0082 - mda: 0.5014 - rmse: 0.0777 - val_loss: 9.2220e-05 - val_mae: 0.0063 - val_mda: 0.5041 - val_rmse: 0.0108 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m282/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2531e-04 - mae: 0.0077 - mda: 0.5033 - rmse: 0.0778\n",
            "Epoch 12: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.2518e-04 - mae: 0.0077 - mda: 0.5033 - rmse: 0.0778 - val_loss: 1.6122e-04 - val_mae: 0.0091 - val_mda: 0.5037 - val_rmse: 0.0129 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m273/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2133e-04 - mae: 0.0075 - mda: 0.5027 - rmse: 0.0778\n",
            "Epoch 13: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.2130e-04 - mae: 0.0075 - mda: 0.5027 - rmse: 0.0778 - val_loss: 1.4659e-04 - val_mae: 0.0082 - val_mda: 0.5048 - val_rmse: 0.0123 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m279/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1626e-04 - mae: 0.0074 - mda: 0.5029 - rmse: 0.0788\n",
            "Epoch 14: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1620e-04 - mae: 0.0074 - mda: 0.5029 - rmse: 0.0788 - val_loss: 1.2082e-04 - val_mae: 0.0073 - val_mda: 0.5065 - val_rmse: 0.0116 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m278/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.9596e-05 - mae: 0.0069 - mda: 0.5026 - rmse: 0.0781\n",
            "Epoch 15: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 9.9802e-05 - mae: 0.0069 - mda: 0.5027 - rmse: 0.0781 - val_loss: 2.1730e-04 - val_mae: 0.0108 - val_mda: 0.5069 - val_rmse: 0.0144 - learning_rate: 5.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m274/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.5199e-05 - mae: 0.0067 - mda: 0.5024 - rmse: 0.0777\n",
            "Epoch 16: val_loss improved from 0.00009 to 0.00008, saving model to best_model.keras\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 9.5297e-05 - mae: 0.0067 - mda: 0.5025 - rmse: 0.0777 - val_loss: 8.0768e-05 - val_mae: 0.0059 - val_mda: 0.5070 - val_rmse: 0.0105 - learning_rate: 5.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m273/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.8874e-05 - mae: 0.0066 - mda: 0.5028 - rmse: 0.0778\n",
            "Epoch 17: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.8846e-05 - mae: 0.0066 - mda: 0.5028 - rmse: 0.0778 - val_loss: 1.5514e-04 - val_mae: 0.0086 - val_mda: 0.5072 - val_rmse: 0.0126 - learning_rate: 2.5000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.7611e-05 - mae: 0.0065 - mda: 0.5034 - rmse: 0.0779\n",
            "Epoch 18: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.7612e-05 - mae: 0.0065 - mda: 0.5034 - rmse: 0.0779 - val_loss: 1.1953e-04 - val_mae: 0.0073 - val_mda: 0.5072 - val_rmse: 0.0116 - learning_rate: 2.5000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m279/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.4360e-05 - mae: 0.0063 - mda: 0.5026 - rmse: 0.0785\n",
            "Epoch 19: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.4199e-05 - mae: 0.0063 - mda: 0.5026 - rmse: 0.0785 - val_loss: 1.9245e-04 - val_mae: 0.0102 - val_mda: 0.5068 - val_rmse: 0.0138 - learning_rate: 2.5000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m281/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9280e-05 - mae: 0.0063 - mda: 0.5029 - rmse: 0.0778\n",
            "Epoch 20: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7.9318e-05 - mae: 0.0063 - mda: 0.5029 - rmse: 0.0778 - val_loss: 1.1986e-04 - val_mae: 0.0071 - val_mda: 0.5062 - val_rmse: 0.0114 - learning_rate: 2.5000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.0006e-05 - mae: 0.0061 - mda: 0.5026 - rmse: 0.0787\n",
            "Epoch 21: val_loss did not improve from 0.00008\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7.9999e-05 - mae: 0.0061 - mda: 0.5026 - rmse: 0.0787 - val_loss: 1.7269e-04 - val_mae: 0.0091 - val_mda: 0.5071 - val_rmse: 0.0130 - learning_rate: 2.5000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.5021e-05 - mae: 0.0060 - mda: 0.5029 - rmse: 0.0788\n",
            "Epoch 22: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7.5062e-05 - mae: 0.0060 - mda: 0.5029 - rmse: 0.0788 - val_loss: 1.4080e-04 - val_mae: 0.0079 - val_mda: 0.5079 - val_rmse: 0.0120 - learning_rate: 1.2500e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m282/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8502e-05 - mae: 0.0062 - mda: 0.5036 - rmse: 0.0786\n",
            "Epoch 23: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7.8420e-05 - mae: 0.0062 - mda: 0.5036 - rmse: 0.0786 - val_loss: 9.0755e-05 - val_mae: 0.0063 - val_mda: 0.5077 - val_rmse: 0.0107 - learning_rate: 1.2500e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6898e-05 - mae: 0.0058 - mda: 0.5031 - rmse: 0.0777\n",
            "Epoch 24: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.6947e-05 - mae: 0.0058 - mda: 0.5031 - rmse: 0.0777 - val_loss: 1.7519e-04 - val_mae: 0.0094 - val_mda: 0.5074 - val_rmse: 0.0132 - learning_rate: 1.2500e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8581e-05 - mae: 0.0059 - mda: 0.5026 - rmse: 0.0785\n",
            "Epoch 25: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.8622e-05 - mae: 0.0059 - mda: 0.5026 - rmse: 0.0785 - val_loss: 1.2405e-04 - val_mae: 0.0073 - val_mda: 0.5072 - val_rmse: 0.0115 - learning_rate: 1.2500e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3062e-05 - mae: 0.0056 - mda: 0.5031 - rmse: 0.0773\n",
            "Epoch 26: val_loss did not improve from 0.00008\n",
            "\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.3164e-05 - mae: 0.0056 - mda: 0.5031 - rmse: 0.0773 - val_loss: 1.7751e-04 - val_mae: 0.0093 - val_mda: 0.5073 - val_rmse: 0.0131 - learning_rate: 1.2500e-04\n",
            "Epoch 26: early stopping\n",
            "Restoring model weights from the end of the best epoch: 16.\n",
            "Validation Loss: 0.00008, RMSE: 0.01047, MDA: 0.50790, MAE: 0.00592\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m278/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0325 - mae: 0.1021 - mda: 0.5004 - rmse: 0.1603\n",
            "Epoch 1: val_loss improved from inf to 0.00179, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0313 - mae: 0.0998 - mda: 0.5003 - rmse: 0.1580 - val_loss: 0.0018 - val_mae: 0.0259 - val_mda: 0.4970 - val_rmse: 0.0293 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0014 - mae: 0.0245 - mda: 0.4997 - rmse: 0.0854\n",
            "Epoch 2: val_loss improved from 0.00179 to 0.00093, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0014 - mae: 0.0245 - mda: 0.4997 - rmse: 0.0854 - val_loss: 9.2697e-04 - val_mae: 0.0214 - val_mda: 0.4989 - val_rmse: 0.0243 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6074e-04 - mae: 0.0172 - mda: 0.5015 - rmse: 0.0817\n",
            "Epoch 3: val_loss improved from 0.00093 to 0.00021, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6.5914e-04 - mae: 0.0171 - mda: 0.5015 - rmse: 0.0817 - val_loss: 2.0584e-04 - val_mae: 0.0092 - val_mda: 0.4991 - val_rmse: 0.0130 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1878e-04 - mae: 0.0138 - mda: 0.5024 - rmse: 0.0807\n",
            "Epoch 4: val_loss did not improve from 0.00021\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.1851e-04 - mae: 0.0138 - mda: 0.5024 - rmse: 0.0807 - val_loss: 4.2087e-04 - val_mae: 0.0140 - val_mda: 0.4990 - val_rmse: 0.0173 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1411e-04 - mae: 0.0117 - mda: 0.5018 - rmse: 0.0799\n",
            "Epoch 5: val_loss improved from 0.00021 to 0.00015, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.1403e-04 - mae: 0.0117 - mda: 0.5018 - rmse: 0.0799 - val_loss: 1.5323e-04 - val_mae: 0.0080 - val_mda: 0.5004 - val_rmse: 0.0120 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5617e-04 - mae: 0.0105 - mda: 0.5016 - rmse: 0.0806\n",
            "Epoch 6: val_loss did not improve from 0.00015\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.5564e-04 - mae: 0.0105 - mda: 0.5016 - rmse: 0.0806 - val_loss: 1.6725e-04 - val_mae: 0.0085 - val_mda: 0.4981 - val_rmse: 0.0123 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8609e-04 - mae: 0.0090 - mda: 0.5028 - rmse: 0.0787\n",
            "Epoch 7: val_loss did not improve from 0.00015\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.8597e-04 - mae: 0.0090 - mda: 0.5028 - rmse: 0.0787 - val_loss: 2.3168e-04 - val_mae: 0.0101 - val_mda: 0.4998 - val_rmse: 0.0136 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5796e-04 - mae: 0.0084 - mda: 0.5033 - rmse: 0.0786\n",
            "Epoch 8: val_loss did not improve from 0.00015\n",
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.5791e-04 - mae: 0.0084 - mda: 0.5033 - rmse: 0.0786 - val_loss: 2.4962e-04 - val_mae: 0.0112 - val_mda: 0.4998 - val_rmse: 0.0145 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3564e-04 - mae: 0.0076 - mda: 0.5030 - rmse: 0.0782\n",
            "Epoch 9: val_loss did not improve from 0.00015\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.3540e-04 - mae: 0.0076 - mda: 0.5030 - rmse: 0.0782 - val_loss: 1.6980e-04 - val_mae: 0.0089 - val_mda: 0.5009 - val_rmse: 0.0126 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2912e-04 - mae: 0.0075 - mda: 0.5028 - rmse: 0.0784\n",
            "Epoch 10: val_loss improved from 0.00015 to 0.00011, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2915e-04 - mae: 0.0075 - mda: 0.5028 - rmse: 0.0784 - val_loss: 1.0865e-04 - val_mae: 0.0068 - val_mda: 0.5018 - val_rmse: 0.0110 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1345e-04 - mae: 0.0071 - mda: 0.5021 - rmse: 0.0781\n",
            "Epoch 11: val_loss did not improve from 0.00011\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1337e-04 - mae: 0.0071 - mda: 0.5021 - rmse: 0.0781 - val_loss: 1.2163e-04 - val_mae: 0.0075 - val_mda: 0.5021 - val_rmse: 0.0115 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.9597e-05 - mae: 0.0067 - mda: 0.5027 - rmse: 0.0783\n",
            "Epoch 12: val_loss did not improve from 0.00011\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 9.9596e-05 - mae: 0.0067 - mda: 0.5027 - rmse: 0.0783 - val_loss: 1.6699e-04 - val_mae: 0.0091 - val_mda: 0.5010 - val_rmse: 0.0128 - learning_rate: 5.0000e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.2837e-05 - mae: 0.0065 - mda: 0.5026 - rmse: 0.0789\n",
            "Epoch 13: val_loss did not improve from 0.00011\n",
            "\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 9.2842e-05 - mae: 0.0065 - mda: 0.5026 - rmse: 0.0789 - val_loss: 1.0934e-04 - val_mae: 0.0070 - val_mda: 0.5025 - val_rmse: 0.0110 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1236e-05 - mae: 0.0064 - mda: 0.5028 - rmse: 0.0781\n",
            "Epoch 14: val_loss improved from 0.00011 to 0.00007, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 9.1242e-05 - mae: 0.0064 - mda: 0.5028 - rmse: 0.0781 - val_loss: 7.2416e-05 - val_mae: 0.0057 - val_mda: 0.5031 - val_rmse: 0.0101 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.5099e-05 - mae: 0.0061 - mda: 0.5022 - rmse: 0.0790\n",
            "Epoch 15: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 8.5109e-05 - mae: 0.0061 - mda: 0.5022 - rmse: 0.0790 - val_loss: 7.6114e-05 - val_mae: 0.0058 - val_mda: 0.5034 - val_rmse: 0.0102 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.9692e-05 - mae: 0.0060 - mda: 0.5034 - rmse: 0.0790\n",
            "Epoch 16: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7.9709e-05 - mae: 0.0060 - mda: 0.5034 - rmse: 0.0790 - val_loss: 1.7129e-04 - val_mae: 0.0095 - val_mda: 0.5031 - val_rmse: 0.0130 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5859e-05 - mae: 0.0059 - mda: 0.5036 - rmse: 0.0784\n",
            "Epoch 17: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7.5888e-05 - mae: 0.0059 - mda: 0.5036 - rmse: 0.0784 - val_loss: 1.0027e-04 - val_mae: 0.0067 - val_mda: 0.5031 - val_rmse: 0.0109 - learning_rate: 2.5000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0715e-05 - mae: 0.0060 - mda: 0.5026 - rmse: 0.0793\n",
            "Epoch 18: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 8.0618e-05 - mae: 0.0060 - mda: 0.5026 - rmse: 0.0793 - val_loss: 1.2522e-04 - val_mae: 0.0081 - val_mda: 0.5043 - val_rmse: 0.0119 - learning_rate: 2.5000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2734e-05 - mae: 0.0057 - mda: 0.5034 - rmse: 0.0780\n",
            "Epoch 19: val_loss did not improve from 0.00007\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7.2693e-05 - mae: 0.0057 - mda: 0.5034 - rmse: 0.0780 - val_loss: 8.8333e-05 - val_mae: 0.0062 - val_mda: 0.5047 - val_rmse: 0.0105 - learning_rate: 2.5000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6120e-05 - mae: 0.0055 - mda: 0.5031 - rmse: 0.0777\n",
            "Epoch 20: val_loss improved from 0.00007 to 0.00006, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6.6149e-05 - mae: 0.0055 - mda: 0.5031 - rmse: 0.0777 - val_loss: 6.1039e-05 - val_mae: 0.0053 - val_mda: 0.5061 - val_rmse: 0.0098 - learning_rate: 1.2500e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5055e-05 - mae: 0.0055 - mda: 0.5021 - rmse: 0.0789\n",
            "Epoch 21: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6.5038e-05 - mae: 0.0055 - mda: 0.5021 - rmse: 0.0788 - val_loss: 9.5759e-05 - val_mae: 0.0069 - val_mda: 0.5061 - val_rmse: 0.0110 - learning_rate: 1.2500e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7108e-05 - mae: 0.0055 - mda: 0.5020 - rmse: 0.0790\n",
            "Epoch 22: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6.7021e-05 - mae: 0.0055 - mda: 0.5020 - rmse: 0.0789 - val_loss: 6.8480e-05 - val_mae: 0.0057 - val_mda: 0.5064 - val_rmse: 0.0102 - learning_rate: 1.2500e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0808e-05 - mae: 0.0052 - mda: 0.5047 - rmse: 0.0783\n",
            "Epoch 23: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6.0762e-05 - mae: 0.0052 - mda: 0.5047 - rmse: 0.0783 - val_loss: 9.6480e-05 - val_mae: 0.0067 - val_mda: 0.5065 - val_rmse: 0.0109 - learning_rate: 1.2500e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m282/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.9045e-05 - mae: 0.0053 - mda: 0.5022 - rmse: 0.0782\n",
            "Epoch 24: val_loss did not improve from 0.00006\n",
            "\n",
            "Epoch 24: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5.9048e-05 - mae: 0.0053 - mda: 0.5022 - rmse: 0.0782 - val_loss: 7.5154e-05 - val_mae: 0.0057 - val_mda: 0.5055 - val_rmse: 0.0102 - learning_rate: 1.2500e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0136e-05 - mae: 0.0052 - mda: 0.5031 - rmse: 0.0786\n",
            "Epoch 25: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6.0079e-05 - mae: 0.0052 - mda: 0.5031 - rmse: 0.0786 - val_loss: 7.6309e-05 - val_mae: 0.0060 - val_mda: 0.5070 - val_rmse: 0.0103 - learning_rate: 6.2500e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5135e-05 - mae: 0.0050 - mda: 0.5033 - rmse: 0.0782\n",
            "Epoch 26: val_loss improved from 0.00006 to 0.00005, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5.5131e-05 - mae: 0.0050 - mda: 0.5033 - rmse: 0.0782 - val_loss: 5.2859e-05 - val_mae: 0.0049 - val_mda: 0.5069 - val_rmse: 0.0096 - learning_rate: 6.2500e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.4877e-05 - mae: 0.0050 - mda: 0.5034 - rmse: 0.0791\n",
            "Epoch 27: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5.4900e-05 - mae: 0.0050 - mda: 0.5034 - rmse: 0.0791 - val_loss: 1.0362e-04 - val_mae: 0.0071 - val_mda: 0.5048 - val_rmse: 0.0112 - learning_rate: 6.2500e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8867e-05 - mae: 0.0048 - mda: 0.5025 - rmse: 0.0778\n",
            "Epoch 28: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.8911e-05 - mae: 0.0048 - mda: 0.5025 - rmse: 0.0779 - val_loss: 6.6114e-05 - val_mae: 0.0055 - val_mda: 0.5055 - val_rmse: 0.0100 - learning_rate: 6.2500e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5204e-05 - mae: 0.0050 - mda: 0.5034 - rmse: 0.0784\n",
            "Epoch 29: val_loss improved from 0.00005 to 0.00005, saving model to best_model.keras\n",
            "\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5.5151e-05 - mae: 0.0050 - mda: 0.5034 - rmse: 0.0784 - val_loss: 4.5614e-05 - val_mae: 0.0045 - val_mda: 0.5068 - val_rmse: 0.0094 - learning_rate: 6.2500e-05\n",
            "Epoch 30/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.3102e-05 - mae: 0.0050 - mda: 0.5032 - rmse: 0.0796\n",
            "Epoch 30: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5.3060e-05 - mae: 0.0050 - mda: 0.5032 - rmse: 0.0796 - val_loss: 7.9199e-05 - val_mae: 0.0062 - val_mda: 0.5066 - val_rmse: 0.0106 - learning_rate: 3.1250e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8446e-05 - mae: 0.0048 - mda: 0.5031 - rmse: 0.0785\n",
            "Epoch 31: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.8431e-05 - mae: 0.0048 - mda: 0.5031 - rmse: 0.0785 - val_loss: 6.9269e-05 - val_mae: 0.0057 - val_mda: 0.5069 - val_rmse: 0.0101 - learning_rate: 3.1250e-05\n",
            "Epoch 32/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6841e-05 - mae: 0.0047 - mda: 0.5036 - rmse: 0.0780\n",
            "Epoch 32: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.6859e-05 - mae: 0.0047 - mda: 0.5036 - rmse: 0.0780 - val_loss: 6.6073e-05 - val_mae: 0.0055 - val_mda: 0.5071 - val_rmse: 0.0100 - learning_rate: 3.1250e-05\n",
            "Epoch 33/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8450e-05 - mae: 0.0048 - mda: 0.5034 - rmse: 0.0785\n",
            "Epoch 33: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.8454e-05 - mae: 0.0048 - mda: 0.5034 - rmse: 0.0785 - val_loss: 1.1076e-04 - val_mae: 0.0076 - val_mda: 0.5072 - val_rmse: 0.0116 - learning_rate: 3.1250e-05\n",
            "Epoch 34/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7937e-05 - mae: 0.0047 - mda: 0.5022 - rmse: 0.0777\n",
            "Epoch 34: val_loss did not improve from 0.00005\n",
            "\n",
            "Epoch 34: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.7935e-05 - mae: 0.0047 - mda: 0.5022 - rmse: 0.0777 - val_loss: 7.8515e-05 - val_mae: 0.0061 - val_mda: 0.5073 - val_rmse: 0.0104 - learning_rate: 3.1250e-05\n",
            "Epoch 35/50\n",
            "\u001b[1m278/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8475e-05 - mae: 0.0047 - mda: 0.5032 - rmse: 0.0788\n",
            "Epoch 35: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.8441e-05 - mae: 0.0047 - mda: 0.5032 - rmse: 0.0788 - val_loss: 6.7340e-05 - val_mae: 0.0056 - val_mda: 0.5069 - val_rmse: 0.0101 - learning_rate: 1.5625e-05\n",
            "Epoch 36/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4470e-05 - mae: 0.0046 - mda: 0.5036 - rmse: 0.0785\n",
            "Epoch 36: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.4499e-05 - mae: 0.0046 - mda: 0.5036 - rmse: 0.0785 - val_loss: 1.0444e-04 - val_mae: 0.0072 - val_mda: 0.5075 - val_rmse: 0.0113 - learning_rate: 1.5625e-05\n",
            "Epoch 37/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2726e-05 - mae: 0.0048 - mda: 0.5032 - rmse: 0.0788\n",
            "Epoch 37: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5.2668e-05 - mae: 0.0048 - mda: 0.5032 - rmse: 0.0788 - val_loss: 7.2498e-05 - val_mae: 0.0059 - val_mda: 0.5077 - val_rmse: 0.0103 - learning_rate: 1.5625e-05\n",
            "Epoch 38/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7374e-05 - mae: 0.0046 - mda: 0.5026 - rmse: 0.0786\n",
            "Epoch 38: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.7360e-05 - mae: 0.0046 - mda: 0.5026 - rmse: 0.0786 - val_loss: 4.9650e-05 - val_mae: 0.0047 - val_mda: 0.5086 - val_rmse: 0.0095 - learning_rate: 1.5625e-05\n",
            "Epoch 39/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3406e-05 - mae: 0.0045 - mda: 0.5028 - rmse: 0.0788\n",
            "Epoch 39: val_loss did not improve from 0.00005\n",
            "\n",
            "Epoch 39: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.3441e-05 - mae: 0.0045 - mda: 0.5028 - rmse: 0.0788 - val_loss: 7.3134e-05 - val_mae: 0.0058 - val_mda: 0.5077 - val_rmse: 0.0103 - learning_rate: 1.5625e-05\n",
            "Epoch 39: early stopping\n",
            "Restoring model weights from the end of the best epoch: 29.\n",
            "Validation Loss: 0.00005, RMSE: 0.00939, MDA: 0.50857, MAE: 0.00453\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0237 - mae: 0.0818 - mda: 0.5004 - rmse: 0.1411\n",
            "Epoch 1: val_loss improved from inf to 0.00171, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0236 - mae: 0.0817 - mda: 0.5004 - rmse: 0.1410 - val_loss: 0.0017 - val_mae: 0.0273 - val_mda: 0.4957 - val_rmse: 0.0305 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.3692e-04 - mae: 0.0205 - mda: 0.5019 - rmse: 0.0831\n",
            "Epoch 2: val_loss improved from 0.00171 to 0.00033, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 9.3322e-04 - mae: 0.0205 - mda: 0.5019 - rmse: 0.0831 - val_loss: 3.2570e-04 - val_mae: 0.0129 - val_mda: 0.4979 - val_rmse: 0.0162 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.7382e-04 - mae: 0.0145 - mda: 0.5025 - rmse: 0.0808\n",
            "Epoch 3: val_loss improved from 0.00033 to 0.00009, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 4.7281e-04 - mae: 0.0145 - mda: 0.5025 - rmse: 0.0808 - val_loss: 9.3319e-05 - val_mae: 0.0079 - val_mda: 0.4980 - val_rmse: 0.0118 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7988e-04 - mae: 0.0113 - mda: 0.5019 - rmse: 0.0803\n",
            "Epoch 4: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.7960e-04 - mae: 0.0113 - mda: 0.5019 - rmse: 0.0803 - val_loss: 2.8527e-04 - val_mae: 0.0122 - val_mda: 0.4983 - val_rmse: 0.0156 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.0156e-04 - mae: 0.0097 - mda: 0.5026 - rmse: 0.0789\n",
            "Epoch 5: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.0135e-04 - mae: 0.0097 - mda: 0.5026 - rmse: 0.0789 - val_loss: 1.3880e-04 - val_mae: 0.0086 - val_mda: 0.5015 - val_rmse: 0.0124 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.5657e-04 - mae: 0.0086 - mda: 0.5019 - rmse: 0.0800\n",
            "Epoch 6: val_loss improved from 0.00009 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.5639e-04 - mae: 0.0086 - mda: 0.5020 - rmse: 0.0800 - val_loss: 3.5462e-05 - val_mae: 0.0046 - val_mda: 0.5017 - val_rmse: 0.0094 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.3973e-04 - mae: 0.0080 - mda: 0.5029 - rmse: 0.0799\n",
            "Epoch 7: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.3958e-04 - mae: 0.0080 - mda: 0.5029 - rmse: 0.0799 - val_loss: 6.6143e-05 - val_mae: 0.0060 - val_mda: 0.5010 - val_rmse: 0.0104 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.1835e-04 - mae: 0.0072 - mda: 0.5036 - rmse: 0.0786\n",
            "Epoch 8: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.1811e-04 - mae: 0.0072 - mda: 0.5036 - rmse: 0.0786 - val_loss: 1.0239e-04 - val_mae: 0.0079 - val_mda: 0.5004 - val_rmse: 0.0118 - learning_rate: 0.0010\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.1475e-05 - mae: 0.0065 - mda: 0.5036 - rmse: 0.0785\n",
            "Epoch 9: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 9.1379e-05 - mae: 0.0065 - mda: 0.5036 - rmse: 0.0785 - val_loss: 7.4198e-05 - val_mae: 0.0066 - val_mda: 0.5001 - val_rmse: 0.0109 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.4682e-05 - mae: 0.0061 - mda: 0.5030 - rmse: 0.0789\n",
            "Epoch 10: val_loss improved from 0.00004 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 7.4681e-05 - mae: 0.0060 - mda: 0.5030 - rmse: 0.0789 - val_loss: 2.7822e-05 - val_mae: 0.0039 - val_mda: 0.5029 - val_rmse: 0.0090 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.3652e-05 - mae: 0.0059 - mda: 0.5028 - rmse: 0.0787\n",
            "Epoch 11: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 7.3675e-05 - mae: 0.0059 - mda: 0.5028 - rmse: 0.0787 - val_loss: 6.1377e-05 - val_mae: 0.0059 - val_mda: 0.5036 - val_rmse: 0.0104 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.9405e-05 - mae: 0.0058 - mda: 0.5027 - rmse: 0.0787\n",
            "Epoch 12: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 6.9347e-05 - mae: 0.0058 - mda: 0.5027 - rmse: 0.0787 - val_loss: 5.1587e-05 - val_mae: 0.0052 - val_mda: 0.5020 - val_rmse: 0.0099 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7019e-05 - mae: 0.0056 - mda: 0.5037 - rmse: 0.0787\n",
            "Epoch 13: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 6.7000e-05 - mae: 0.0056 - mda: 0.5037 - rmse: 0.0787 - val_loss: 3.2430e-05 - val_mae: 0.0045 - val_mda: 0.5020 - val_rmse: 0.0095 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.1323e-05 - mae: 0.0054 - mda: 0.5023 - rmse: 0.0784\n",
            "Epoch 14: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 6.1279e-05 - mae: 0.0054 - mda: 0.5023 - rmse: 0.0784 - val_loss: 2.8476e-05 - val_mae: 0.0039 - val_mda: 0.5052 - val_rmse: 0.0090 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.7495e-05 - mae: 0.0053 - mda: 0.5031 - rmse: 0.0790\n",
            "Epoch 15: val_loss improved from 0.00003 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 5.7485e-05 - mae: 0.0053 - mda: 0.5031 - rmse: 0.0790 - val_loss: 2.0240e-05 - val_mae: 0.0034 - val_mda: 0.5029 - val_rmse: 0.0087 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.5413e-05 - mae: 0.0051 - mda: 0.5027 - rmse: 0.0786\n",
            "Epoch 16: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 5.5401e-05 - mae: 0.0051 - mda: 0.5027 - rmse: 0.0786 - val_loss: 6.4141e-05 - val_mae: 0.0061 - val_mda: 0.5044 - val_rmse: 0.0104 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.8556e-05 - mae: 0.0048 - mda: 0.5033 - rmse: 0.0778\n",
            "Epoch 17: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 4.8572e-05 - mae: 0.0048 - mda: 0.5033 - rmse: 0.0778 - val_loss: 2.0315e-05 - val_mae: 0.0033 - val_mda: 0.5038 - val_rmse: 0.0087 - learning_rate: 2.5000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.8506e-05 - mae: 0.0049 - mda: 0.5040 - rmse: 0.0788\n",
            "Epoch 18: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 4.8506e-05 - mae: 0.0049 - mda: 0.5040 - rmse: 0.0788 - val_loss: 5.1472e-05 - val_mae: 0.0059 - val_mda: 0.5039 - val_rmse: 0.0104 - learning_rate: 2.5000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6068e-05 - mae: 0.0047 - mda: 0.5028 - rmse: 0.0781\n",
            "Epoch 19: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 4.6077e-05 - mae: 0.0047 - mda: 0.5028 - rmse: 0.0781 - val_loss: 7.8400e-05 - val_mae: 0.0070 - val_mda: 0.5033 - val_rmse: 0.0111 - learning_rate: 1.2500e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.3059e-05 - mae: 0.0045 - mda: 0.5029 - rmse: 0.0789\n",
            "Epoch 20: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 4.3091e-05 - mae: 0.0045 - mda: 0.5029 - rmse: 0.0788 - val_loss: 2.5195e-05 - val_mae: 0.0036 - val_mda: 0.5046 - val_rmse: 0.0090 - learning_rate: 1.2500e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.4713e-05 - mae: 0.0046 - mda: 0.5038 - rmse: 0.0783\n",
            "Epoch 21: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 4.4676e-05 - mae: 0.0046 - mda: 0.5038 - rmse: 0.0783 - val_loss: 2.1037e-05 - val_mae: 0.0033 - val_mda: 0.5055 - val_rmse: 0.0088 - learning_rate: 1.2500e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.4496e-05 - mae: 0.0046 - mda: 0.5034 - rmse: 0.0785\n",
            "Epoch 22: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 4.4480e-05 - mae: 0.0046 - mda: 0.5034 - rmse: 0.0785 - val_loss: 8.4361e-05 - val_mae: 0.0075 - val_mda: 0.5051 - val_rmse: 0.0116 - learning_rate: 1.2500e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.8554e-05 - mae: 0.0044 - mda: 0.5037 - rmse: 0.0782\n",
            "Epoch 23: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 23: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.8577e-05 - mae: 0.0044 - mda: 0.5037 - rmse: 0.0782 - val_loss: 3.7893e-05 - val_mae: 0.0047 - val_mda: 0.5050 - val_rmse: 0.0096 - learning_rate: 1.2500e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.6854e-05 - mae: 0.0043 - mda: 0.5042 - rmse: 0.0780\n",
            "Epoch 24: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.6854e-05 - mae: 0.0043 - mda: 0.5042 - rmse: 0.0780 - val_loss: 2.3144e-05 - val_mae: 0.0036 - val_mda: 0.5058 - val_rmse: 0.0089 - learning_rate: 6.2500e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.8140e-05 - mae: 0.0043 - mda: 0.5037 - rmse: 0.0785\n",
            "Epoch 25: val_loss improved from 0.00002 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.8143e-05 - mae: 0.0043 - mda: 0.5037 - rmse: 0.0785 - val_loss: 1.6462e-05 - val_mae: 0.0030 - val_mda: 0.5049 - val_rmse: 0.0086 - learning_rate: 6.2500e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.8715e-05 - mae: 0.0043 - mda: 0.5044 - rmse: 0.0791\n",
            "Epoch 26: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.8703e-05 - mae: 0.0043 - mda: 0.5044 - rmse: 0.0790 - val_loss: 3.5461e-05 - val_mae: 0.0044 - val_mda: 0.5030 - val_rmse: 0.0094 - learning_rate: 6.2500e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.8012e-05 - mae: 0.0043 - mda: 0.5039 - rmse: 0.0798\n",
            "Epoch 27: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.8006e-05 - mae: 0.0043 - mda: 0.5039 - rmse: 0.0797 - val_loss: 1.9369e-05 - val_mae: 0.0032 - val_mda: 0.5048 - val_rmse: 0.0087 - learning_rate: 6.2500e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.7442e-05 - mae: 0.0042 - mda: 0.5031 - rmse: 0.0784\n",
            "Epoch 28: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 28: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.7437e-05 - mae: 0.0042 - mda: 0.5031 - rmse: 0.0784 - val_loss: 2.6883e-05 - val_mae: 0.0039 - val_mda: 0.5054 - val_rmse: 0.0092 - learning_rate: 6.2500e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.5839e-05 - mae: 0.0041 - mda: 0.5045 - rmse: 0.0784\n",
            "Epoch 29: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.5843e-05 - mae: 0.0041 - mda: 0.5044 - rmse: 0.0784 - val_loss: 1.8257e-05 - val_mae: 0.0031 - val_mda: 0.5040 - val_rmse: 0.0087 - learning_rate: 3.1250e-05\n",
            "Epoch 30/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.4177e-05 - mae: 0.0041 - mda: 0.5020 - rmse: 0.0792\n",
            "Epoch 30: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.4174e-05 - mae: 0.0041 - mda: 0.5020 - rmse: 0.0792 - val_loss: 2.1445e-05 - val_mae: 0.0034 - val_mda: 0.5044 - val_rmse: 0.0089 - learning_rate: 3.1250e-05\n",
            "Epoch 31/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.1696e-05 - mae: 0.0040 - mda: 0.5042 - rmse: 0.0787\n",
            "Epoch 31: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.1719e-05 - mae: 0.0040 - mda: 0.5042 - rmse: 0.0787 - val_loss: 1.6862e-05 - val_mae: 0.0030 - val_mda: 0.5062 - val_rmse: 0.0087 - learning_rate: 3.1250e-05\n",
            "Epoch 32/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.0760e-05 - mae: 0.0039 - mda: 0.5024 - rmse: 0.0792\n",
            "Epoch 32: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.0775e-05 - mae: 0.0039 - mda: 0.5024 - rmse: 0.0792 - val_loss: 1.8234e-05 - val_mae: 0.0031 - val_mda: 0.5062 - val_rmse: 0.0087 - learning_rate: 3.1250e-05\n",
            "Epoch 33/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.1871e-05 - mae: 0.0039 - mda: 0.5032 - rmse: 0.0783\n",
            "Epoch 33: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 33: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.1892e-05 - mae: 0.0039 - mda: 0.5032 - rmse: 0.0783 - val_loss: 1.7262e-05 - val_mae: 0.0031 - val_mda: 0.5054 - val_rmse: 0.0087 - learning_rate: 3.1250e-05\n",
            "Epoch 34/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.1682e-05 - mae: 0.0039 - mda: 0.5032 - rmse: 0.0790\n",
            "Epoch 34: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.1693e-05 - mae: 0.0039 - mda: 0.5032 - rmse: 0.0790 - val_loss: 2.5309e-05 - val_mae: 0.0038 - val_mda: 0.5054 - val_rmse: 0.0091 - learning_rate: 1.5625e-05\n",
            "Epoch 35/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.3224e-05 - mae: 0.0039 - mda: 0.5043 - rmse: 0.0790\n",
            "Epoch 35: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.3193e-05 - mae: 0.0039 - mda: 0.5043 - rmse: 0.0790 - val_loss: 1.9951e-05 - val_mae: 0.0033 - val_mda: 0.5047 - val_rmse: 0.0089 - learning_rate: 1.5625e-05\n",
            "Epoch 35: early stopping\n",
            "Restoring model weights from the end of the best epoch: 25.\n",
            "Validation Loss: 0.00002, RMSE: 0.00862, MDA: 0.50623, MAE: 0.00300\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0352 - mae: 0.1039 - mda: 0.5005 - rmse: 0.1612\n",
            "Epoch 1: val_loss improved from inf to 0.00188, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0347 - mae: 0.1027 - mda: 0.5005 - rmse: 0.1600 - val_loss: 0.0019 - val_mae: 0.0276 - val_mda: 0.4976 - val_rmse: 0.0305 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3868e-04 - mae: 0.0157 - mda: 0.5023 - rmse: 0.0797\n",
            "Epoch 2: val_loss improved from 0.00188 to 0.00097, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.3673e-04 - mae: 0.0156 - mda: 0.5023 - rmse: 0.0797 - val_loss: 9.7221e-04 - val_mae: 0.0209 - val_mda: 0.4989 - val_rmse: 0.0237 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m282/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2311e-04 - mae: 0.0110 - mda: 0.5015 - rmse: 0.0773\n",
            "Epoch 3: val_loss improved from 0.00097 to 0.00029, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.2211e-04 - mae: 0.0109 - mda: 0.5016 - rmse: 0.0774 - val_loss: 2.8818e-04 - val_mae: 0.0105 - val_mda: 0.4991 - val_rmse: 0.0140 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4727e-04 - mae: 0.0090 - mda: 0.5029 - rmse: 0.0786\n",
            "Epoch 4: val_loss did not improve from 0.00029\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.4720e-04 - mae: 0.0090 - mda: 0.5029 - rmse: 0.0786 - val_loss: 3.6634e-04 - val_mae: 0.0121 - val_mda: 0.4984 - val_rmse: 0.0156 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1632e-04 - mae: 0.0079 - mda: 0.5017 - rmse: 0.0786\n",
            "Epoch 5: val_loss did not improve from 0.00029\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1613e-04 - mae: 0.0079 - mda: 0.5018 - rmse: 0.0786 - val_loss: 3.2402e-04 - val_mae: 0.0115 - val_mda: 0.5014 - val_rmse: 0.0151 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m282/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.4830e-05 - mae: 0.0070 - mda: 0.5026 - rmse: 0.0770\n",
            "Epoch 6: val_loss improved from 0.00029 to 0.00024, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 9.4926e-05 - mae: 0.0070 - mda: 0.5026 - rmse: 0.0770 - val_loss: 2.3755e-04 - val_mae: 0.0102 - val_mda: 0.5015 - val_rmse: 0.0139 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.9016e-05 - mae: 0.0069 - mda: 0.5023 - rmse: 0.0787\n",
            "Epoch 7: val_loss did not improve from 0.00024\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.9038e-05 - mae: 0.0069 - mda: 0.5023 - rmse: 0.0787 - val_loss: 7.0126e-04 - val_mae: 0.0183 - val_mda: 0.5018 - val_rmse: 0.0211 - learning_rate: 0.0100\n",
            "Epoch 8/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2091e-05 - mae: 0.0062 - mda: 0.5031 - rmse: 0.0778\n",
            "Epoch 8: val_loss did not improve from 0.00024\n",
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7.2112e-05 - mae: 0.0062 - mda: 0.5031 - rmse: 0.0778 - val_loss: 8.5414e-04 - val_mae: 0.0210 - val_mda: 0.5024 - val_rmse: 0.0236 - learning_rate: 0.0100\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "\u001b[1m276/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6880e-05 - mae: 0.0059 - mda: 0.5042 - rmse: 0.0785\n",
            "Epoch 9: val_loss did not improve from 0.00024\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.6795e-05 - mae: 0.0059 - mda: 0.5042 - rmse: 0.0785 - val_loss: 4.4874e-04 - val_mae: 0.0141 - val_mda: 0.5036 - val_rmse: 0.0174 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1404e-05 - mae: 0.0056 - mda: 0.5035 - rmse: 0.0790\n",
            "Epoch 10: val_loss did not improve from 0.00024\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.1408e-05 - mae: 0.0056 - mda: 0.5035 - rmse: 0.0790 - val_loss: 8.1020e-04 - val_mae: 0.0202 - val_mda: 0.5029 - val_rmse: 0.0229 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.2211e-05 - mae: 0.0056 - mda: 0.5037 - rmse: 0.0779\n",
            "Epoch 11: val_loss did not improve from 0.00024\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.2201e-05 - mae: 0.0056 - mda: 0.5037 - rmse: 0.0779 - val_loss: 6.0566e-04 - val_mae: 0.0146 - val_mda: 0.5037 - val_rmse: 0.0182 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1635e-05 - mae: 0.0056 - mda: 0.5033 - rmse: 0.0784\n",
            "Epoch 12: val_loss did not improve from 0.00024\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.1618e-05 - mae: 0.0056 - mda: 0.5033 - rmse: 0.0784 - val_loss: 4.8738e-04 - val_mae: 0.0136 - val_mda: 0.5048 - val_rmse: 0.0171 - learning_rate: 0.0050\n",
            "Epoch 13/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9454e-05 - mae: 0.0056 - mda: 0.5024 - rmse: 0.0791\n",
            "Epoch 13: val_loss did not improve from 0.00024\n",
            "\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 5.9437e-05 - mae: 0.0056 - mda: 0.5024 - rmse: 0.0791 - val_loss: 3.2543e-04 - val_mae: 0.0100 - val_mda: 0.5045 - val_rmse: 0.0142 - learning_rate: 0.0050\n",
            "Epoch 14/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1947e-05 - mae: 0.0052 - mda: 0.5027 - rmse: 0.0784\n",
            "Epoch 14: val_loss did not improve from 0.00024\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 5.1919e-05 - mae: 0.0052 - mda: 0.5027 - rmse: 0.0784 - val_loss: 6.6942e-04 - val_mae: 0.0170 - val_mda: 0.5041 - val_rmse: 0.0202 - learning_rate: 0.0025\n",
            "Epoch 15/50\n",
            "\u001b[1m280/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0476e-05 - mae: 0.0051 - mda: 0.5026 - rmse: 0.0781\n",
            "Epoch 15: val_loss did not improve from 0.00024\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 5.0562e-05 - mae: 0.0051 - mda: 0.5026 - rmse: 0.0781 - val_loss: 3.8978e-04 - val_mae: 0.0110 - val_mda: 0.5062 - val_rmse: 0.0151 - learning_rate: 0.0025\n",
            "Epoch 16/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0206e-05 - mae: 0.0051 - mda: 0.5041 - rmse: 0.0786\n",
            "Epoch 16: val_loss did not improve from 0.00024\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 5.0217e-05 - mae: 0.0051 - mda: 0.5041 - rmse: 0.0786 - val_loss: 5.3439e-04 - val_mae: 0.0139 - val_mda: 0.5079 - val_rmse: 0.0175 - learning_rate: 0.0025\n",
            "Epoch 16: early stopping\n",
            "Restoring model weights from the end of the best epoch: 6.\n",
            "Validation Loss: 0.00024, RMSE: 0.01390, MDA: 0.50785, MAE: 0.01004\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7544 - mae: 0.5728 - mda: 0.5003 - rmse: 0.6541\n",
            "Epoch 1: val_loss improved from inf to 0.04348, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.7453 - mae: 0.5677 - mda: 0.5003 - rmse: 0.6485 - val_loss: 0.0435 - val_mae: 0.1822 - val_mda: 0.5004 - val_rmse: 0.1845 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0089 - mae: 0.0722 - mda: 0.4996 - rmse: 0.0958\n",
            "Epoch 2: val_loss improved from 0.04348 to 0.01249, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0087 - mae: 0.0716 - mda: 0.4996 - rmse: 0.0954 - val_loss: 0.0125 - val_mae: 0.0704 - val_mda: 0.5003 - val_rmse: 0.0732 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6897e-04 - mae: 0.0169 - mda: 0.5012 - rmse: 0.0768\n",
            "Epoch 3: val_loss did not improve from 0.01249\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.6722e-04 - mae: 0.0169 - mda: 0.5012 - rmse: 0.0768 - val_loss: 0.0129 - val_mae: 0.0713 - val_mda: 0.5012 - val_rmse: 0.0742 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4701e-04 - mae: 0.0144 - mda: 0.5020 - rmse: 0.0776\n",
            "Epoch 4: val_loss did not improve from 0.01249\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.4677e-04 - mae: 0.0144 - mda: 0.5020 - rmse: 0.0776 - val_loss: 0.0137 - val_mae: 0.0745 - val_mda: 0.5027 - val_rmse: 0.0774 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2109e-04 - mae: 0.0139 - mda: 0.5017 - rmse: 0.0768\n",
            "Epoch 5: val_loss improved from 0.01249 to 0.01231, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.2129e-04 - mae: 0.0139 - mda: 0.5017 - rmse: 0.0768 - val_loss: 0.0123 - val_mae: 0.0670 - val_mda: 0.5027 - val_rmse: 0.0700 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9515e-04 - mae: 0.0134 - mda: 0.5028 - rmse: 0.0778\n",
            "Epoch 6: val_loss did not improve from 0.01231\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.9512e-04 - mae: 0.0134 - mda: 0.5028 - rmse: 0.0778 - val_loss: 0.0127 - val_mae: 0.0687 - val_mda: 0.5030 - val_rmse: 0.0718 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8374e-04 - mae: 0.0132 - mda: 0.5015 - rmse: 0.0771\n",
            "Epoch 7: val_loss did not improve from 0.01231\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.8357e-04 - mae: 0.0132 - mda: 0.5015 - rmse: 0.0771 - val_loss: 0.0126 - val_mae: 0.0678 - val_mda: 0.5027 - val_rmse: 0.0708 - learning_rate: 0.0100\n",
            "Epoch 8/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5925e-04 - mae: 0.0125 - mda: 0.5025 - rmse: 0.0776\n",
            "Epoch 8: val_loss improved from 0.01231 to 0.01177, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.5922e-04 - mae: 0.0125 - mda: 0.5025 - rmse: 0.0776 - val_loss: 0.0118 - val_mae: 0.0641 - val_mda: 0.5026 - val_rmse: 0.0670 - learning_rate: 0.0100\n",
            "Epoch 9/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4369e-04 - mae: 0.0121 - mda: 0.5029 - rmse: 0.0774\n",
            "Epoch 9: val_loss did not improve from 0.01177\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.4409e-04 - mae: 0.0121 - mda: 0.5029 - rmse: 0.0774 - val_loss: 0.0143 - val_mae: 0.0782 - val_mda: 0.5034 - val_rmse: 0.0810 - learning_rate: 0.0100\n",
            "Epoch 10/50\n",
            "\u001b[1m282/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5066e-04 - mae: 0.0123 - mda: 0.5026 - rmse: 0.0775\n",
            "Epoch 10: val_loss did not improve from 0.01177\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.5033e-04 - mae: 0.0123 - mda: 0.5026 - rmse: 0.0775 - val_loss: 0.0124 - val_mae: 0.0670 - val_mda: 0.5036 - val_rmse: 0.0700 - learning_rate: 0.0100\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6833e-04 - mae: 0.0128 - mda: 0.5020 - rmse: 0.0778\n",
            "Epoch 11: val_loss improved from 0.01177 to 0.01085, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.6835e-04 - mae: 0.0128 - mda: 0.5020 - rmse: 0.0778 - val_loss: 0.0108 - val_mae: 0.0599 - val_mda: 0.5032 - val_rmse: 0.0629 - learning_rate: 0.0100\n",
            "Epoch 12/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5831e-04 - mae: 0.0126 - mda: 0.5027 - rmse: 0.0781\n",
            "Epoch 12: val_loss did not improve from 0.01085\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.5872e-04 - mae: 0.0126 - mda: 0.5027 - rmse: 0.0781 - val_loss: 0.0132 - val_mae: 0.0725 - val_mda: 0.5037 - val_rmse: 0.0753 - learning_rate: 0.0100\n",
            "Epoch 13/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6276e-04 - mae: 0.0127 - mda: 0.5021 - rmse: 0.0779\n",
            "Epoch 13: val_loss did not improve from 0.01085\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.6292e-04 - mae: 0.0127 - mda: 0.5021 - rmse: 0.0779 - val_loss: 0.0111 - val_mae: 0.0629 - val_mda: 0.5048 - val_rmse: 0.0658 - learning_rate: 0.0100\n",
            "Epoch 14/50\n",
            "\u001b[1m280/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1110e-04 - mae: 0.0139 - mda: 0.5018 - rmse: 0.0785\n",
            "Epoch 14: val_loss improved from 0.01085 to 0.00954, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.1163e-04 - mae: 0.0139 - mda: 0.5018 - rmse: 0.0785 - val_loss: 0.0095 - val_mae: 0.0628 - val_mda: 0.5061 - val_rmse: 0.0653 - learning_rate: 0.0100\n",
            "Epoch 15/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7590e-04 - mae: 0.0154 - mda: 0.5025 - rmse: 0.0791\n",
            "Epoch 15: val_loss did not improve from 0.00954\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.7483e-04 - mae: 0.0154 - mda: 0.5025 - rmse: 0.0791 - val_loss: 0.0132 - val_mae: 0.0737 - val_mda: 0.5055 - val_rmse: 0.0766 - learning_rate: 0.0100\n",
            "Epoch 16/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9349e-04 - mae: 0.0135 - mda: 0.5022 - rmse: 0.0791\n",
            "Epoch 16: val_loss did not improve from 0.00954\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.9424e-04 - mae: 0.0135 - mda: 0.5022 - rmse: 0.0791 - val_loss: 0.0105 - val_mae: 0.0627 - val_mda: 0.5075 - val_rmse: 0.0654 - learning_rate: 0.0100\n",
            "Epoch 17/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2335e-04 - mae: 0.0142 - mda: 0.5012 - rmse: 0.0784\n",
            "Epoch 17: val_loss did not improve from 0.00954\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.2538e-04 - mae: 0.0142 - mda: 0.5012 - rmse: 0.0784 - val_loss: 0.0104 - val_mae: 0.0591 - val_mda: 0.5032 - val_rmse: 0.0620 - learning_rate: 0.0100\n",
            "Epoch 18/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.3840e-04 - mae: 0.0186 - mda: 0.5020 - rmse: 0.0800\n",
            "Epoch 18: val_loss did not improve from 0.00954\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5.3972e-04 - mae: 0.0187 - mda: 0.5020 - rmse: 0.0800 - val_loss: 0.0123 - val_mae: 0.0706 - val_mda: 0.5020 - val_rmse: 0.0733 - learning_rate: 0.0100\n",
            "Epoch 19/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.8966e-04 - mae: 0.0186 - mda: 0.5009 - rmse: 0.0804\n",
            "Epoch 19: val_loss did not improve from 0.00954\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5.8897e-04 - mae: 0.0186 - mda: 0.5009 - rmse: 0.0804 - val_loss: 0.0132 - val_mae: 0.0729 - val_mda: 0.5048 - val_rmse: 0.0758 - learning_rate: 0.0100\n",
            "Epoch 20/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9860e-04 - mae: 0.0110 - mda: 0.5020 - rmse: 0.0779\n",
            "Epoch 20: val_loss did not improve from 0.00954\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.9839e-04 - mae: 0.0110 - mda: 0.5020 - rmse: 0.0779 - val_loss: 0.0098 - val_mae: 0.0555 - val_mda: 0.5071 - val_rmse: 0.0585 - learning_rate: 0.0050\n",
            "Epoch 21/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8439e-04 - mae: 0.0107 - mda: 0.5017 - rmse: 0.0783\n",
            "Epoch 21: val_loss did not improve from 0.00954\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.8547e-04 - mae: 0.0108 - mda: 0.5017 - rmse: 0.0783 - val_loss: 0.0097 - val_mae: 0.0558 - val_mda: 0.5068 - val_rmse: 0.0589 - learning_rate: 0.0050\n",
            "Epoch 22/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3964e-04 - mae: 0.0121 - mda: 0.5021 - rmse: 0.0794\n",
            "Epoch 22: val_loss did not improve from 0.00954\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.3994e-04 - mae: 0.0121 - mda: 0.5022 - rmse: 0.0794 - val_loss: 0.0137 - val_mae: 0.0729 - val_mda: 0.5085 - val_rmse: 0.0758 - learning_rate: 0.0050\n",
            "Epoch 23/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7194e-04 - mae: 0.0129 - mda: 0.5024 - rmse: 0.0784\n",
            "Epoch 23: val_loss did not improve from 0.00954\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.7188e-04 - mae: 0.0129 - mda: 0.5024 - rmse: 0.0784 - val_loss: 0.0124 - val_mae: 0.0683 - val_mda: 0.4991 - val_rmse: 0.0713 - learning_rate: 0.0050\n",
            "Epoch 24/50\n",
            "\u001b[1m281/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9595e-04 - mae: 0.0157 - mda: 0.5020 - rmse: 0.0797\n",
            "Epoch 24: val_loss did not improve from 0.00954\n",
            "\n",
            "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.9429e-04 - mae: 0.0156 - mda: 0.5020 - rmse: 0.0797 - val_loss: 0.0125 - val_mae: 0.0694 - val_mda: 0.5074 - val_rmse: 0.0724 - learning_rate: 0.0050\n",
            "Epoch 24: early stopping\n",
            "Restoring model weights from the end of the best epoch: 14.\n",
            "Validation Loss: 0.00954, RMSE: 0.05852, MDA: 0.50852, MAE: 0.05547\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5000 - mae: 0.4389 - mda: 0.4996 - rmse: 0.5226\n",
            "Epoch 1: val_loss improved from inf to 0.04377, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.4989 - mae: 0.4383 - mda: 0.4996 - rmse: 0.5219 - val_loss: 0.0438 - val_mae: 0.1829 - val_mda: 0.4980 - val_rmse: 0.1852 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0053 - mae: 0.0583 - mda: 0.4994 - rmse: 0.0714\n",
            "Epoch 2: val_loss did not improve from 0.04377\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0053 - mae: 0.0584 - mda: 0.4994 - rmse: 0.0715 - val_loss: 0.1096 - val_mae: 0.3141 - val_mda: 0.4969 - val_rmse: 0.3164 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0084 - mae: 0.0726 - mda: 0.5003 - rmse: 0.0875\n",
            "Epoch 3: val_loss improved from 0.04377 to 0.03564, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0083 - mae: 0.0723 - mda: 0.5003 - rmse: 0.0872 - val_loss: 0.0356 - val_mae: 0.1592 - val_mda: 0.5022 - val_rmse: 0.1615 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0041 - mae: 0.0529 - mda: 0.5001 - rmse: 0.0627\n",
            "Epoch 4: val_loss improved from 0.03564 to 0.02533, saving model to best_model.keras\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0041 - mae: 0.0530 - mda: 0.5001 - rmse: 0.0627 - val_loss: 0.0253 - val_mae: 0.1235 - val_mda: 0.4991 - val_rmse: 0.1261 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0045 - mae: 0.0555 - mda: 0.4998 - rmse: 0.0664\n",
            "Epoch 5: val_loss did not improve from 0.02533\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0045 - mae: 0.0556 - mda: 0.4998 - rmse: 0.0665 - val_loss: 0.0698 - val_mae: 0.2435 - val_mda: 0.5047 - val_rmse: 0.2457 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0063 - mae: 0.0644 - mda: 0.5006 - rmse: 0.0769\n",
            "Epoch 6: val_loss improved from 0.02533 to 0.02394, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0063 - mae: 0.0643 - mda: 0.5006 - rmse: 0.0768 - val_loss: 0.0239 - val_mae: 0.1188 - val_mda: 0.5065 - val_rmse: 0.1213 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0045 - mae: 0.0549 - mda: 0.5002 - rmse: 0.0679\n",
            "Epoch 7: val_loss improved from 0.02394 to 0.02187, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0046 - mae: 0.0549 - mda: 0.5002 - rmse: 0.0679 - val_loss: 0.0219 - val_mae: 0.1209 - val_mda: 0.5099 - val_rmse: 0.1231 - learning_rate: 0.0100\n",
            "Epoch 8/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0022 - mae: 0.0364 - mda: 0.5007 - rmse: 0.0762\n",
            "Epoch 8: val_loss improved from 0.02187 to 0.01977, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0022 - mae: 0.0362 - mda: 0.5007 - rmse: 0.0762 - val_loss: 0.0198 - val_mae: 0.1049 - val_mda: 0.5106 - val_rmse: 0.1076 - learning_rate: 0.0100\n",
            "Epoch 9/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0016 - mae: 0.0316 - mda: 0.5015 - rmse: 0.0811\n",
            "Epoch 9: val_loss improved from 0.01977 to 0.01008, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0016 - mae: 0.0317 - mda: 0.5015 - rmse: 0.0812 - val_loss: 0.0101 - val_mae: 0.0669 - val_mda: 0.5081 - val_rmse: 0.0695 - learning_rate: 0.0100\n",
            "Epoch 10/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0026 - mae: 0.0404 - mda: 0.5007 - rmse: 0.0853\n",
            "Epoch 10: val_loss did not improve from 0.01008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0026 - mae: 0.0403 - mda: 0.5007 - rmse: 0.0852 - val_loss: 0.0127 - val_mae: 0.0699 - val_mda: 0.5092 - val_rmse: 0.0727 - learning_rate: 0.0100\n",
            "Epoch 11/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0012 - mae: 0.0271 - mda: 0.5020 - rmse: 0.0808\n",
            "Epoch 11: val_loss did not improve from 0.01008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0012 - mae: 0.0270 - mda: 0.5020 - rmse: 0.0807 - val_loss: 0.0114 - val_mae: 0.0635 - val_mda: 0.5099 - val_rmse: 0.0666 - learning_rate: 0.0100\n",
            "Epoch 12/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.4385e-04 - mae: 0.0199 - mda: 0.5014 - rmse: 0.0791\n",
            "Epoch 12: val_loss did not improve from 0.01008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 6.5404e-04 - mae: 0.0200 - mda: 0.5014 - rmse: 0.0792 - val_loss: 0.0888 - val_mae: 0.2892 - val_mda: 0.5090 - val_rmse: 0.2910 - learning_rate: 0.0100\n",
            "Epoch 13/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0189 - mae: 0.1067 - mda: 0.4993 - rmse: 0.1220\n",
            "Epoch 13: val_loss did not improve from 0.01008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0188 - mae: 0.1062 - mda: 0.4993 - rmse: 0.1215 - val_loss: 0.0759 - val_mae: 0.2574 - val_mda: 0.5067 - val_rmse: 0.2596 - learning_rate: 0.0100\n",
            "Epoch 14/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0064 - mae: 0.0643 - mda: 0.5007 - rmse: 0.0836\n",
            "Epoch 14: val_loss did not improve from 0.01008\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0064 - mae: 0.0642 - mda: 0.5007 - rmse: 0.0835 - val_loss: 0.0215 - val_mae: 0.1183 - val_mda: 0.5062 - val_rmse: 0.1205 - learning_rate: 0.0100\n",
            "Epoch 15/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0018 - mae: 0.0329 - mda: 0.5007 - rmse: 0.0742\n",
            "Epoch 15: val_loss did not improve from 0.01008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0018 - mae: 0.0328 - mda: 0.5007 - rmse: 0.0742 - val_loss: 0.0113 - val_mae: 0.0633 - val_mda: 0.5113 - val_rmse: 0.0667 - learning_rate: 0.0050\n",
            "Epoch 16/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.3706e-04 - mae: 0.0186 - mda: 0.5018 - rmse: 0.0774\n",
            "Epoch 16: val_loss did not improve from 0.01008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 5.3711e-04 - mae: 0.0186 - mda: 0.5018 - rmse: 0.0774 - val_loss: 0.0126 - val_mae: 0.0694 - val_mda: 0.5120 - val_rmse: 0.0725 - learning_rate: 0.0050\n",
            "Epoch 17/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.1336e-04 - mae: 0.0162 - mda: 0.5016 - rmse: 0.0777\n",
            "Epoch 17: val_loss did not improve from 0.01008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 4.1280e-04 - mae: 0.0162 - mda: 0.5016 - rmse: 0.0777 - val_loss: 0.0174 - val_mae: 0.0968 - val_mda: 0.5107 - val_rmse: 0.0994 - learning_rate: 0.0050\n",
            "Epoch 18/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.0808e-04 - mae: 0.0176 - mda: 0.5022 - rmse: 0.0787\n",
            "Epoch 18: val_loss did not improve from 0.01008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 5.0583e-04 - mae: 0.0175 - mda: 0.5022 - rmse: 0.0787 - val_loss: 0.0103 - val_mae: 0.0610 - val_mda: 0.5095 - val_rmse: 0.0639 - learning_rate: 0.0050\n",
            "Epoch 19/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6147e-04 - mae: 0.0170 - mda: 0.5020 - rmse: 0.0783\n",
            "Epoch 19: val_loss improved from 0.01008 to 0.01005, saving model to best_model.keras\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 4.6124e-04 - mae: 0.0170 - mda: 0.5020 - rmse: 0.0783 - val_loss: 0.0101 - val_mae: 0.0614 - val_mda: 0.5098 - val_rmse: 0.0642 - learning_rate: 0.0050\n",
            "Epoch 20/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.1864e-04 - mae: 0.0116 - mda: 0.5017 - rmse: 0.0786\n",
            "Epoch 20: val_loss improved from 0.01005 to 0.00895, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.1863e-04 - mae: 0.0116 - mda: 0.5017 - rmse: 0.0786 - val_loss: 0.0090 - val_mae: 0.0557 - val_mda: 0.5100 - val_rmse: 0.0589 - learning_rate: 0.0025\n",
            "Epoch 21/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5019e-04 - mae: 0.0125 - mda: 0.5015 - rmse: 0.0783\n",
            "Epoch 21: val_loss did not improve from 0.00895\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.4965e-04 - mae: 0.0125 - mda: 0.5016 - rmse: 0.0783 - val_loss: 0.0101 - val_mae: 0.0601 - val_mda: 0.5094 - val_rmse: 0.0630 - learning_rate: 0.0025\n",
            "Epoch 22/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.6751e-04 - mae: 0.0101 - mda: 0.5011 - rmse: 0.0772\n",
            "Epoch 22: val_loss improved from 0.00895 to 0.00872, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.6769e-04 - mae: 0.0101 - mda: 0.5011 - rmse: 0.0772 - val_loss: 0.0087 - val_mae: 0.0552 - val_mda: 0.5109 - val_rmse: 0.0584 - learning_rate: 0.0025\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.3802e-04 - mae: 0.0123 - mda: 0.5031 - rmse: 0.0773\n",
            "Epoch 23: val_loss improved from 0.00872 to 0.00840, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.3765e-04 - mae: 0.0123 - mda: 0.5031 - rmse: 0.0773 - val_loss: 0.0084 - val_mae: 0.0596 - val_mda: 0.5090 - val_rmse: 0.0621 - learning_rate: 0.0025\n",
            "Epoch 24/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2127e-04 - mae: 0.0117 - mda: 0.5018 - rmse: 0.0785\n",
            "Epoch 24: val_loss did not improve from 0.00840\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.2159e-04 - mae: 0.0117 - mda: 0.5018 - rmse: 0.0785 - val_loss: 0.0103 - val_mae: 0.0585 - val_mda: 0.5104 - val_rmse: 0.0619 - learning_rate: 0.0025\n",
            "Epoch 25/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.0018e-04 - mae: 0.0139 - mda: 0.5027 - rmse: 0.0787\n",
            "Epoch 25: val_loss improved from 0.00840 to 0.00778, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.9999e-04 - mae: 0.0139 - mda: 0.5027 - rmse: 0.0787 - val_loss: 0.0078 - val_mae: 0.0589 - val_mda: 0.5099 - val_rmse: 0.0615 - learning_rate: 0.0025\n",
            "Epoch 26/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5653e-04 - mae: 0.0127 - mda: 0.5014 - rmse: 0.0788\n",
            "Epoch 26: val_loss did not improve from 0.00778\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.5695e-04 - mae: 0.0127 - mda: 0.5014 - rmse: 0.0788 - val_loss: 0.0101 - val_mae: 0.0594 - val_mda: 0.5092 - val_rmse: 0.0624 - learning_rate: 0.0025\n",
            "Epoch 27/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.2436e-04 - mae: 0.0144 - mda: 0.5016 - rmse: 0.0785\n",
            "Epoch 27: val_loss did not improve from 0.00778\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.2497e-04 - mae: 0.0145 - mda: 0.5016 - rmse: 0.0785 - val_loss: 0.0093 - val_mae: 0.0540 - val_mda: 0.5103 - val_rmse: 0.0576 - learning_rate: 0.0025\n",
            "Epoch 28/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.8408e-04 - mae: 0.0107 - mda: 0.5020 - rmse: 0.0791\n",
            "Epoch 28: val_loss did not improve from 0.00778\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.8431e-04 - mae: 0.0107 - mda: 0.5020 - rmse: 0.0791 - val_loss: 0.0112 - val_mae: 0.0623 - val_mda: 0.5102 - val_rmse: 0.0654 - learning_rate: 0.0025\n",
            "Epoch 29/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.7919e-04 - mae: 0.0173 - mda: 0.5027 - rmse: 0.0804\n",
            "Epoch 29: val_loss did not improve from 0.00778\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 5.8161e-04 - mae: 0.0173 - mda: 0.5027 - rmse: 0.0804 - val_loss: 0.0126 - val_mae: 0.0677 - val_mda: 0.5106 - val_rmse: 0.0712 - learning_rate: 0.0025\n",
            "Epoch 30/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.2480e-04 - mae: 0.0120 - mda: 0.5028 - rmse: 0.0778\n",
            "Epoch 30: val_loss did not improve from 0.00778\n",
            "\n",
            "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.2445e-04 - mae: 0.0119 - mda: 0.5028 - rmse: 0.0778 - val_loss: 0.0100 - val_mae: 0.0596 - val_mda: 0.5088 - val_rmse: 0.0627 - learning_rate: 0.0025\n",
            "Epoch 31/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.3131e-04 - mae: 0.0090 - mda: 0.5023 - rmse: 0.0787\n",
            "Epoch 31: val_loss did not improve from 0.00778\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.3122e-04 - mae: 0.0090 - mda: 0.5023 - rmse: 0.0787 - val_loss: 0.0102 - val_mae: 0.0611 - val_mda: 0.5090 - val_rmse: 0.0639 - learning_rate: 0.0012\n",
            "Epoch 32/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.2431e-04 - mae: 0.0087 - mda: 0.5041 - rmse: 0.0788\n",
            "Epoch 32: val_loss did not improve from 0.00778\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.2432e-04 - mae: 0.0087 - mda: 0.5041 - rmse: 0.0788 - val_loss: 0.0094 - val_mae: 0.0569 - val_mda: 0.5112 - val_rmse: 0.0603 - learning_rate: 0.0012\n",
            "Epoch 33/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.1546e-04 - mae: 0.0085 - mda: 0.5030 - rmse: 0.0768\n",
            "Epoch 33: val_loss did not improve from 0.00778\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.1541e-04 - mae: 0.0085 - mda: 0.5030 - rmse: 0.0768 - val_loss: 0.0100 - val_mae: 0.0594 - val_mda: 0.5097 - val_rmse: 0.0625 - learning_rate: 0.0012\n",
            "Epoch 34/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.1644e-04 - mae: 0.0085 - mda: 0.5038 - rmse: 0.0781\n",
            "Epoch 34: val_loss did not improve from 0.00778\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.1661e-04 - mae: 0.0085 - mda: 0.5038 - rmse: 0.0781 - val_loss: 0.0100 - val_mae: 0.0584 - val_mda: 0.5109 - val_rmse: 0.0617 - learning_rate: 0.0012\n",
            "Epoch 35/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.3819e-04 - mae: 0.0093 - mda: 0.5026 - rmse: 0.0778\n",
            "Epoch 35: val_loss did not improve from 0.00778\n",
            "\n",
            "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.3826e-04 - mae: 0.0093 - mda: 0.5026 - rmse: 0.0779 - val_loss: 0.0093 - val_mae: 0.0585 - val_mda: 0.5108 - val_rmse: 0.0615 - learning_rate: 0.0012\n",
            "Epoch 35: early stopping\n",
            "Restoring model weights from the end of the best epoch: 25.\n",
            "Validation Loss: 0.00778, RMSE: 0.05765, MDA: 0.51202, MAE: 0.05405\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0285 - mae: 0.1065 - mda: 0.5008 - rmse: 0.1691\n",
            "Epoch 1: val_loss improved from inf to 0.00291, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0281 - mae: 0.1057 - mda: 0.5008 - rmse: 0.1682 - val_loss: 0.0029 - val_mae: 0.0350 - val_mda: 0.4972 - val_rmse: 0.0384 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m278/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0020 - mae: 0.0294 - mda: 0.5007 - rmse: 0.0879\n",
            "Epoch 2: val_loss improved from 0.00291 to 0.00097, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0020 - mae: 0.0293 - mda: 0.5007 - rmse: 0.0877 - val_loss: 9.7460e-04 - val_mae: 0.0185 - val_mda: 0.4969 - val_rmse: 0.0218 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m277/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.3585e-04 - mae: 0.0211 - mda: 0.5017 - rmse: 0.0813\n",
            "Epoch 3: val_loss did not improve from 0.00097\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 9.2966e-04 - mae: 0.0211 - mda: 0.5017 - rmse: 0.0813 - val_loss: 0.0012 - val_mae: 0.0224 - val_mda: 0.4969 - val_rmse: 0.0252 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4995e-04 - mae: 0.0171 - mda: 0.5016 - rmse: 0.0802\n",
            "Epoch 4: val_loss improved from 0.00097 to 0.00074, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.4889e-04 - mae: 0.0171 - mda: 0.5016 - rmse: 0.0802 - val_loss: 7.4237e-04 - val_mae: 0.0174 - val_mda: 0.4983 - val_rmse: 0.0205 - learning_rate: 0.0010\n",
            "Epoch 5/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1792e-04 - mae: 0.0142 - mda: 0.5015 - rmse: 0.0793\n",
            "Epoch 5: val_loss improved from 0.00074 to 0.00063, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.1745e-04 - mae: 0.0142 - mda: 0.5015 - rmse: 0.0793 - val_loss: 6.2915e-04 - val_mae: 0.0162 - val_mda: 0.4986 - val_rmse: 0.0193 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5172e-04 - mae: 0.0129 - mda: 0.5030 - rmse: 0.0788\n",
            "Epoch 6: val_loss improved from 0.00063 to 0.00047, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.5134e-04 - mae: 0.0129 - mda: 0.5030 - rmse: 0.0788 - val_loss: 4.7121e-04 - val_mae: 0.0139 - val_mda: 0.5000 - val_rmse: 0.0171 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1323e-04 - mae: 0.0118 - mda: 0.5023 - rmse: 0.0798\n",
            "Epoch 7: val_loss improved from 0.00047 to 0.00041, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.1239e-04 - mae: 0.0118 - mda: 0.5023 - rmse: 0.0798 - val_loss: 4.0986e-04 - val_mae: 0.0136 - val_mda: 0.5005 - val_rmse: 0.0166 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5413e-04 - mae: 0.0108 - mda: 0.5025 - rmse: 0.0789\n",
            "Epoch 8: val_loss did not improve from 0.00041\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.5323e-04 - mae: 0.0107 - mda: 0.5025 - rmse: 0.0788 - val_loss: 4.9954e-04 - val_mae: 0.0158 - val_mda: 0.4993 - val_rmse: 0.0186 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m272/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0832e-04 - mae: 0.0102 - mda: 0.5029 - rmse: 0.0785\n",
            "Epoch 9: val_loss improved from 0.00041 to 0.00038, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.0732e-04 - mae: 0.0102 - mda: 0.5029 - rmse: 0.0785 - val_loss: 3.7745e-04 - val_mae: 0.0133 - val_mda: 0.4997 - val_rmse: 0.0164 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9074e-04 - mae: 0.0095 - mda: 0.5029 - rmse: 0.0788\n",
            "Epoch 10: val_loss improved from 0.00038 to 0.00021, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.9054e-04 - mae: 0.0095 - mda: 0.5030 - rmse: 0.0788 - val_loss: 2.1450e-04 - val_mae: 0.0103 - val_mda: 0.4995 - val_rmse: 0.0138 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6721e-04 - mae: 0.0092 - mda: 0.5019 - rmse: 0.0797\n",
            "Epoch 11: val_loss did not improve from 0.00021\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.6700e-04 - mae: 0.0092 - mda: 0.5019 - rmse: 0.0797 - val_loss: 3.3211e-04 - val_mae: 0.0128 - val_mda: 0.5002 - val_rmse: 0.0159 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5310e-04 - mae: 0.0087 - mda: 0.5027 - rmse: 0.0780\n",
            "Epoch 12: val_loss did not improve from 0.00021\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.5290e-04 - mae: 0.0087 - mda: 0.5027 - rmse: 0.0780 - val_loss: 2.6200e-04 - val_mae: 0.0115 - val_mda: 0.5010 - val_rmse: 0.0148 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4477e-04 - mae: 0.0083 - mda: 0.5021 - rmse: 0.0783\n",
            "Epoch 13: val_loss did not improve from 0.00021\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.4461e-04 - mae: 0.0083 - mda: 0.5021 - rmse: 0.0783 - val_loss: 3.7769e-04 - val_mae: 0.0143 - val_mda: 0.5025 - val_rmse: 0.0172 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3318e-04 - mae: 0.0083 - mda: 0.5030 - rmse: 0.0781\n",
            "Epoch 14: val_loss did not improve from 0.00021\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.3312e-04 - mae: 0.0083 - mda: 0.5030 - rmse: 0.0781 - val_loss: 2.3155e-04 - val_mae: 0.0102 - val_mda: 0.5009 - val_rmse: 0.0137 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2454e-04 - mae: 0.0079 - mda: 0.5024 - rmse: 0.0782\n",
            "Epoch 15: val_loss improved from 0.00021 to 0.00011, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.2442e-04 - mae: 0.0079 - mda: 0.5024 - rmse: 0.0782 - val_loss: 1.0951e-04 - val_mae: 0.0068 - val_mda: 0.5047 - val_rmse: 0.0109 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1042e-04 - mae: 0.0074 - mda: 0.5018 - rmse: 0.0782\n",
            "Epoch 16: val_loss did not improve from 0.00011\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1032e-04 - mae: 0.0074 - mda: 0.5018 - rmse: 0.0782 - val_loss: 1.5255e-04 - val_mae: 0.0079 - val_mda: 0.5052 - val_rmse: 0.0118 - learning_rate: 0.0010\n",
            "Epoch 17/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0078e-04 - mae: 0.0072 - mda: 0.5039 - rmse: 0.0780\n",
            "Epoch 17: val_loss did not improve from 0.00011\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0083e-04 - mae: 0.0072 - mda: 0.5039 - rmse: 0.0781 - val_loss: 2.7821e-04 - val_mae: 0.0123 - val_mda: 0.5065 - val_rmse: 0.0155 - learning_rate: 0.0010\n",
            "Epoch 18/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.9702e-05 - mae: 0.0072 - mda: 0.5027 - rmse: 0.0780\n",
            "Epoch 18: val_loss did not improve from 0.00011\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 9.9674e-05 - mae: 0.0072 - mda: 0.5027 - rmse: 0.0780 - val_loss: 1.3109e-04 - val_mae: 0.0078 - val_mda: 0.5050 - val_rmse: 0.0117 - learning_rate: 0.0010\n",
            "Epoch 19/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.8939e-05 - mae: 0.0068 - mda: 0.5034 - rmse: 0.0782\n",
            "Epoch 19: val_loss did not improve from 0.00011\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.8955e-05 - mae: 0.0068 - mda: 0.5034 - rmse: 0.0782 - val_loss: 4.2351e-04 - val_mae: 0.0154 - val_mda: 0.5049 - val_rmse: 0.0183 - learning_rate: 0.0010\n",
            "Epoch 20/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.7163e-05 - mae: 0.0067 - mda: 0.5025 - rmse: 0.0786\n",
            "Epoch 20: val_loss improved from 0.00011 to 0.00009, saving model to best_model.keras\n",
            "\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.7238e-05 - mae: 0.0067 - mda: 0.5025 - rmse: 0.0786 - val_loss: 8.7825e-05 - val_mae: 0.0060 - val_mda: 0.5080 - val_rmse: 0.0105 - learning_rate: 0.0010\n",
            "Epoch 21/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.8252e-05 - mae: 0.0067 - mda: 0.5033 - rmse: 0.0786\n",
            "Epoch 21: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.8067e-05 - mae: 0.0067 - mda: 0.5033 - rmse: 0.0786 - val_loss: 1.9545e-04 - val_mae: 0.0088 - val_mda: 0.5075 - val_rmse: 0.0127 - learning_rate: 5.0000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m281/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.8182e-05 - mae: 0.0064 - mda: 0.5027 - rmse: 0.0788\n",
            "Epoch 22: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7.8248e-05 - mae: 0.0064 - mda: 0.5027 - rmse: 0.0787 - val_loss: 1.8474e-04 - val_mae: 0.0083 - val_mda: 0.5074 - val_rmse: 0.0123 - learning_rate: 5.0000e-04\n",
            "Epoch 23/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7656e-05 - mae: 0.0063 - mda: 0.5027 - rmse: 0.0788\n",
            "Epoch 23: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7.7608e-05 - mae: 0.0063 - mda: 0.5027 - rmse: 0.0788 - val_loss: 2.7240e-04 - val_mae: 0.0107 - val_mda: 0.5079 - val_rmse: 0.0144 - learning_rate: 5.0000e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.2379e-05 - mae: 0.0061 - mda: 0.5036 - rmse: 0.0774\n",
            "Epoch 24: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7.2394e-05 - mae: 0.0061 - mda: 0.5036 - rmse: 0.0774 - val_loss: 2.4275e-04 - val_mae: 0.0097 - val_mda: 0.5075 - val_rmse: 0.0135 - learning_rate: 5.0000e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3911e-05 - mae: 0.0062 - mda: 0.5025 - rmse: 0.0784\n",
            "Epoch 25: val_loss did not improve from 0.00009\n",
            "\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7.3922e-05 - mae: 0.0062 - mda: 0.5025 - rmse: 0.0784 - val_loss: 1.3795e-04 - val_mae: 0.0070 - val_mda: 0.5087 - val_rmse: 0.0114 - learning_rate: 5.0000e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9888e-05 - mae: 0.0059 - mda: 0.5020 - rmse: 0.0780\n",
            "Epoch 26: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.9870e-05 - mae: 0.0059 - mda: 0.5020 - rmse: 0.0780 - val_loss: 2.9616e-04 - val_mae: 0.0111 - val_mda: 0.5091 - val_rmse: 0.0147 - learning_rate: 2.5000e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7194e-05 - mae: 0.0059 - mda: 0.5035 - rmse: 0.0785\n",
            "Epoch 27: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.7242e-05 - mae: 0.0059 - mda: 0.5035 - rmse: 0.0785 - val_loss: 2.2838e-04 - val_mae: 0.0090 - val_mda: 0.5089 - val_rmse: 0.0130 - learning_rate: 2.5000e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m280/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.7012e-05 - mae: 0.0059 - mda: 0.5031 - rmse: 0.0782\n",
            "Epoch 28: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.7019e-05 - mae: 0.0059 - mda: 0.5031 - rmse: 0.0782 - val_loss: 2.6380e-04 - val_mae: 0.0100 - val_mda: 0.5101 - val_rmse: 0.0138 - learning_rate: 2.5000e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.8829e-05 - mae: 0.0059 - mda: 0.5036 - rmse: 0.0785\n",
            "Epoch 29: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.8735e-05 - mae: 0.0059 - mda: 0.5036 - rmse: 0.0785 - val_loss: 3.9786e-04 - val_mae: 0.0131 - val_mda: 0.5098 - val_rmse: 0.0164 - learning_rate: 2.5000e-04\n",
            "Epoch 30/50\n",
            "\u001b[1m271/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4596e-05 - mae: 0.0057 - mda: 0.5032 - rmse: 0.0785\n",
            "Epoch 30: val_loss did not improve from 0.00009\n",
            "\n",
            "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.4778e-05 - mae: 0.0057 - mda: 0.5033 - rmse: 0.0785 - val_loss: 4.5321e-04 - val_mae: 0.0141 - val_mda: 0.5087 - val_rmse: 0.0174 - learning_rate: 2.5000e-04\n",
            "Epoch 30: early stopping\n",
            "Restoring model weights from the end of the best epoch: 20.\n",
            "Validation Loss: 0.00009, RMSE: 0.01052, MDA: 0.51008, MAE: 0.00603\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m280/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0219 - mae: 0.0906 - mda: 0.4997 - rmse: 0.1516\n",
            "Epoch 1: val_loss improved from inf to 0.00182, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0213 - mae: 0.0890 - mda: 0.4997 - rmse: 0.1499 - val_loss: 0.0018 - val_mae: 0.0241 - val_mda: 0.4958 - val_rmse: 0.0278 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0014 - mae: 0.0251 - mda: 0.5015 - rmse: 0.0863\n",
            "Epoch 2: val_loss improved from 0.00182 to 0.00093, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0014 - mae: 0.0250 - mda: 0.5015 - rmse: 0.0862 - val_loss: 9.3177e-04 - val_mae: 0.0215 - val_mda: 0.4977 - val_rmse: 0.0246 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9048e-04 - mae: 0.0176 - mda: 0.5020 - rmse: 0.0825\n",
            "Epoch 3: val_loss improved from 0.00093 to 0.00056, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6.8822e-04 - mae: 0.0176 - mda: 0.5020 - rmse: 0.0825 - val_loss: 5.6079e-04 - val_mae: 0.0150 - val_mda: 0.4992 - val_rmse: 0.0182 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5302e-04 - mae: 0.0139 - mda: 0.5026 - rmse: 0.0804\n",
            "Epoch 4: val_loss improved from 0.00056 to 0.00047, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.5162e-04 - mae: 0.0139 - mda: 0.5026 - rmse: 0.0804 - val_loss: 4.7313e-04 - val_mae: 0.0142 - val_mda: 0.4991 - val_rmse: 0.0172 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9111e-04 - mae: 0.0115 - mda: 0.5026 - rmse: 0.0785\n",
            "Epoch 5: val_loss improved from 0.00047 to 0.00017, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.9113e-04 - mae: 0.0115 - mda: 0.5026 - rmse: 0.0785 - val_loss: 1.6821e-04 - val_mae: 0.0088 - val_mda: 0.5015 - val_rmse: 0.0123 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4444e-04 - mae: 0.0103 - mda: 0.5035 - rmse: 0.0794\n",
            "Epoch 6: val_loss did not improve from 0.00017\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.4400e-04 - mae: 0.0103 - mda: 0.5035 - rmse: 0.0794 - val_loss: 1.9597e-04 - val_mae: 0.0092 - val_mda: 0.5005 - val_rmse: 0.0127 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8970e-04 - mae: 0.0093 - mda: 0.5026 - rmse: 0.0788\n",
            "Epoch 7: val_loss did not improve from 0.00017\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.8974e-04 - mae: 0.0093 - mda: 0.5026 - rmse: 0.0788 - val_loss: 3.1075e-04 - val_mae: 0.0124 - val_mda: 0.5011 - val_rmse: 0.0155 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5542e-04 - mae: 0.0084 - mda: 0.5022 - rmse: 0.0786\n",
            "Epoch 8: val_loss improved from 0.00017 to 0.00011, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.5526e-04 - mae: 0.0084 - mda: 0.5022 - rmse: 0.0786 - val_loss: 1.0773e-04 - val_mae: 0.0070 - val_mda: 0.5018 - val_rmse: 0.0110 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4655e-04 - mae: 0.0083 - mda: 0.5027 - rmse: 0.0792\n",
            "Epoch 9: val_loss did not improve from 0.00011\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.4627e-04 - mae: 0.0083 - mda: 0.5027 - rmse: 0.0792 - val_loss: 1.2836e-04 - val_mae: 0.0075 - val_mda: 0.5015 - val_rmse: 0.0116 - learning_rate: 0.0010\n",
            "Epoch 10/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2400e-04 - mae: 0.0076 - mda: 0.5023 - rmse: 0.0783\n",
            "Epoch 10: val_loss did not improve from 0.00011\n",
            "\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2388e-04 - mae: 0.0076 - mda: 0.5023 - rmse: 0.0783 - val_loss: 1.5211e-04 - val_mae: 0.0081 - val_mda: 0.5026 - val_rmse: 0.0119 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1163e-04 - mae: 0.0072 - mda: 0.5045 - rmse: 0.0777\n",
            "Epoch 11: val_loss improved from 0.00011 to 0.00009, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.1154e-04 - mae: 0.0072 - mda: 0.5045 - rmse: 0.0777 - val_loss: 9.3088e-05 - val_mae: 0.0064 - val_mda: 0.5039 - val_rmse: 0.0106 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.8388e-05 - mae: 0.0069 - mda: 0.5036 - rmse: 0.0772\n",
            "Epoch 12: val_loss improved from 0.00009 to 0.00008, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 9.8424e-05 - mae: 0.0069 - mda: 0.5035 - rmse: 0.0772 - val_loss: 8.2799e-05 - val_mae: 0.0061 - val_mda: 0.5027 - val_rmse: 0.0103 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1118e-05 - mae: 0.0067 - mda: 0.5034 - rmse: 0.0787\n",
            "Epoch 13: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 9.1178e-05 - mae: 0.0067 - mda: 0.5034 - rmse: 0.0787 - val_loss: 9.5765e-05 - val_mae: 0.0066 - val_mda: 0.5032 - val_rmse: 0.0108 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.5298e-05 - mae: 0.0065 - mda: 0.5024 - rmse: 0.0783\n",
            "Epoch 14: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 8.5238e-05 - mae: 0.0065 - mda: 0.5024 - rmse: 0.0783 - val_loss: 1.4447e-04 - val_mae: 0.0081 - val_mda: 0.5044 - val_rmse: 0.0120 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m279/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6973e-05 - mae: 0.0061 - mda: 0.5041 - rmse: 0.0784\n",
            "Epoch 15: val_loss did not improve from 0.00008\n",
            "\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7.7126e-05 - mae: 0.0061 - mda: 0.5041 - rmse: 0.0784 - val_loss: 1.6534e-04 - val_mae: 0.0085 - val_mda: 0.5036 - val_rmse: 0.0123 - learning_rate: 5.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2178e-05 - mae: 0.0060 - mda: 0.5021 - rmse: 0.0778\n",
            "Epoch 16: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7.2233e-05 - mae: 0.0060 - mda: 0.5021 - rmse: 0.0779 - val_loss: 1.2371e-04 - val_mae: 0.0074 - val_mda: 0.5048 - val_rmse: 0.0114 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1502e-05 - mae: 0.0059 - mda: 0.5049 - rmse: 0.0783\n",
            "Epoch 17: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7.1555e-05 - mae: 0.0059 - mda: 0.5048 - rmse: 0.0783 - val_loss: 1.1193e-04 - val_mae: 0.0070 - val_mda: 0.5061 - val_rmse: 0.0111 - learning_rate: 2.5000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9768e-05 - mae: 0.0058 - mda: 0.5037 - rmse: 0.0794\n",
            "Epoch 18: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6.9756e-05 - mae: 0.0058 - mda: 0.5037 - rmse: 0.0794 - val_loss: 8.5515e-05 - val_mae: 0.0060 - val_mda: 0.5054 - val_rmse: 0.0103 - learning_rate: 2.5000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9908e-05 - mae: 0.0059 - mda: 0.5031 - rmse: 0.0787\n",
            "Epoch 19: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6.9919e-05 - mae: 0.0059 - mda: 0.5031 - rmse: 0.0787 - val_loss: 1.9306e-04 - val_mae: 0.0100 - val_mda: 0.5054 - val_rmse: 0.0134 - learning_rate: 2.5000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5592e-05 - mae: 0.0057 - mda: 0.5027 - rmse: 0.0784\n",
            "Epoch 20: val_loss improved from 0.00008 to 0.00007, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6.5644e-05 - mae: 0.0057 - mda: 0.5027 - rmse: 0.0784 - val_loss: 6.7682e-05 - val_mae: 0.0054 - val_mda: 0.5058 - val_rmse: 0.0099 - learning_rate: 2.5000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0000e-05 - mae: 0.0055 - mda: 0.5042 - rmse: 0.0790\n",
            "Epoch 21: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6.0023e-05 - mae: 0.0055 - mda: 0.5042 - rmse: 0.0789 - val_loss: 1.4118e-04 - val_mae: 0.0077 - val_mda: 0.5040 - val_rmse: 0.0118 - learning_rate: 2.5000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3047e-05 - mae: 0.0056 - mda: 0.5029 - rmse: 0.0788\n",
            "Epoch 22: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6.3111e-05 - mae: 0.0056 - mda: 0.5029 - rmse: 0.0787 - val_loss: 7.3947e-05 - val_mae: 0.0055 - val_mda: 0.5068 - val_rmse: 0.0101 - learning_rate: 2.5000e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.6754e-05 - mae: 0.0054 - mda: 0.5043 - rmse: 0.0785\n",
            "Epoch 23: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5.6794e-05 - mae: 0.0054 - mda: 0.5043 - rmse: 0.0785 - val_loss: 7.6372e-05 - val_mae: 0.0056 - val_mda: 0.5083 - val_rmse: 0.0102 - learning_rate: 2.5000e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0070e-05 - mae: 0.0054 - mda: 0.5026 - rmse: 0.0787\n",
            "Epoch 24: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6.0066e-05 - mae: 0.0054 - mda: 0.5026 - rmse: 0.0787 - val_loss: 1.1606e-04 - val_mae: 0.0069 - val_mda: 0.5089 - val_rmse: 0.0111 - learning_rate: 2.5000e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2401e-05 - mae: 0.0052 - mda: 0.5030 - rmse: 0.0784\n",
            "Epoch 25: val_loss did not improve from 0.00007\n",
            "\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5.2362e-05 - mae: 0.0052 - mda: 0.5031 - rmse: 0.0784 - val_loss: 8.2470e-05 - val_mae: 0.0057 - val_mda: 0.5068 - val_rmse: 0.0102 - learning_rate: 2.5000e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2877e-05 - mae: 0.0052 - mda: 0.5038 - rmse: 0.0794\n",
            "Epoch 26: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5.2841e-05 - mae: 0.0052 - mda: 0.5038 - rmse: 0.0794 - val_loss: 8.7658e-05 - val_mae: 0.0058 - val_mda: 0.5075 - val_rmse: 0.0104 - learning_rate: 1.2500e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9211e-05 - mae: 0.0050 - mda: 0.5035 - rmse: 0.0783\n",
            "Epoch 27: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.9210e-05 - mae: 0.0050 - mda: 0.5035 - rmse: 0.0783 - val_loss: 1.0294e-04 - val_mae: 0.0063 - val_mda: 0.5085 - val_rmse: 0.0107 - learning_rate: 1.2500e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.0831e-05 - mae: 0.0050 - mda: 0.5027 - rmse: 0.0788\n",
            "Epoch 28: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5.0788e-05 - mae: 0.0050 - mda: 0.5028 - rmse: 0.0788 - val_loss: 1.4283e-04 - val_mae: 0.0075 - val_mda: 0.5093 - val_rmse: 0.0116 - learning_rate: 1.2500e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6733e-05 - mae: 0.0048 - mda: 0.5044 - rmse: 0.0782\n",
            "Epoch 29: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.6729e-05 - mae: 0.0048 - mda: 0.5044 - rmse: 0.0782 - val_loss: 7.0999e-05 - val_mae: 0.0053 - val_mda: 0.5087 - val_rmse: 0.0100 - learning_rate: 1.2500e-04\n",
            "Epoch 30/50\n",
            "\u001b[1m279/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6488e-05 - mae: 0.0048 - mda: 0.5026 - rmse: 0.0787\n",
            "Epoch 30: val_loss did not improve from 0.00007\n",
            "\n",
            "Epoch 30: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.6600e-05 - mae: 0.0048 - mda: 0.5027 - rmse: 0.0787 - val_loss: 1.1858e-04 - val_mae: 0.0068 - val_mda: 0.5092 - val_rmse: 0.0111 - learning_rate: 1.2500e-04\n",
            "Epoch 30: early stopping\n",
            "Restoring model weights from the end of the best epoch: 20.\n",
            "Validation Loss: 0.00007, RMSE: 0.00993, MDA: 0.50926, MAE: 0.00527\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0397 - mae: 0.1078 - mda: 0.5005 - rmse: 0.1707\n",
            "Epoch 1: val_loss improved from inf to 0.00127, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0395 - mae: 0.1075 - mda: 0.5005 - rmse: 0.1703 - val_loss: 0.0013 - val_mae: 0.0231 - val_mda: 0.4966 - val_rmse: 0.0263 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0016 - mae: 0.0261 - mda: 0.5015 - rmse: 0.0862\n",
            "Epoch 2: val_loss improved from 0.00127 to 0.00058, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0016 - mae: 0.0261 - mda: 0.5015 - rmse: 0.0862 - val_loss: 5.7926e-04 - val_mae: 0.0169 - val_mda: 0.4975 - val_rmse: 0.0198 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.7849e-04 - mae: 0.0180 - mda: 0.5012 - rmse: 0.0821\n",
            "Epoch 3: val_loss improved from 0.00058 to 0.00025, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 7.7706e-04 - mae: 0.0180 - mda: 0.5012 - rmse: 0.0821 - val_loss: 2.4865e-04 - val_mae: 0.0109 - val_mda: 0.4980 - val_rmse: 0.0142 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.4800e-04 - mae: 0.0144 - mda: 0.5028 - rmse: 0.0806\n",
            "Epoch 4: val_loss improved from 0.00025 to 0.00014, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 4.4725e-04 - mae: 0.0144 - mda: 0.5028 - rmse: 0.0806 - val_loss: 1.4459e-04 - val_mae: 0.0090 - val_mda: 0.5002 - val_rmse: 0.0126 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.3773e-04 - mae: 0.0122 - mda: 0.5016 - rmse: 0.0794\n",
            "Epoch 5: val_loss did not improve from 0.00014\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.3757e-04 - mae: 0.0122 - mda: 0.5016 - rmse: 0.0794 - val_loss: 1.8345e-04 - val_mae: 0.0101 - val_mda: 0.5004 - val_rmse: 0.0136 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7698e-04 - mae: 0.0111 - mda: 0.5027 - rmse: 0.0793\n",
            "Epoch 6: val_loss improved from 0.00014 to 0.00013, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.7667e-04 - mae: 0.0111 - mda: 0.5027 - rmse: 0.0793 - val_loss: 1.3311e-04 - val_mae: 0.0089 - val_mda: 0.5004 - val_rmse: 0.0125 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.1774e-04 - mae: 0.0099 - mda: 0.5021 - rmse: 0.0792\n",
            "Epoch 7: val_loss improved from 0.00013 to 0.00012, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.1748e-04 - mae: 0.0099 - mda: 0.5021 - rmse: 0.0792 - val_loss: 1.2340e-04 - val_mae: 0.0088 - val_mda: 0.5021 - val_rmse: 0.0123 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.7089e-04 - mae: 0.0089 - mda: 0.5024 - rmse: 0.0789\n",
            "Epoch 8: val_loss improved from 0.00012 to 0.00011, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.7078e-04 - mae: 0.0089 - mda: 0.5024 - rmse: 0.0789 - val_loss: 1.0644e-04 - val_mae: 0.0082 - val_mda: 0.5011 - val_rmse: 0.0120 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.3595e-04 - mae: 0.0081 - mda: 0.5027 - rmse: 0.0784\n",
            "Epoch 9: val_loss improved from 0.00011 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.3595e-04 - mae: 0.0081 - mda: 0.5027 - rmse: 0.0784 - val_loss: 3.3666e-05 - val_mae: 0.0046 - val_mda: 0.5033 - val_rmse: 0.0093 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.2612e-04 - mae: 0.0078 - mda: 0.5034 - rmse: 0.0790\n",
            "Epoch 10: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.2605e-04 - mae: 0.0078 - mda: 0.5034 - rmse: 0.0790 - val_loss: 5.4489e-05 - val_mae: 0.0056 - val_mda: 0.5025 - val_rmse: 0.0100 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.1200e-04 - mae: 0.0075 - mda: 0.5021 - rmse: 0.0790\n",
            "Epoch 11: val_loss improved from 0.00003 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.1192e-04 - mae: 0.0075 - mda: 0.5021 - rmse: 0.0790 - val_loss: 2.7925e-05 - val_mae: 0.0039 - val_mda: 0.5029 - val_rmse: 0.0088 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.9844e-05 - mae: 0.0070 - mda: 0.5029 - rmse: 0.0789\n",
            "Epoch 12: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 9.9796e-05 - mae: 0.0070 - mda: 0.5029 - rmse: 0.0789 - val_loss: 5.2136e-05 - val_mae: 0.0054 - val_mda: 0.5030 - val_rmse: 0.0099 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.6422e-05 - mae: 0.0066 - mda: 0.5033 - rmse: 0.0776\n",
            "Epoch 13: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 8.6427e-05 - mae: 0.0066 - mda: 0.5033 - rmse: 0.0776 - val_loss: 2.8054e-05 - val_mae: 0.0039 - val_mda: 0.5023 - val_rmse: 0.0089 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.2692e-05 - mae: 0.0064 - mda: 0.5023 - rmse: 0.0779\n",
            "Epoch 14: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 8.2685e-05 - mae: 0.0064 - mda: 0.5023 - rmse: 0.0779 - val_loss: 3.7825e-05 - val_mae: 0.0047 - val_mda: 0.5022 - val_rmse: 0.0095 - learning_rate: 0.0010\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.8652e-05 - mae: 0.0060 - mda: 0.5033 - rmse: 0.0788\n",
            "Epoch 15: val_loss improved from 0.00003 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 6.8618e-05 - mae: 0.0060 - mda: 0.5033 - rmse: 0.0788 - val_loss: 1.9010e-05 - val_mae: 0.0033 - val_mda: 0.5053 - val_rmse: 0.0086 - learning_rate: 5.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.0408e-05 - mae: 0.0056 - mda: 0.5027 - rmse: 0.0789\n",
            "Epoch 16: val_loss improved from 0.00002 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 6.0438e-05 - mae: 0.0056 - mda: 0.5027 - rmse: 0.0789 - val_loss: 1.8595e-05 - val_mae: 0.0033 - val_mda: 0.5055 - val_rmse: 0.0086 - learning_rate: 5.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.4178e-05 - mae: 0.0057 - mda: 0.5034 - rmse: 0.0791\n",
            "Epoch 17: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 6.4142e-05 - mae: 0.0057 - mda: 0.5034 - rmse: 0.0791 - val_loss: 3.6613e-05 - val_mae: 0.0045 - val_mda: 0.5046 - val_rmse: 0.0094 - learning_rate: 5.0000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.7734e-05 - mae: 0.0054 - mda: 0.5033 - rmse: 0.0783\n",
            "Epoch 18: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 5.7740e-05 - mae: 0.0054 - mda: 0.5033 - rmse: 0.0783 - val_loss: 3.2014e-05 - val_mae: 0.0042 - val_mda: 0.5042 - val_rmse: 0.0091 - learning_rate: 5.0000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.3355e-05 - mae: 0.0052 - mda: 0.5022 - rmse: 0.0786\n",
            "Epoch 19: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 5.3403e-05 - mae: 0.0052 - mda: 0.5022 - rmse: 0.0786 - val_loss: 5.7451e-05 - val_mae: 0.0055 - val_mda: 0.5040 - val_rmse: 0.0100 - learning_rate: 5.0000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.1951e-05 - mae: 0.0052 - mda: 0.5035 - rmse: 0.0783\n",
            "Epoch 20: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 5.1953e-05 - mae: 0.0052 - mda: 0.5035 - rmse: 0.0783 - val_loss: 2.0479e-05 - val_mae: 0.0034 - val_mda: 0.5048 - val_rmse: 0.0087 - learning_rate: 2.5000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.1825e-05 - mae: 0.0051 - mda: 0.5029 - rmse: 0.0786\n",
            "Epoch 21: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 5.1775e-05 - mae: 0.0051 - mda: 0.5029 - rmse: 0.0786 - val_loss: 2.2170e-05 - val_mae: 0.0036 - val_mda: 0.5057 - val_rmse: 0.0088 - learning_rate: 2.5000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.1512e-05 - mae: 0.0050 - mda: 0.5034 - rmse: 0.0797\n",
            "Epoch 22: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 5.1481e-05 - mae: 0.0050 - mda: 0.5034 - rmse: 0.0797 - val_loss: 4.0851e-05 - val_mae: 0.0049 - val_mda: 0.5053 - val_rmse: 0.0096 - learning_rate: 2.5000e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.2656e-05 - mae: 0.0051 - mda: 0.5039 - rmse: 0.0783\n",
            "Epoch 23: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 5.2624e-05 - mae: 0.0051 - mda: 0.5039 - rmse: 0.0783 - val_loss: 2.0366e-05 - val_mae: 0.0033 - val_mda: 0.5028 - val_rmse: 0.0086 - learning_rate: 2.5000e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.7534e-05 - mae: 0.0049 - mda: 0.5035 - rmse: 0.0793\n",
            "Epoch 24: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 4.7516e-05 - mae: 0.0049 - mda: 0.5035 - rmse: 0.0792 - val_loss: 6.4574e-05 - val_mae: 0.0062 - val_mda: 0.5045 - val_rmse: 0.0105 - learning_rate: 2.5000e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.5429e-05 - mae: 0.0048 - mda: 0.5030 - rmse: 0.0785\n",
            "Epoch 25: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 4.5406e-05 - mae: 0.0048 - mda: 0.5030 - rmse: 0.0785 - val_loss: 2.5132e-05 - val_mae: 0.0037 - val_mda: 0.5043 - val_rmse: 0.0089 - learning_rate: 1.2500e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.2818e-05 - mae: 0.0047 - mda: 0.5036 - rmse: 0.0788\n",
            "Epoch 26: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 4.2802e-05 - mae: 0.0047 - mda: 0.5036 - rmse: 0.0788 - val_loss: 5.3751e-05 - val_mae: 0.0054 - val_mda: 0.5033 - val_rmse: 0.0100 - learning_rate: 1.2500e-04\n",
            "Epoch 26: early stopping\n",
            "Restoring model weights from the end of the best epoch: 16.\n",
            "Validation Loss: 0.00002, RMSE: 0.00861, MDA: 0.50569, MAE: 0.00326\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0332 - mae: 0.0838 - mda: 0.5007 - rmse: 0.1429\n",
            "Epoch 1: val_loss improved from inf to 0.00122, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0329 - mae: 0.0832 - mda: 0.5007 - rmse: 0.1423 - val_loss: 0.0012 - val_mae: 0.0260 - val_mda: 0.4978 - val_rmse: 0.0286 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m280/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8121e-04 - mae: 0.0139 - mda: 0.5028 - rmse: 0.0789\n",
            "Epoch 2: val_loss improved from 0.00122 to 0.00049, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.7943e-04 - mae: 0.0138 - mda: 0.5028 - rmse: 0.0789 - val_loss: 4.9116e-04 - val_mae: 0.0149 - val_mda: 0.4980 - val_rmse: 0.0179 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2978e-04 - mae: 0.0108 - mda: 0.5020 - rmse: 0.0781\n",
            "Epoch 3: val_loss improved from 0.00049 to 0.00026, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.2926e-04 - mae: 0.0108 - mda: 0.5021 - rmse: 0.0781 - val_loss: 2.5870e-04 - val_mae: 0.0103 - val_mda: 0.4986 - val_rmse: 0.0138 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m279/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8391e-04 - mae: 0.0097 - mda: 0.5023 - rmse: 0.0790\n",
            "Epoch 4: val_loss did not improve from 0.00026\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.8332e-04 - mae: 0.0097 - mda: 0.5023 - rmse: 0.0790 - val_loss: 3.7887e-04 - val_mae: 0.0128 - val_mda: 0.4983 - val_rmse: 0.0161 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5206e-04 - mae: 0.0089 - mda: 0.5015 - rmse: 0.0783\n",
            "Epoch 5: val_loss improved from 0.00026 to 0.00015, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.5180e-04 - mae: 0.0088 - mda: 0.5015 - rmse: 0.0783 - val_loss: 1.5297e-04 - val_mae: 0.0079 - val_mda: 0.4988 - val_rmse: 0.0118 - learning_rate: 0.0100\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/50\n",
            "\u001b[1m281/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3216e-04 - mae: 0.0083 - mda: 0.5020 - rmse: 0.0784\n",
            "Epoch 6: val_loss did not improve from 0.00015\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.3199e-04 - mae: 0.0083 - mda: 0.5021 - rmse: 0.0784 - val_loss: 2.5063e-04 - val_mae: 0.0110 - val_mda: 0.5009 - val_rmse: 0.0143 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2178e-04 - mae: 0.0082 - mda: 0.5037 - rmse: 0.0780\n",
            "Epoch 7: val_loss did not improve from 0.00015\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.2183e-04 - mae: 0.0082 - mda: 0.5037 - rmse: 0.0780 - val_loss: 3.6604e-04 - val_mae: 0.0126 - val_mda: 0.5028 - val_rmse: 0.0159 - learning_rate: 0.0100\n",
            "Epoch 8/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0652e-04 - mae: 0.0076 - mda: 0.5016 - rmse: 0.0781\n",
            "Epoch 8: val_loss did not improve from 0.00015\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0657e-04 - mae: 0.0076 - mda: 0.5017 - rmse: 0.0781 - val_loss: 2.4798e-04 - val_mae: 0.0100 - val_mda: 0.5009 - val_rmse: 0.0136 - learning_rate: 0.0100\n",
            "Epoch 9/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0300e-04 - mae: 0.0074 - mda: 0.5025 - rmse: 0.0787\n",
            "Epoch 9: val_loss did not improve from 0.00015\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0295e-04 - mae: 0.0074 - mda: 0.5025 - rmse: 0.0787 - val_loss: 2.4982e-04 - val_mae: 0.0101 - val_mda: 0.5047 - val_rmse: 0.0138 - learning_rate: 0.0100\n",
            "Epoch 10/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0273e-04 - mae: 0.0075 - mda: 0.5028 - rmse: 0.0789\n",
            "Epoch 10: val_loss did not improve from 0.00015\n",
            "\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.0258e-04 - mae: 0.0075 - mda: 0.5028 - rmse: 0.0788 - val_loss: 5.6038e-04 - val_mae: 0.0159 - val_mda: 0.5048 - val_rmse: 0.0189 - learning_rate: 0.0100\n",
            "Epoch 11/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.9633e-05 - mae: 0.0064 - mda: 0.5028 - rmse: 0.0780\n",
            "Epoch 11: val_loss did not improve from 0.00015\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7.9661e-05 - mae: 0.0064 - mda: 0.5028 - rmse: 0.0780 - val_loss: 5.8836e-04 - val_mae: 0.0155 - val_mda: 0.5057 - val_rmse: 0.0187 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m273/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2499e-05 - mae: 0.0066 - mda: 0.5036 - rmse: 0.0783\n",
            "Epoch 12: val_loss did not improve from 0.00015\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.2338e-05 - mae: 0.0066 - mda: 0.5036 - rmse: 0.0783 - val_loss: 1.8297e-04 - val_mae: 0.0111 - val_mda: 0.5043 - val_rmse: 0.0143 - learning_rate: 0.0050\n",
            "Epoch 13/50\n",
            "\u001b[1m281/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.7283e-05 - mae: 0.0067 - mda: 0.5031 - rmse: 0.0783\n",
            "Epoch 13: val_loss did not improve from 0.00015\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.7181e-05 - mae: 0.0067 - mda: 0.5032 - rmse: 0.0783 - val_loss: 2.9311e-04 - val_mae: 0.0098 - val_mda: 0.5074 - val_rmse: 0.0138 - learning_rate: 0.0050\n",
            "Epoch 14/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6918e-05 - mae: 0.0068 - mda: 0.5022 - rmse: 0.0792\n",
            "Epoch 14: val_loss did not improve from 0.00015\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.6814e-05 - mae: 0.0068 - mda: 0.5022 - rmse: 0.0791 - val_loss: 5.3001e-04 - val_mae: 0.0146 - val_mda: 0.5069 - val_rmse: 0.0179 - learning_rate: 0.0050\n",
            "Epoch 15/50\n",
            "\u001b[1m282/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1606e-05 - mae: 0.0067 - mda: 0.5028 - rmse: 0.0781\n",
            "Epoch 15: val_loss did not improve from 0.00015\n",
            "\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.1651e-05 - mae: 0.0067 - mda: 0.5028 - rmse: 0.0782 - val_loss: 4.4939e-04 - val_mae: 0.0130 - val_mda: 0.5069 - val_rmse: 0.0165 - learning_rate: 0.0050\n",
            "Epoch 15: early stopping\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "Validation Loss: 0.00015, RMSE: 0.01177, MDA: 0.50735, MAE: 0.00785\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m278/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4308 - mae: 0.4388 - mda: 0.5000 - rmse: 0.5301\n",
            "Epoch 1: val_loss improved from inf to 0.03285, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.4169 - mae: 0.4275 - mda: 0.5000 - rmse: 0.5166 - val_loss: 0.0328 - val_mae: 0.1509 - val_mda: 0.4995 - val_rmse: 0.1532 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0034 - mae: 0.0500 - mda: 0.5006 - rmse: 0.0590\n",
            "Epoch 2: val_loss did not improve from 0.03285\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0034 - mae: 0.0500 - mda: 0.5006 - rmse: 0.0590 - val_loss: 0.0337 - val_mae: 0.1542 - val_mda: 0.5016 - val_rmse: 0.1566 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0035 - mae: 0.0498 - mda: 0.5001 - rmse: 0.0594\n",
            "Epoch 3: val_loss did not improve from 0.03285\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0035 - mae: 0.0498 - mda: 0.5001 - rmse: 0.0595 - val_loss: 0.0408 - val_mae: 0.1755 - val_mda: 0.5023 - val_rmse: 0.1778 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0037 - mae: 0.0508 - mda: 0.4989 - rmse: 0.0616\n",
            "Epoch 4: val_loss did not improve from 0.03285\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0037 - mae: 0.0508 - mda: 0.4989 - rmse: 0.0616 - val_loss: 0.0377 - val_mae: 0.1673 - val_mda: 0.5035 - val_rmse: 0.1695 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0036 - mae: 0.0503 - mda: 0.5000 - rmse: 0.0615\n",
            "Epoch 5: val_loss improved from 0.03285 to 0.03114, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0036 - mae: 0.0503 - mda: 0.5000 - rmse: 0.0615 - val_loss: 0.0311 - val_mae: 0.1495 - val_mda: 0.5053 - val_rmse: 0.1517 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0035 - mae: 0.0491 - mda: 0.4996 - rmse: 0.0646\n",
            "Epoch 6: val_loss improved from 0.03114 to 0.01895, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0035 - mae: 0.0490 - mda: 0.4996 - rmse: 0.0646 - val_loss: 0.0189 - val_mae: 0.1167 - val_mda: 0.5064 - val_rmse: 0.1186 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0022 - mae: 0.0372 - mda: 0.4997 - rmse: 0.0697\n",
            "Epoch 7: val_loss improved from 0.01895 to 0.00625, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0022 - mae: 0.0371 - mda: 0.4997 - rmse: 0.0698 - val_loss: 0.0062 - val_mae: 0.0436 - val_mda: 0.5062 - val_rmse: 0.0470 - learning_rate: 0.0100\n",
            "Epoch 8/50\n",
            "\u001b[1m281/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.8407e-04 - mae: 0.0248 - mda: 0.5009 - rmse: 0.0759\n",
            "Epoch 8: val_loss did not improve from 0.00625\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 9.7826e-04 - mae: 0.0247 - mda: 0.5009 - rmse: 0.0759 - val_loss: 0.0093 - val_mae: 0.0640 - val_mda: 0.5059 - val_rmse: 0.0667 - learning_rate: 0.0100\n",
            "Epoch 9/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2818e-04 - mae: 0.0199 - mda: 0.5015 - rmse: 0.0782\n",
            "Epoch 9: val_loss did not improve from 0.00625\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 6.2593e-04 - mae: 0.0198 - mda: 0.5015 - rmse: 0.0782 - val_loss: 0.0103 - val_mae: 0.0595 - val_mda: 0.4977 - val_rmse: 0.0625 - learning_rate: 0.0100\n",
            "Epoch 10/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2615e-04 - mae: 0.0162 - mda: 0.5014 - rmse: 0.0778\n",
            "Epoch 10: val_loss did not improve from 0.00625\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.2668e-04 - mae: 0.0162 - mda: 0.5014 - rmse: 0.0778 - val_loss: 0.0166 - val_mae: 0.0890 - val_mda: 0.4991 - val_rmse: 0.0917 - learning_rate: 0.0100\n",
            "Epoch 11/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6862e-04 - mae: 0.0171 - mda: 0.5012 - rmse: 0.0769\n",
            "Epoch 11: val_loss did not improve from 0.00625\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.6729e-04 - mae: 0.0171 - mda: 0.5012 - rmse: 0.0769 - val_loss: 0.0107 - val_mae: 0.0597 - val_mda: 0.4970 - val_rmse: 0.0627 - learning_rate: 0.0100\n",
            "Epoch 12/50\n",
            "\u001b[1m281/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1033e-04 - mae: 0.0160 - mda: 0.5014 - rmse: 0.0791\n",
            "Epoch 12: val_loss did not improve from 0.00625\n",
            "\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.1037e-04 - mae: 0.0160 - mda: 0.5014 - rmse: 0.0790 - val_loss: 0.0111 - val_mae: 0.0626 - val_mda: 0.4975 - val_rmse: 0.0655 - learning_rate: 0.0100\n",
            "Epoch 13/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2254e-04 - mae: 0.0141 - mda: 0.5022 - rmse: 0.0780\n",
            "Epoch 13: val_loss did not improve from 0.00625\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.2121e-04 - mae: 0.0140 - mda: 0.5022 - rmse: 0.0780 - val_loss: 0.0131 - val_mae: 0.0688 - val_mda: 0.4977 - val_rmse: 0.0718 - learning_rate: 0.0050\n",
            "Epoch 14/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5507e-04 - mae: 0.0124 - mda: 0.5021 - rmse: 0.0784\n",
            "Epoch 14: val_loss did not improve from 0.00625\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.5534e-04 - mae: 0.0124 - mda: 0.5021 - rmse: 0.0784 - val_loss: 0.0134 - val_mae: 0.0714 - val_mda: 0.4998 - val_rmse: 0.0743 - learning_rate: 0.0050\n",
            "Epoch 15/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5681e-04 - mae: 0.0125 - mda: 0.5027 - rmse: 0.0781\n",
            "Epoch 15: val_loss did not improve from 0.00625\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.5717e-04 - mae: 0.0125 - mda: 0.5027 - rmse: 0.0781 - val_loss: 0.0110 - val_mae: 0.0608 - val_mda: 0.5004 - val_rmse: 0.0639 - learning_rate: 0.0050\n",
            "Epoch 16/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3136e-04 - mae: 0.0165 - mda: 0.5029 - rmse: 0.0780\n",
            "Epoch 16: val_loss did not improve from 0.00625\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.3015e-04 - mae: 0.0165 - mda: 0.5029 - rmse: 0.0780 - val_loss: 0.0138 - val_mae: 0.0746 - val_mda: 0.5008 - val_rmse: 0.0775 - learning_rate: 0.0050\n",
            "Epoch 17/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2766e-04 - mae: 0.0141 - mda: 0.5015 - rmse: 0.0777\n",
            "Epoch 17: val_loss did not improve from 0.00625\n",
            "\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.2682e-04 - mae: 0.0141 - mda: 0.5015 - rmse: 0.0777 - val_loss: 0.0133 - val_mae: 0.0720 - val_mda: 0.4969 - val_rmse: 0.0750 - learning_rate: 0.0050\n",
            "Epoch 17: early stopping\n",
            "Restoring model weights from the end of the best epoch: 7.\n",
            "Validation Loss: 0.00625, RMSE: 0.04701, MDA: 0.50641, MAE: 0.04361\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6003 - mae: 0.5210 - mda: 0.5001 - rmse: 0.6312\n",
            "Epoch 1: val_loss improved from inf to 0.04339, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.5938 - mae: 0.5167 - mda: 0.5001 - rmse: 0.6260 - val_loss: 0.0434 - val_mae: 0.1815 - val_mda: 0.4944 - val_rmse: 0.1839 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0037 - mae: 0.0512 - mda: 0.4996 - rmse: 0.0608\n",
            "Epoch 2: val_loss improved from 0.04339 to 0.03744, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0037 - mae: 0.0513 - mda: 0.4996 - rmse: 0.0608 - val_loss: 0.0374 - val_mae: 0.1646 - val_mda: 0.4980 - val_rmse: 0.1669 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0038 - mae: 0.0520 - mda: 0.4995 - rmse: 0.0609\n",
            "Epoch 3: val_loss improved from 0.03744 to 0.02907, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0038 - mae: 0.0520 - mda: 0.4995 - rmse: 0.0609 - val_loss: 0.0291 - val_mae: 0.1372 - val_mda: 0.4856 - val_rmse: 0.1397 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0043 - mae: 0.0546 - mda: 0.4998 - rmse: 0.0645\n",
            "Epoch 4: val_loss improved from 0.02907 to 0.02545, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0043 - mae: 0.0546 - mda: 0.4998 - rmse: 0.0646 - val_loss: 0.0254 - val_mae: 0.1240 - val_mda: 0.4932 - val_rmse: 0.1265 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0050 - mae: 0.0579 - mda: 0.4999 - rmse: 0.0703\n",
            "Epoch 5: val_loss did not improve from 0.02545\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0050 - mae: 0.0579 - mda: 0.4999 - rmse: 0.0703 - val_loss: 0.0554 - val_mae: 0.2119 - val_mda: 0.5012 - val_rmse: 0.2142 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0053 - mae: 0.0593 - mda: 0.5003 - rmse: 0.0718\n",
            "Epoch 6: val_loss did not improve from 0.02545\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0053 - mae: 0.0592 - mda: 0.5003 - rmse: 0.0717 - val_loss: 0.0449 - val_mae: 0.1857 - val_mda: 0.4999 - val_rmse: 0.1880 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0066 - mae: 0.0656 - mda: 0.4996 - rmse: 0.0792\n",
            "Epoch 7: val_loss did not improve from 0.02545\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0066 - mae: 0.0656 - mda: 0.4996 - rmse: 0.0792 - val_loss: 0.0318 - val_mae: 0.1470 - val_mda: 0.4982 - val_rmse: 0.1494 - learning_rate: 0.0100\n",
            "Epoch 8/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0055 - mae: 0.0601 - mda: 0.4999 - rmse: 0.0736\n",
            "Epoch 8: val_loss did not improve from 0.02545\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0055 - mae: 0.0601 - mda: 0.4999 - rmse: 0.0736 - val_loss: 0.0329 - val_mae: 0.1509 - val_mda: 0.5059 - val_rmse: 0.1533 - learning_rate: 0.0100\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0056 - mae: 0.0606 - mda: 0.4994 - rmse: 0.0741\n",
            "Epoch 9: val_loss improved from 0.02545 to 0.02005, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0056 - mae: 0.0606 - mda: 0.4994 - rmse: 0.0742 - val_loss: 0.0201 - val_mae: 0.1028 - val_mda: 0.4973 - val_rmse: 0.1054 - learning_rate: 0.0100\n",
            "Epoch 10/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0063 - mae: 0.0644 - mda: 0.5000 - rmse: 0.0783\n",
            "Epoch 10: val_loss did not improve from 0.02005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0063 - mae: 0.0644 - mda: 0.5000 - rmse: 0.0783 - val_loss: 0.0220 - val_mae: 0.1117 - val_mda: 0.5016 - val_rmse: 0.1142 - learning_rate: 0.0100\n",
            "Epoch 11/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0087 - mae: 0.0746 - mda: 0.5000 - rmse: 0.0904\n",
            "Epoch 11: val_loss did not improve from 0.02005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0087 - mae: 0.0746 - mda: 0.5000 - rmse: 0.0903 - val_loss: 0.0352 - val_mae: 0.1606 - val_mda: 0.5021 - val_rmse: 0.1628 - learning_rate: 0.0100\n",
            "Epoch 12/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0067 - mae: 0.0656 - mda: 0.4992 - rmse: 0.0807\n",
            "Epoch 12: val_loss did not improve from 0.02005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0067 - mae: 0.0656 - mda: 0.4992 - rmse: 0.0807 - val_loss: 0.0612 - val_mae: 0.2277 - val_mda: 0.5061 - val_rmse: 0.2299 - learning_rate: 0.0100\n",
            "Epoch 13/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0066 - mae: 0.0651 - mda: 0.4999 - rmse: 0.0823\n",
            "Epoch 13: val_loss improved from 0.02005 to 0.01382, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0066 - mae: 0.0651 - mda: 0.4999 - rmse: 0.0823 - val_loss: 0.0138 - val_mae: 0.0755 - val_mda: 0.5001 - val_rmse: 0.0782 - learning_rate: 0.0100\n",
            "Epoch 14/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0065 - mae: 0.0647 - mda: 0.4997 - rmse: 0.0798\n",
            "Epoch 14: val_loss did not improve from 0.01382\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0065 - mae: 0.0647 - mda: 0.4997 - rmse: 0.0798 - val_loss: 0.0419 - val_mae: 0.1825 - val_mda: 0.5106 - val_rmse: 0.1846 - learning_rate: 0.0100\n",
            "Epoch 15/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0070 - mae: 0.0674 - mda: 0.5000 - rmse: 0.0844\n",
            "Epoch 15: val_loss did not improve from 0.01382\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0070 - mae: 0.0674 - mda: 0.5000 - rmse: 0.0843 - val_loss: 0.0570 - val_mae: 0.2159 - val_mda: 0.4989 - val_rmse: 0.2182 - learning_rate: 0.0100\n",
            "Epoch 16/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0081 - mae: 0.0723 - mda: 0.4995 - rmse: 0.0879\n",
            "Epoch 16: val_loss did not improve from 0.01382\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0081 - mae: 0.0723 - mda: 0.4995 - rmse: 0.0879 - val_loss: 0.0259 - val_mae: 0.1294 - val_mda: 0.5058 - val_rmse: 0.1317 - learning_rate: 0.0100\n",
            "Epoch 17/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0068 - mae: 0.0661 - mda: 0.5003 - rmse: 0.0831\n",
            "Epoch 17: val_loss did not improve from 0.01382\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0068 - mae: 0.0661 - mda: 0.5003 - rmse: 0.0832 - val_loss: 0.0295 - val_mae: 0.1381 - val_mda: 0.4943 - val_rmse: 0.1405 - learning_rate: 0.0100\n",
            "Epoch 18/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0071 - mae: 0.0679 - mda: 0.4997 - rmse: 0.0858\n",
            "Epoch 18: val_loss did not improve from 0.01382\n",
            "\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0071 - mae: 0.0679 - mda: 0.4997 - rmse: 0.0858 - val_loss: 0.0190 - val_mae: 0.1212 - val_mda: 0.5094 - val_rmse: 0.1229 - learning_rate: 0.0100\n",
            "Epoch 19/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0014 - mae: 0.0280 - mda: 0.5004 - rmse: 0.0775\n",
            "Epoch 19: val_loss improved from 0.01382 to 0.00955, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0014 - mae: 0.0279 - mda: 0.5004 - rmse: 0.0775 - val_loss: 0.0096 - val_mae: 0.0624 - val_mda: 0.5122 - val_rmse: 0.0652 - learning_rate: 0.0050\n",
            "Epoch 20/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.3885e-04 - mae: 0.0164 - mda: 0.5014 - rmse: 0.0774\n",
            "Epoch 20: val_loss did not improve from 0.00955\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 4.3973e-04 - mae: 0.0164 - mda: 0.5014 - rmse: 0.0774 - val_loss: 0.0105 - val_mae: 0.0671 - val_mda: 0.5107 - val_rmse: 0.0695 - learning_rate: 0.0050\n",
            "Epoch 21/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.5154e-04 - mae: 0.0204 - mda: 0.5014 - rmse: 0.0786\n",
            "Epoch 21: val_loss did not improve from 0.00955\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 6.4916e-04 - mae: 0.0204 - mda: 0.5014 - rmse: 0.0786 - val_loss: 0.0103 - val_mae: 0.0656 - val_mda: 0.5118 - val_rmse: 0.0681 - learning_rate: 0.0050\n",
            "Epoch 22/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.6941e-04 - mae: 0.0151 - mda: 0.5010 - rmse: 0.0781\n",
            "Epoch 22: val_loss did not improve from 0.00955\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.7061e-04 - mae: 0.0151 - mda: 0.5010 - rmse: 0.0781 - val_loss: 0.0165 - val_mae: 0.0898 - val_mda: 0.5139 - val_rmse: 0.0925 - learning_rate: 0.0050\n",
            "Epoch 23/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.9905e-04 - mae: 0.0156 - mda: 0.5017 - rmse: 0.0777\n",
            "Epoch 23: val_loss improved from 0.00955 to 0.00924, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.9852e-04 - mae: 0.0156 - mda: 0.5017 - rmse: 0.0777 - val_loss: 0.0092 - val_mae: 0.0681 - val_mda: 0.5146 - val_rmse: 0.0703 - learning_rate: 0.0050\n",
            "Epoch 24/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.7127e-04 - mae: 0.0195 - mda: 0.5021 - rmse: 0.0800\n",
            "Epoch 24: val_loss improved from 0.00924 to 0.00854, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 6.7426e-04 - mae: 0.0195 - mda: 0.5021 - rmse: 0.0800 - val_loss: 0.0085 - val_mae: 0.0712 - val_mda: 0.5097 - val_rmse: 0.0733 - learning_rate: 0.0050\n",
            "Epoch 25/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.8069e-04 - mae: 0.0234 - mda: 0.5010 - rmse: 0.0789\n",
            "Epoch 25: val_loss improved from 0.00854 to 0.00791, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 8.7818e-04 - mae: 0.0234 - mda: 0.5010 - rmse: 0.0789 - val_loss: 0.0079 - val_mae: 0.0625 - val_mda: 0.5121 - val_rmse: 0.0652 - learning_rate: 0.0050\n",
            "Epoch 26/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0020 - mae: 0.0337 - mda: 0.5017 - rmse: 0.0829\n",
            "Epoch 26: val_loss did not improve from 0.00791\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0020 - mae: 0.0336 - mda: 0.5017 - rmse: 0.0829 - val_loss: 0.0128 - val_mae: 0.0700 - val_mda: 0.5113 - val_rmse: 0.0731 - learning_rate: 0.0050\n",
            "Epoch 27/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.0158e-04 - mae: 0.0210 - mda: 0.5019 - rmse: 0.0795\n",
            "Epoch 27: val_loss improved from 0.00791 to 0.00758, saving model to best_model.keras\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 7.0325e-04 - mae: 0.0210 - mda: 0.5019 - rmse: 0.0795 - val_loss: 0.0076 - val_mae: 0.0625 - val_mda: 0.5103 - val_rmse: 0.0648 - learning_rate: 0.0050\n",
            "Epoch 28/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.3431e-04 - mae: 0.0202 - mda: 0.5012 - rmse: 0.0798\n",
            "Epoch 28: val_loss did not improve from 0.00758\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 6.3299e-04 - mae: 0.0202 - mda: 0.5012 - rmse: 0.0798 - val_loss: 0.0134 - val_mae: 0.0719 - val_mda: 0.5099 - val_rmse: 0.0748 - learning_rate: 0.0050\n",
            "Epoch 29/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.5400e-04 - mae: 0.0211 - mda: 0.5009 - rmse: 0.0804\n",
            "Epoch 29: val_loss did not improve from 0.00758\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 7.7778e-04 - mae: 0.0213 - mda: 0.5009 - rmse: 0.0805 - val_loss: 0.0086 - val_mae: 0.0656 - val_mda: 0.5090 - val_rmse: 0.0678 - learning_rate: 0.0050\n",
            "Epoch 30/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0020 - mae: 0.0346 - mda: 0.5016 - rmse: 0.0842\n",
            "Epoch 30: val_loss did not improve from 0.00758\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0020 - mae: 0.0344 - mda: 0.5016 - rmse: 0.0841 - val_loss: 0.0137 - val_mae: 0.0763 - val_mda: 0.5111 - val_rmse: 0.0791 - learning_rate: 0.0050\n",
            "Epoch 31/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7.9746e-04 - mae: 0.0221 - mda: 0.5015 - rmse: 0.0809\n",
            "Epoch 31: val_loss did not improve from 0.00758\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 8.0009e-04 - mae: 0.0221 - mda: 0.5015 - rmse: 0.0809 - val_loss: 0.0086 - val_mae: 0.0621 - val_mda: 0.5080 - val_rmse: 0.0645 - learning_rate: 0.0050\n",
            "Epoch 32/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.8399e-04 - mae: 0.0234 - mda: 0.5015 - rmse: 0.0809\n",
            "Epoch 32: val_loss did not improve from 0.00758\n",
            "\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 8.8809e-04 - mae: 0.0234 - mda: 0.5015 - rmse: 0.0810 - val_loss: 0.0093 - val_mae: 0.0623 - val_mda: 0.5075 - val_rmse: 0.0649 - learning_rate: 0.0050\n",
            "Epoch 33/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.3878e-04 - mae: 0.0143 - mda: 0.5026 - rmse: 0.0791\n",
            "Epoch 33: val_loss did not improve from 0.00758\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.3779e-04 - mae: 0.0142 - mda: 0.5026 - rmse: 0.0791 - val_loss: 0.0101 - val_mae: 0.0627 - val_mda: 0.5095 - val_rmse: 0.0655 - learning_rate: 0.0025\n",
            "Epoch 34/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.1993e-04 - mae: 0.0118 - mda: 0.5024 - rmse: 0.0785\n",
            "Epoch 34: val_loss did not improve from 0.00758\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.1965e-04 - mae: 0.0118 - mda: 0.5024 - rmse: 0.0785 - val_loss: 0.0099 - val_mae: 0.0640 - val_mda: 0.5102 - val_rmse: 0.0666 - learning_rate: 0.0025\n",
            "Epoch 35/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.8922e-04 - mae: 0.0109 - mda: 0.5025 - rmse: 0.0787\n",
            "Epoch 35: val_loss did not improve from 0.00758\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.8922e-04 - mae: 0.0109 - mda: 0.5025 - rmse: 0.0787 - val_loss: 0.0101 - val_mae: 0.0639 - val_mda: 0.5091 - val_rmse: 0.0666 - learning_rate: 0.0025\n",
            "Epoch 36/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.8482e-04 - mae: 0.0107 - mda: 0.5022 - rmse: 0.0785\n",
            "Epoch 36: val_loss did not improve from 0.00758\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.8530e-04 - mae: 0.0107 - mda: 0.5022 - rmse: 0.0785 - val_loss: 0.0091 - val_mae: 0.0656 - val_mda: 0.5082 - val_rmse: 0.0681 - learning_rate: 0.0025\n",
            "Epoch 37/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4140e-04 - mae: 0.0122 - mda: 0.5016 - rmse: 0.0796\n",
            "Epoch 37: val_loss did not improve from 0.00758\n",
            "\n",
            "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.4175e-04 - mae: 0.0122 - mda: 0.5016 - rmse: 0.0796 - val_loss: 0.0096 - val_mae: 0.0617 - val_mda: 0.5082 - val_rmse: 0.0646 - learning_rate: 0.0025\n",
            "Epoch 37: early stopping\n",
            "Restoring model weights from the end of the best epoch: 27.\n",
            "Validation Loss: 0.00758, RMSE: 0.06455, MDA: 0.51464, MAE: 0.06168\n",
            "--------------------------------------------------\n",
            "Best SimpleRNN Parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 128}, Validation Loss: 1.6153358956216834e-05\n"
          ]
        }
      ],
      "source": [
        "# Tuning the models\n",
        "print(\"Tuning SimpleRNN...\")\n",
        "best_rnn_model, best_rnn_params, best_rnn_loss, rnn_results = tune_model(build_simple_rnn, X_train, y_train, X_val, y_val, param_grid)\n",
        "print(f\"Best SimpleRNN Parameters: {best_rnn_params}, Validation Loss: {best_rnn_loss}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5739a4e",
      "metadata": {
        "id": "d5739a4e"
      },
      "source": [
        "### Tuning GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bd0beb0",
      "metadata": {
        "scrolled": true,
        "id": "2bd0beb0",
        "outputId": "1b23d80b-411e-47bc-9f51-0b0895f318f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning GRU...\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4313e-04 - mae: 0.0127 - mda: 0.5065 - rmse: 0.0768\n",
            "Epoch 1: val_loss improved from inf to 0.00014, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 4.3734e-04 - mae: 0.0126 - mda: 0.5065 - rmse: 0.0768 - val_loss: 1.3741e-04 - val_mae: 0.0094 - val_mda: 0.5242 - val_rmse: 0.0111 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5683e-05 - mae: 0.0057 - mda: 0.5083 - rmse: 0.0771\n",
            "Epoch 2: val_loss improved from 0.00014 to 0.00013, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.5607e-05 - mae: 0.0057 - mda: 0.5083 - rmse: 0.0771 - val_loss: 1.3262e-04 - val_mae: 0.0088 - val_mda: 0.5240 - val_rmse: 0.0106 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.1329e-05 - mae: 0.0056 - mda: 0.5069 - rmse: 0.0761\n",
            "Epoch 3: val_loss improved from 0.00013 to 0.00008, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.1325e-05 - mae: 0.0056 - mda: 0.5069 - rmse: 0.0761 - val_loss: 8.3846e-05 - val_mae: 0.0071 - val_mda: 0.5239 - val_rmse: 0.0091 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m570/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.4030e-05 - mae: 0.0054 - mda: 0.5074 - rmse: 0.0766\n",
            "Epoch 4: val_loss improved from 0.00008 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.4032e-05 - mae: 0.0054 - mda: 0.5074 - rmse: 0.0766 - val_loss: 3.0091e-05 - val_mae: 0.0041 - val_mda: 0.5254 - val_rmse: 0.0065 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7204e-05 - mae: 0.0050 - mda: 0.5073 - rmse: 0.0751\n",
            "Epoch 5: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.7280e-05 - mae: 0.0050 - mda: 0.5073 - rmse: 0.0752 - val_loss: 8.8797e-05 - val_mae: 0.0068 - val_mda: 0.5250 - val_rmse: 0.0089 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6821e-05 - mae: 0.0050 - mda: 0.5065 - rmse: 0.0766\n",
            "Epoch 6: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.6816e-05 - mae: 0.0050 - mda: 0.5065 - rmse: 0.0766 - val_loss: 5.2672e-05 - val_mae: 0.0055 - val_mda: 0.5256 - val_rmse: 0.0077 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.3374e-05 - mae: 0.0053 - mda: 0.5076 - rmse: 0.0770\n",
            "Epoch 7: val_loss improved from 0.00003 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.3367e-05 - mae: 0.0053 - mda: 0.5076 - rmse: 0.0770 - val_loss: 2.1525e-05 - val_mae: 0.0035 - val_mda: 0.5260 - val_rmse: 0.0060 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.1874e-05 - mae: 0.0053 - mda: 0.5081 - rmse: 0.0763\n",
            "Epoch 8: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.1802e-05 - mae: 0.0053 - mda: 0.5081 - rmse: 0.0763 - val_loss: 7.5855e-05 - val_mae: 0.0064 - val_mda: 0.5272 - val_rmse: 0.0085 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6734e-05 - mae: 0.0050 - mda: 0.5076 - rmse: 0.0760\n",
            "Epoch 9: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.6685e-05 - mae: 0.0050 - mda: 0.5076 - rmse: 0.0760 - val_loss: 1.9765e-04 - val_mae: 0.0115 - val_mda: 0.5267 - val_rmse: 0.0131 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0135e-05 - mae: 0.0045 - mda: 0.5085 - rmse: 0.0765\n",
            "Epoch 10: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.0100e-05 - mae: 0.0045 - mda: 0.5085 - rmse: 0.0765 - val_loss: 2.6287e-05 - val_mae: 0.0039 - val_mda: 0.5267 - val_rmse: 0.0063 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0970e-05 - mae: 0.0045 - mda: 0.5087 - rmse: 0.0764\n",
            "Epoch 11: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.0961e-05 - mae: 0.0045 - mda: 0.5086 - rmse: 0.0764 - val_loss: 3.2665e-05 - val_mae: 0.0044 - val_mda: 0.5272 - val_rmse: 0.0066 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0469e-05 - mae: 0.0046 - mda: 0.5079 - rmse: 0.0770\n",
            "Epoch 12: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.0435e-05 - mae: 0.0046 - mda: 0.5079 - rmse: 0.0770 - val_loss: 4.7496e-05 - val_mae: 0.0050 - val_mda: 0.5276 - val_rmse: 0.0072 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m570/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9057e-05 - mae: 0.0045 - mda: 0.5082 - rmse: 0.0765\n",
            "Epoch 13: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.9073e-05 - mae: 0.0045 - mda: 0.5082 - rmse: 0.0765 - val_loss: 2.5972e-05 - val_mae: 0.0038 - val_mda: 0.5273 - val_rmse: 0.0062 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9578e-05 - mae: 0.0045 - mda: 0.5084 - rmse: 0.0775\n",
            "Epoch 14: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.9579e-05 - mae: 0.0045 - mda: 0.5084 - rmse: 0.0775 - val_loss: 1.7761e-04 - val_mae: 0.0105 - val_mda: 0.5272 - val_rmse: 0.0121 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0005e-05 - mae: 0.0044 - mda: 0.5072 - rmse: 0.0761\n",
            "Epoch 15: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.9985e-05 - mae: 0.0044 - mda: 0.5072 - rmse: 0.0761 - val_loss: 7.6862e-05 - val_mae: 0.0064 - val_mda: 0.5281 - val_rmse: 0.0085 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8051e-05 - mae: 0.0043 - mda: 0.5071 - rmse: 0.0766\n",
            "Epoch 16: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.8046e-05 - mae: 0.0043 - mda: 0.5071 - rmse: 0.0766 - val_loss: 6.8072e-05 - val_mae: 0.0061 - val_mda: 0.5275 - val_rmse: 0.0081 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m570/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7039e-05 - mae: 0.0043 - mda: 0.5087 - rmse: 0.0760\n",
            "Epoch 17: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.7047e-05 - mae: 0.0043 - mda: 0.5086 - rmse: 0.0760 - val_loss: 7.2628e-05 - val_mae: 0.0063 - val_mda: 0.5275 - val_rmse: 0.0083 - learning_rate: 2.5000e-04\n",
            "Epoch 17: early stopping\n",
            "Restoring model weights from the end of the best epoch: 7.\n",
            "Validation Loss: 0.00002, RMSE: 0.00598, MDA: 0.52810, MAE: 0.00348\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5390e-04 - mae: 0.0079 - mda: 0.5071 - rmse: 0.0760\n",
            "Epoch 1: val_loss improved from inf to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 1.5282e-04 - mae: 0.0079 - mda: 0.5071 - rmse: 0.0761 - val_loss: 4.4849e-05 - val_mae: 0.0053 - val_mda: 0.5247 - val_rmse: 0.0077 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.5159e-05 - mae: 0.0048 - mda: 0.5087 - rmse: 0.0767\n",
            "Epoch 2: val_loss improved from 0.00004 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.5163e-05 - mae: 0.0048 - mda: 0.5087 - rmse: 0.0767 - val_loss: 3.3119e-05 - val_mae: 0.0045 - val_mda: 0.5260 - val_rmse: 0.0072 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9327e-05 - mae: 0.0045 - mda: 0.5081 - rmse: 0.0757\n",
            "Epoch 3: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.9317e-05 - mae: 0.0045 - mda: 0.5081 - rmse: 0.0757 - val_loss: 3.7944e-05 - val_mae: 0.0051 - val_mda: 0.5244 - val_rmse: 0.0077 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1646e-05 - mae: 0.0041 - mda: 0.5078 - rmse: 0.0759\n",
            "Epoch 4: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.1657e-05 - mae: 0.0041 - mda: 0.5078 - rmse: 0.0759 - val_loss: 4.8427e-05 - val_mae: 0.0050 - val_mda: 0.5240 - val_rmse: 0.0075 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.6211e-05 - mae: 0.0043 - mda: 0.5084 - rmse: 0.0764\n",
            "Epoch 5: val_loss improved from 0.00003 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.6197e-05 - mae: 0.0043 - mda: 0.5084 - rmse: 0.0764 - val_loss: 2.3699e-05 - val_mae: 0.0036 - val_mda: 0.5252 - val_rmse: 0.0064 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2874e-05 - mae: 0.0041 - mda: 0.5066 - rmse: 0.0770\n",
            "Epoch 6: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.2865e-05 - mae: 0.0041 - mda: 0.5066 - rmse: 0.0770 - val_loss: 2.7909e-05 - val_mae: 0.0038 - val_mda: 0.5252 - val_rmse: 0.0066 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.4611e-05 - mae: 0.0036 - mda: 0.5079 - rmse: 0.0771\n",
            "Epoch 7: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.4614e-05 - mae: 0.0036 - mda: 0.5079 - rmse: 0.0771 - val_loss: 6.0086e-05 - val_mae: 0.0059 - val_mda: 0.5251 - val_rmse: 0.0083 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5802e-05 - mae: 0.0036 - mda: 0.5075 - rmse: 0.0768\n",
            "Epoch 8: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.5804e-05 - mae: 0.0036 - mda: 0.5075 - rmse: 0.0768 - val_loss: 2.3841e-05 - val_mae: 0.0035 - val_mda: 0.5245 - val_rmse: 0.0064 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5731e-05 - mae: 0.0036 - mda: 0.5079 - rmse: 0.0770\n",
            "Epoch 9: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.5729e-05 - mae: 0.0036 - mda: 0.5079 - rmse: 0.0770 - val_loss: 7.2435e-05 - val_mae: 0.0073 - val_mda: 0.5258 - val_rmse: 0.0096 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6389e-05 - mae: 0.0037 - mda: 0.5081 - rmse: 0.0765\n",
            "Epoch 10: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.6392e-05 - mae: 0.0037 - mda: 0.5081 - rmse: 0.0765 - val_loss: 4.8469e-05 - val_mae: 0.0052 - val_mda: 0.5245 - val_rmse: 0.0077 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5857e-05 - mae: 0.0037 - mda: 0.5090 - rmse: 0.0759\n",
            "Epoch 11: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.5857e-05 - mae: 0.0037 - mda: 0.5090 - rmse: 0.0759 - val_loss: 4.0226e-05 - val_mae: 0.0045 - val_mda: 0.5239 - val_rmse: 0.0071 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2169e-05 - mae: 0.0034 - mda: 0.5098 - rmse: 0.0764\n",
            "Epoch 12: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.2177e-05 - mae: 0.0034 - mda: 0.5098 - rmse: 0.0764 - val_loss: 4.0922e-05 - val_mae: 0.0046 - val_mda: 0.5239 - val_rmse: 0.0072 - learning_rate: 2.5000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2214e-05 - mae: 0.0034 - mda: 0.5087 - rmse: 0.0769\n",
            "Epoch 13: val_loss improved from 0.00002 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.2223e-05 - mae: 0.0034 - mda: 0.5087 - rmse: 0.0769 - val_loss: 1.9740e-05 - val_mae: 0.0033 - val_mda: 0.5239 - val_rmse: 0.0062 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2835e-05 - mae: 0.0033 - mda: 0.5084 - rmse: 0.0773\n",
            "Epoch 14: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.2835e-05 - mae: 0.0033 - mda: 0.5084 - rmse: 0.0773 - val_loss: 3.0084e-05 - val_mae: 0.0039 - val_mda: 0.5241 - val_rmse: 0.0066 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2845e-05 - mae: 0.0034 - mda: 0.5091 - rmse: 0.0761\n",
            "Epoch 15: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.2843e-05 - mae: 0.0034 - mda: 0.5091 - rmse: 0.0761 - val_loss: 2.7855e-05 - val_mae: 0.0038 - val_mda: 0.5243 - val_rmse: 0.0065 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2227e-05 - mae: 0.0034 - mda: 0.5083 - rmse: 0.0764\n",
            "Epoch 16: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.2231e-05 - mae: 0.0034 - mda: 0.5083 - rmse: 0.0764 - val_loss: 2.0280e-05 - val_mae: 0.0035 - val_mda: 0.5248 - val_rmse: 0.0064 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1426e-05 - mae: 0.0032 - mda: 0.5083 - rmse: 0.0760\n",
            "Epoch 17: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.1418e-05 - mae: 0.0032 - mda: 0.5083 - rmse: 0.0760 - val_loss: 4.0661e-05 - val_mae: 0.0047 - val_mda: 0.5256 - val_rmse: 0.0072 - learning_rate: 1.2500e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0328e-05 - mae: 0.0031 - mda: 0.5092 - rmse: 0.0762\n",
            "Epoch 18: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.0330e-05 - mae: 0.0031 - mda: 0.5092 - rmse: 0.0762 - val_loss: 4.2669e-05 - val_mae: 0.0047 - val_mda: 0.5254 - val_rmse: 0.0073 - learning_rate: 1.2500e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0500e-05 - mae: 0.0032 - mda: 0.5085 - rmse: 0.0772\n",
            "Epoch 19: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.0505e-05 - mae: 0.0032 - mda: 0.5085 - rmse: 0.0772 - val_loss: 3.4081e-05 - val_mae: 0.0042 - val_mda: 0.5245 - val_rmse: 0.0069 - learning_rate: 1.2500e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0695e-05 - mae: 0.0032 - mda: 0.5087 - rmse: 0.0762\n",
            "Epoch 20: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.0697e-05 - mae: 0.0032 - mda: 0.5087 - rmse: 0.0762 - val_loss: 2.8017e-05 - val_mae: 0.0038 - val_mda: 0.5254 - val_rmse: 0.0066 - learning_rate: 1.2500e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9411e-05 - mae: 0.0031 - mda: 0.5082 - rmse: 0.0763\n",
            "Epoch 21: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1.9419e-05 - mae: 0.0031 - mda: 0.5082 - rmse: 0.0763 - val_loss: 2.7968e-05 - val_mae: 0.0038 - val_mda: 0.5254 - val_rmse: 0.0066 - learning_rate: 1.2500e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9411e-05 - mae: 0.0031 - mda: 0.5090 - rmse: 0.0760\n",
            "Epoch 22: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1.9416e-05 - mae: 0.0031 - mda: 0.5090 - rmse: 0.0760 - val_loss: 2.2618e-05 - val_mae: 0.0034 - val_mda: 0.5254 - val_rmse: 0.0063 - learning_rate: 6.2500e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0305e-05 - mae: 0.0032 - mda: 0.5087 - rmse: 0.0761\n",
            "Epoch 23: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.0305e-05 - mae: 0.0032 - mda: 0.5087 - rmse: 0.0761 - val_loss: 3.4328e-05 - val_mae: 0.0042 - val_mda: 0.5252 - val_rmse: 0.0069 - learning_rate: 6.2500e-05\n",
            "Epoch 23: early stopping\n",
            "Restoring model weights from the end of the best epoch: 13.\n",
            "Validation Loss: 0.00002, RMSE: 0.00615, MDA: 0.52603, MAE: 0.00329\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.9831e-04 - mae: 0.0076 - mda: 0.5077 - rmse: 0.0770\n",
            "Epoch 1: val_loss improved from inf to 0.00039, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.9766e-04 - mae: 0.0075 - mda: 0.5077 - rmse: 0.0770 - val_loss: 3.8893e-04 - val_mae: 0.0177 - val_mda: 0.5245 - val_rmse: 0.0188 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6581e-05 - mae: 0.0043 - mda: 0.5100 - rmse: 0.0760\n",
            "Epoch 2: val_loss improved from 0.00039 to 0.00018, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 3.6573e-05 - mae: 0.0043 - mda: 0.5100 - rmse: 0.0760 - val_loss: 1.8285e-04 - val_mae: 0.0117 - val_mda: 0.5245 - val_rmse: 0.0133 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1021e-05 - mae: 0.0040 - mda: 0.5079 - rmse: 0.0769\n",
            "Epoch 3: val_loss improved from 0.00018 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 3.1013e-05 - mae: 0.0040 - mda: 0.5080 - rmse: 0.0769 - val_loss: 2.8534e-05 - val_mae: 0.0041 - val_mda: 0.5222 - val_rmse: 0.0069 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.7186e-05 - mae: 0.0038 - mda: 0.5086 - rmse: 0.0767\n",
            "Epoch 4: val_loss improved from 0.00003 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.7188e-05 - mae: 0.0038 - mda: 0.5086 - rmse: 0.0767 - val_loss: 2.5399e-05 - val_mae: 0.0039 - val_mda: 0.5220 - val_rmse: 0.0069 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.4805e-05 - mae: 0.0037 - mda: 0.5077 - rmse: 0.0765\n",
            "Epoch 5: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.4803e-05 - mae: 0.0037 - mda: 0.5077 - rmse: 0.0765 - val_loss: 1.1064e-04 - val_mae: 0.0089 - val_mda: 0.5239 - val_rmse: 0.0110 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.3212e-05 - mae: 0.0035 - mda: 0.5083 - rmse: 0.0771\n",
            "Epoch 6: val_loss improved from 0.00003 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.3210e-05 - mae: 0.0035 - mda: 0.5083 - rmse: 0.0771 - val_loss: 1.7276e-05 - val_mae: 0.0031 - val_mda: 0.5221 - val_rmse: 0.0062 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.3655e-05 - mae: 0.0035 - mda: 0.5092 - rmse: 0.0771\n",
            "Epoch 7: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.3650e-05 - mae: 0.0035 - mda: 0.5092 - rmse: 0.0771 - val_loss: 6.4505e-05 - val_mae: 0.0062 - val_mda: 0.5219 - val_rmse: 0.0087 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.1643e-05 - mae: 0.0034 - mda: 0.5075 - rmse: 0.0764\n",
            "Epoch 8: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.1644e-05 - mae: 0.0034 - mda: 0.5075 - rmse: 0.0764 - val_loss: 2.3020e-05 - val_mae: 0.0035 - val_mda: 0.5222 - val_rmse: 0.0066 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.6495e-05 - mae: 0.0029 - mda: 0.5083 - rmse: 0.0765\n",
            "Epoch 9: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.6496e-05 - mae: 0.0029 - mda: 0.5083 - rmse: 0.0765 - val_loss: 3.0020e-05 - val_mae: 0.0041 - val_mda: 0.5227 - val_rmse: 0.0070 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.6324e-05 - mae: 0.0029 - mda: 0.5099 - rmse: 0.0763\n",
            "Epoch 10: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.6323e-05 - mae: 0.0029 - mda: 0.5099 - rmse: 0.0763 - val_loss: 3.8035e-05 - val_mae: 0.0046 - val_mda: 0.5225 - val_rmse: 0.0074 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.6523e-05 - mae: 0.0029 - mda: 0.5088 - rmse: 0.0764\n",
            "Epoch 11: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.6527e-05 - mae: 0.0029 - mda: 0.5088 - rmse: 0.0764 - val_loss: 2.2885e-05 - val_mae: 0.0038 - val_mda: 0.5222 - val_rmse: 0.0068 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.8096e-05 - mae: 0.0031 - mda: 0.5078 - rmse: 0.0769\n",
            "Epoch 12: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.8092e-05 - mae: 0.0031 - mda: 0.5078 - rmse: 0.0769 - val_loss: 1.8692e-05 - val_mae: 0.0033 - val_mda: 0.5220 - val_rmse: 0.0065 - learning_rate: 5.0000e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.5706e-05 - mae: 0.0029 - mda: 0.5089 - rmse: 0.0760\n",
            "Epoch 13: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.5708e-05 - mae: 0.0029 - mda: 0.5089 - rmse: 0.0760 - val_loss: 1.7934e-05 - val_mae: 0.0031 - val_mda: 0.5227 - val_rmse: 0.0062 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.3856e-05 - mae: 0.0026 - mda: 0.5082 - rmse: 0.0769\n",
            "Epoch 14: val_loss improved from 0.00002 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.3857e-05 - mae: 0.0026 - mda: 0.5082 - rmse: 0.0769 - val_loss: 1.5270e-05 - val_mae: 0.0029 - val_mda: 0.5218 - val_rmse: 0.0061 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.4074e-05 - mae: 0.0026 - mda: 0.5084 - rmse: 0.0776\n",
            "Epoch 15: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.4075e-05 - mae: 0.0026 - mda: 0.5084 - rmse: 0.0776 - val_loss: 6.4713e-05 - val_mae: 0.0065 - val_mda: 0.5213 - val_rmse: 0.0089 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.3971e-05 - mae: 0.0027 - mda: 0.5084 - rmse: 0.0764\n",
            "Epoch 16: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.3971e-05 - mae: 0.0027 - mda: 0.5084 - rmse: 0.0764 - val_loss: 7.2493e-05 - val_mae: 0.0069 - val_mda: 0.5214 - val_rmse: 0.0092 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.5569e-05 - mae: 0.0028 - mda: 0.5075 - rmse: 0.0760\n",
            "Epoch 17: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.5565e-05 - mae: 0.0028 - mda: 0.5075 - rmse: 0.0760 - val_loss: 1.6069e-05 - val_mae: 0.0030 - val_mda: 0.5220 - val_rmse: 0.0062 - learning_rate: 2.5000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.6081e-05 - mae: 0.0028 - mda: 0.5075 - rmse: 0.0773\n",
            "Epoch 18: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.6077e-05 - mae: 0.0028 - mda: 0.5075 - rmse: 0.0773 - val_loss: 3.1158e-05 - val_mae: 0.0043 - val_mda: 0.5220 - val_rmse: 0.0071 - learning_rate: 2.5000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.2370e-05 - mae: 0.0025 - mda: 0.5083 - rmse: 0.0760\n",
            "Epoch 19: val_loss improved from 0.00002 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.2373e-05 - mae: 0.0025 - mda: 0.5083 - rmse: 0.0760 - val_loss: 1.5254e-05 - val_mae: 0.0029 - val_mda: 0.5214 - val_rmse: 0.0062 - learning_rate: 1.2500e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.1649e-05 - mae: 0.0024 - mda: 0.5085 - rmse: 0.0764\n",
            "Epoch 20: val_loss improved from 0.00002 to 0.00001, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.1652e-05 - mae: 0.0024 - mda: 0.5085 - rmse: 0.0764 - val_loss: 1.4988e-05 - val_mae: 0.0029 - val_mda: 0.5214 - val_rmse: 0.0061 - learning_rate: 1.2500e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.2633e-05 - mae: 0.0025 - mda: 0.5091 - rmse: 0.0772\n",
            "Epoch 21: val_loss improved from 0.00001 to 0.00001, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.2633e-05 - mae: 0.0025 - mda: 0.5091 - rmse: 0.0772 - val_loss: 1.4444e-05 - val_mae: 0.0028 - val_mda: 0.5214 - val_rmse: 0.0061 - learning_rate: 1.2500e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.2885e-05 - mae: 0.0025 - mda: 0.5086 - rmse: 0.0764\n",
            "Epoch 22: val_loss did not improve from 0.00001\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.2887e-05 - mae: 0.0025 - mda: 0.5086 - rmse: 0.0764 - val_loss: 1.6665e-05 - val_mae: 0.0030 - val_mda: 0.5210 - val_rmse: 0.0062 - learning_rate: 1.2500e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.3382e-05 - mae: 0.0026 - mda: 0.5088 - rmse: 0.0767\n",
            "Epoch 23: val_loss did not improve from 0.00001\n",
            "\n",
            "Epoch 23: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.3381e-05 - mae: 0.0026 - mda: 0.5088 - rmse: 0.0767 - val_loss: 2.1564e-05 - val_mae: 0.0034 - val_mda: 0.5209 - val_rmse: 0.0065 - learning_rate: 1.2500e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.2303e-05 - mae: 0.0024 - mda: 0.5087 - rmse: 0.0775\n",
            "Epoch 24: val_loss did not improve from 0.00001\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.2301e-05 - mae: 0.0024 - mda: 0.5087 - rmse: 0.0775 - val_loss: 3.3954e-05 - val_mae: 0.0044 - val_mda: 0.5208 - val_rmse: 0.0072 - learning_rate: 6.2500e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.2423e-05 - mae: 0.0024 - mda: 0.5085 - rmse: 0.0766\n",
            "Epoch 25: val_loss did not improve from 0.00001\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.2421e-05 - mae: 0.0024 - mda: 0.5085 - rmse: 0.0766 - val_loss: 2.3823e-05 - val_mae: 0.0036 - val_mda: 0.5208 - val_rmse: 0.0067 - learning_rate: 6.2500e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.1693e-05 - mae: 0.0024 - mda: 0.5098 - rmse: 0.0768\n",
            "Epoch 26: val_loss improved from 0.00001 to 0.00001, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.1695e-05 - mae: 0.0024 - mda: 0.5098 - rmse: 0.0768 - val_loss: 1.4038e-05 - val_mae: 0.0028 - val_mda: 0.5206 - val_rmse: 0.0061 - learning_rate: 6.2500e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.1881e-05 - mae: 0.0024 - mda: 0.5071 - rmse: 0.0765\n",
            "Epoch 27: val_loss did not improve from 0.00001\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.1881e-05 - mae: 0.0024 - mda: 0.5071 - rmse: 0.0765 - val_loss: 1.4811e-05 - val_mae: 0.0029 - val_mda: 0.5201 - val_rmse: 0.0062 - learning_rate: 6.2500e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.1546e-05 - mae: 0.0024 - mda: 0.5074 - rmse: 0.0758\n",
            "Epoch 28: val_loss did not improve from 0.00001\n",
            "\n",
            "Epoch 28: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.1548e-05 - mae: 0.0024 - mda: 0.5074 - rmse: 0.0758 - val_loss: 2.5542e-05 - val_mae: 0.0037 - val_mda: 0.5203 - val_rmse: 0.0067 - learning_rate: 6.2500e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.0829e-05 - mae: 0.0023 - mda: 0.5087 - rmse: 0.0759\n",
            "Epoch 29: val_loss did not improve from 0.00001\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.0830e-05 - mae: 0.0023 - mda: 0.5087 - rmse: 0.0759 - val_loss: 2.1248e-05 - val_mae: 0.0034 - val_mda: 0.5199 - val_rmse: 0.0065 - learning_rate: 3.1250e-05\n",
            "Epoch 30/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.0995e-05 - mae: 0.0023 - mda: 0.5107 - rmse: 0.0762\n",
            "Epoch 30: val_loss did not improve from 0.00001\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.0996e-05 - mae: 0.0023 - mda: 0.5107 - rmse: 0.0762 - val_loss: 2.6050e-05 - val_mae: 0.0038 - val_mda: 0.5195 - val_rmse: 0.0068 - learning_rate: 3.1250e-05\n",
            "Epoch 31/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.1290e-05 - mae: 0.0024 - mda: 0.5082 - rmse: 0.0769\n",
            "Epoch 31: val_loss did not improve from 0.00001\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.1292e-05 - mae: 0.0024 - mda: 0.5082 - rmse: 0.0769 - val_loss: 1.4125e-05 - val_mae: 0.0028 - val_mda: 0.5201 - val_rmse: 0.0061 - learning_rate: 3.1250e-05\n",
            "Epoch 32/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.1744e-05 - mae: 0.0024 - mda: 0.5090 - rmse: 0.0771\n",
            "Epoch 32: val_loss did not improve from 0.00001\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.1744e-05 - mae: 0.0024 - mda: 0.5090 - rmse: 0.0771 - val_loss: 2.5769e-05 - val_mae: 0.0037 - val_mda: 0.5196 - val_rmse: 0.0068 - learning_rate: 3.1250e-05\n",
            "Epoch 33/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.0789e-05 - mae: 0.0023 - mda: 0.5081 - rmse: 0.0763\n",
            "Epoch 33: val_loss did not improve from 0.00001\n",
            "\n",
            "Epoch 33: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.0790e-05 - mae: 0.0023 - mda: 0.5081 - rmse: 0.0763 - val_loss: 2.6621e-05 - val_mae: 0.0038 - val_mda: 0.5195 - val_rmse: 0.0068 - learning_rate: 3.1250e-05\n",
            "Epoch 34/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.1617e-05 - mae: 0.0024 - mda: 0.5094 - rmse: 0.0763\n",
            "Epoch 34: val_loss did not improve from 0.00001\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.1617e-05 - mae: 0.0024 - mda: 0.5094 - rmse: 0.0763 - val_loss: 2.5638e-05 - val_mae: 0.0038 - val_mda: 0.5198 - val_rmse: 0.0068 - learning_rate: 1.5625e-05\n",
            "Epoch 35/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.1301e-05 - mae: 0.0023 - mda: 0.5096 - rmse: 0.0762\n",
            "Epoch 35: val_loss did not improve from 0.00001\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.1300e-05 - mae: 0.0023 - mda: 0.5096 - rmse: 0.0762 - val_loss: 2.1010e-05 - val_mae: 0.0033 - val_mda: 0.5189 - val_rmse: 0.0065 - learning_rate: 1.5625e-05\n",
            "Epoch 36/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.1714e-05 - mae: 0.0024 - mda: 0.5094 - rmse: 0.0769\n",
            "Epoch 36: val_loss did not improve from 0.00001\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.1712e-05 - mae: 0.0024 - mda: 0.5094 - rmse: 0.0769 - val_loss: 2.3157e-05 - val_mae: 0.0035 - val_mda: 0.5196 - val_rmse: 0.0066 - learning_rate: 1.5625e-05\n",
            "Epoch 36: early stopping\n",
            "Restoring model weights from the end of the best epoch: 26.\n",
            "Validation Loss: 0.00001, RMSE: 0.00607, MDA: 0.52453, MAE: 0.00277\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2076e-04 - mae: 0.0138 - mda: 0.5078 - rmse: 0.0774\n",
            "Epoch 1: val_loss improved from inf to 0.00010, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 7.1814e-04 - mae: 0.0138 - mda: 0.5078 - rmse: 0.0773 - val_loss: 1.0310e-04 - val_mae: 0.0081 - val_mda: 0.5267 - val_rmse: 0.0100 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7618e-05 - mae: 0.0061 - mda: 0.5069 - rmse: 0.0759\n",
            "Epoch 2: val_loss did not improve from 0.00010\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.7728e-05 - mae: 0.0061 - mda: 0.5069 - rmse: 0.0759 - val_loss: 2.0870e-04 - val_mae: 0.0089 - val_mda: 0.5283 - val_rmse: 0.0107 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.1904e-05 - mae: 0.0058 - mda: 0.5076 - rmse: 0.0751\n",
            "Epoch 3: val_loss improved from 0.00010 to 0.00005, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.1919e-05 - mae: 0.0058 - mda: 0.5076 - rmse: 0.0752 - val_loss: 4.6775e-05 - val_mae: 0.0057 - val_mda: 0.5276 - val_rmse: 0.0080 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.8067e-05 - mae: 0.0065 - mda: 0.5074 - rmse: 0.0767\n",
            "Epoch 4: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 7.7849e-05 - mae: 0.0065 - mda: 0.5074 - rmse: 0.0767 - val_loss: 1.0499e-04 - val_mae: 0.0066 - val_mda: 0.5258 - val_rmse: 0.0087 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7707e-05 - mae: 0.0061 - mda: 0.5084 - rmse: 0.0771\n",
            "Epoch 5: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.7600e-05 - mae: 0.0061 - mda: 0.5083 - rmse: 0.0771 - val_loss: 1.8361e-04 - val_mae: 0.0086 - val_mda: 0.5241 - val_rmse: 0.0104 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2075e-05 - mae: 0.0063 - mda: 0.5089 - rmse: 0.0768\n",
            "Epoch 6: val_loss did not improve from 0.00005\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 7.2001e-05 - mae: 0.0063 - mda: 0.5088 - rmse: 0.0768 - val_loss: 7.0900e-05 - val_mae: 0.0066 - val_mda: 0.5188 - val_rmse: 0.0088 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9101e-05 - mae: 0.0052 - mda: 0.5080 - rmse: 0.0764\n",
            "Epoch 7: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.9081e-05 - mae: 0.0052 - mda: 0.5080 - rmse: 0.0764 - val_loss: 1.4829e-04 - val_mae: 0.0112 - val_mda: 0.5189 - val_rmse: 0.0130 - learning_rate: 0.0050\n",
            "Epoch 8/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.7011e-05 - mae: 0.0049 - mda: 0.5072 - rmse: 0.0748\n",
            "Epoch 8: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.7027e-05 - mae: 0.0049 - mda: 0.5072 - rmse: 0.0749 - val_loss: 8.2477e-04 - val_mae: 0.0197 - val_mda: 0.5152 - val_rmse: 0.0210 - learning_rate: 0.0050\n",
            "Epoch 9/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9401e-05 - mae: 0.0050 - mda: 0.5077 - rmse: 0.0764\n",
            "Epoch 9: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.9398e-05 - mae: 0.0050 - mda: 0.5077 - rmse: 0.0764 - val_loss: 3.7029e-04 - val_mae: 0.0115 - val_mda: 0.5149 - val_rmse: 0.0134 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.3825e-05 - mae: 0.0054 - mda: 0.5087 - rmse: 0.0761\n",
            "Epoch 10: val_loss improved from 0.00005 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.3925e-05 - mae: 0.0054 - mda: 0.5087 - rmse: 0.0761 - val_loss: 4.2551e-05 - val_mae: 0.0046 - val_mda: 0.5208 - val_rmse: 0.0073 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.0548e-05 - mae: 0.0052 - mda: 0.5079 - rmse: 0.0757\n",
            "Epoch 11: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.0615e-05 - mae: 0.0052 - mda: 0.5079 - rmse: 0.0757 - val_loss: 1.5859e-04 - val_mae: 0.0107 - val_mda: 0.5175 - val_rmse: 0.0125 - learning_rate: 0.0050\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "\u001b[1m570/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4173e-05 - mae: 0.0048 - mda: 0.5082 - rmse: 0.0768\n",
            "Epoch 12: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.4119e-05 - mae: 0.0048 - mda: 0.5082 - rmse: 0.0768 - val_loss: 1.4807e-04 - val_mae: 0.0074 - val_mda: 0.5174 - val_rmse: 0.0098 - learning_rate: 0.0025\n",
            "Epoch 13/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3026e-05 - mae: 0.0047 - mda: 0.5084 - rmse: 0.0770\n",
            "Epoch 13: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.3008e-05 - mae: 0.0047 - mda: 0.5084 - rmse: 0.0770 - val_loss: 2.4219e-04 - val_mae: 0.0105 - val_mda: 0.5171 - val_rmse: 0.0125 - learning_rate: 0.0025\n",
            "Epoch 14/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0635e-05 - mae: 0.0046 - mda: 0.5081 - rmse: 0.0767\n",
            "Epoch 14: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.0687e-05 - mae: 0.0046 - mda: 0.5081 - rmse: 0.0767 - val_loss: 6.5398e-05 - val_mae: 0.0057 - val_mda: 0.5179 - val_rmse: 0.0084 - learning_rate: 0.0025\n",
            "Epoch 15/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1918e-05 - mae: 0.0047 - mda: 0.5071 - rmse: 0.0758\n",
            "Epoch 15: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.1913e-05 - mae: 0.0047 - mda: 0.5071 - rmse: 0.0758 - val_loss: 3.6321e-04 - val_mae: 0.0143 - val_mda: 0.5174 - val_rmse: 0.0159 - learning_rate: 0.0025\n",
            "Epoch 16/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.2401e-05 - mae: 0.0046 - mda: 0.5089 - rmse: 0.0758\n",
            "Epoch 16: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.2402e-05 - mae: 0.0046 - mda: 0.5089 - rmse: 0.0758 - val_loss: 2.5490e-04 - val_mae: 0.0110 - val_mda: 0.5186 - val_rmse: 0.0128 - learning_rate: 0.0025\n",
            "Epoch 17/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7462e-05 - mae: 0.0044 - mda: 0.5071 - rmse: 0.0767\n",
            "Epoch 17: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.7462e-05 - mae: 0.0044 - mda: 0.5070 - rmse: 0.0767 - val_loss: 1.1658e-04 - val_mae: 0.0071 - val_mda: 0.5177 - val_rmse: 0.0095 - learning_rate: 0.0012\n",
            "Epoch 18/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7614e-05 - mae: 0.0044 - mda: 0.5077 - rmse: 0.0759\n",
            "Epoch 18: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.7599e-05 - mae: 0.0044 - mda: 0.5077 - rmse: 0.0759 - val_loss: 5.9628e-04 - val_mae: 0.0181 - val_mda: 0.5173 - val_rmse: 0.0195 - learning_rate: 0.0012\n",
            "Epoch 19/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5821e-05 - mae: 0.0043 - mda: 0.5094 - rmse: 0.0770\n",
            "Epoch 19: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.5826e-05 - mae: 0.0043 - mda: 0.5094 - rmse: 0.0770 - val_loss: 9.1944e-05 - val_mae: 0.0061 - val_mda: 0.5173 - val_rmse: 0.0087 - learning_rate: 0.0012\n",
            "Epoch 20/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.6283e-05 - mae: 0.0043 - mda: 0.5100 - rmse: 0.0756\n",
            "Epoch 20: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.6323e-05 - mae: 0.0043 - mda: 0.5099 - rmse: 0.0756 - val_loss: 1.6781e-04 - val_mae: 0.0082 - val_mda: 0.5170 - val_rmse: 0.0106 - learning_rate: 0.0012\n",
            "Epoch 20: early stopping\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "Validation Loss: 0.00004, RMSE: 0.00731, MDA: 0.52835, MAE: 0.00463\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0012 - mae: 0.0149 - mda: 0.5059 - rmse: 0.0792\n",
            "Epoch 1: val_loss improved from inf to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0011 - mae: 0.0149 - mda: 0.5059 - rmse: 0.0791 - val_loss: 4.0250e-05 - val_mae: 0.0050 - val_mda: 0.5262 - val_rmse: 0.0073 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.2298e-05 - mae: 0.0054 - mda: 0.5068 - rmse: 0.0758\n",
            "Epoch 2: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.2268e-05 - mae: 0.0054 - mda: 0.5068 - rmse: 0.0758 - val_loss: 1.4480e-04 - val_mae: 0.0097 - val_mda: 0.5270 - val_rmse: 0.0114 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0262e-05 - mae: 0.0052 - mda: 0.5078 - rmse: 0.0761\n",
            "Epoch 3: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.0233e-05 - mae: 0.0052 - mda: 0.5078 - rmse: 0.0761 - val_loss: 1.5366e-04 - val_mae: 0.0101 - val_mda: 0.5264 - val_rmse: 0.0118 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8229e-05 - mae: 0.0046 - mda: 0.5091 - rmse: 0.0762\n",
            "Epoch 4: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.8243e-05 - mae: 0.0046 - mda: 0.5091 - rmse: 0.0762 - val_loss: 3.4100e-04 - val_mae: 0.0143 - val_mda: 0.5264 - val_rmse: 0.0157 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.5912e-05 - mae: 0.0049 - mda: 0.5072 - rmse: 0.0759\n",
            "Epoch 5: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.5925e-05 - mae: 0.0049 - mda: 0.5072 - rmse: 0.0759 - val_loss: 6.1557e-05 - val_mae: 0.0056 - val_mda: 0.5191 - val_rmse: 0.0079 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.7931e-05 - mae: 0.0061 - mda: 0.5062 - rmse: 0.0761\n",
            "Epoch 6: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 6.7856e-05 - mae: 0.0061 - mda: 0.5062 - rmse: 0.0761 - val_loss: 3.0788e-04 - val_mae: 0.0132 - val_mda: 0.5150 - val_rmse: 0.0148 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9855e-05 - mae: 0.0040 - mda: 0.5087 - rmse: 0.0767\n",
            "Epoch 7: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.9850e-05 - mae: 0.0040 - mda: 0.5087 - rmse: 0.0767 - val_loss: 1.8892e-04 - val_mae: 0.0093 - val_mda: 0.5156 - val_rmse: 0.0113 - learning_rate: 0.0050\n",
            "Epoch 8/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0439e-05 - mae: 0.0040 - mda: 0.5087 - rmse: 0.0768\n",
            "Epoch 8: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.0442e-05 - mae: 0.0040 - mda: 0.5087 - rmse: 0.0768 - val_loss: 5.7203e-05 - val_mae: 0.0052 - val_mda: 0.5148 - val_rmse: 0.0079 - learning_rate: 0.0050\n",
            "Epoch 9/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2668e-05 - mae: 0.0042 - mda: 0.5078 - rmse: 0.0760\n",
            "Epoch 9: val_loss improved from 0.00004 to 0.00004, saving model to best_model.keras\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.2684e-05 - mae: 0.0042 - mda: 0.5078 - rmse: 0.0760 - val_loss: 3.8557e-05 - val_mae: 0.0044 - val_mda: 0.5145 - val_rmse: 0.0071 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4340e-05 - mae: 0.0043 - mda: 0.5084 - rmse: 0.0768\n",
            "Epoch 10: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.4351e-05 - mae: 0.0043 - mda: 0.5084 - rmse: 0.0768 - val_loss: 2.7936e-04 - val_mae: 0.0132 - val_mda: 0.5162 - val_rmse: 0.0147 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.6431e-05 - mae: 0.0044 - mda: 0.5090 - rmse: 0.0765\n",
            "Epoch 11: val_loss improved from 0.00004 to 0.00003, saving model to best_model.keras\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.6467e-05 - mae: 0.0044 - mda: 0.5090 - rmse: 0.0765 - val_loss: 3.0359e-05 - val_mae: 0.0039 - val_mda: 0.5139 - val_rmse: 0.0069 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6677e-05 - mae: 0.0038 - mda: 0.5071 - rmse: 0.0767\n",
            "Epoch 12: val_loss improved from 0.00003 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.6669e-05 - mae: 0.0038 - mda: 0.5071 - rmse: 0.0767 - val_loss: 3.0245e-05 - val_mae: 0.0045 - val_mda: 0.5171 - val_rmse: 0.0075 - learning_rate: 0.0025\n",
            "Epoch 13/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7280e-05 - mae: 0.0038 - mda: 0.5081 - rmse: 0.0773\n",
            "Epoch 13: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.7281e-05 - mae: 0.0038 - mda: 0.5081 - rmse: 0.0773 - val_loss: 4.1339e-05 - val_mae: 0.0045 - val_mda: 0.5166 - val_rmse: 0.0075 - learning_rate: 0.0025\n",
            "Epoch 14/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9572e-05 - mae: 0.0040 - mda: 0.5062 - rmse: 0.0774\n",
            "Epoch 14: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.9558e-05 - mae: 0.0040 - mda: 0.5062 - rmse: 0.0774 - val_loss: 4.0197e-05 - val_mae: 0.0042 - val_mda: 0.5153 - val_rmse: 0.0074 - learning_rate: 0.0025\n",
            "Epoch 15/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9474e-05 - mae: 0.0039 - mda: 0.5078 - rmse: 0.0771\n",
            "Epoch 15: val_loss improved from 0.00003 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.9477e-05 - mae: 0.0039 - mda: 0.5078 - rmse: 0.0771 - val_loss: 2.1566e-05 - val_mae: 0.0032 - val_mda: 0.5158 - val_rmse: 0.0066 - learning_rate: 0.0025\n",
            "Epoch 16/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0707e-05 - mae: 0.0040 - mda: 0.5078 - rmse: 0.0766\n",
            "Epoch 16: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.0708e-05 - mae: 0.0040 - mda: 0.5078 - rmse: 0.0766 - val_loss: 5.0890e-05 - val_mae: 0.0046 - val_mda: 0.5136 - val_rmse: 0.0079 - learning_rate: 0.0025\n",
            "Epoch 17/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1840e-05 - mae: 0.0034 - mda: 0.5079 - rmse: 0.0769\n",
            "Epoch 17: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.1848e-05 - mae: 0.0034 - mda: 0.5079 - rmse: 0.0769 - val_loss: 2.1927e-05 - val_mae: 0.0032 - val_mda: 0.5135 - val_rmse: 0.0067 - learning_rate: 0.0012\n",
            "Epoch 18/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2610e-05 - mae: 0.0035 - mda: 0.5105 - rmse: 0.0764\n",
            "Epoch 18: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.2610e-05 - mae: 0.0035 - mda: 0.5105 - rmse: 0.0764 - val_loss: 2.3645e-05 - val_mae: 0.0035 - val_mda: 0.5149 - val_rmse: 0.0069 - learning_rate: 0.0012\n",
            "Epoch 19/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3459e-05 - mae: 0.0035 - mda: 0.5071 - rmse: 0.0772\n",
            "Epoch 19: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.3459e-05 - mae: 0.0035 - mda: 0.5071 - rmse: 0.0772 - val_loss: 2.9100e-05 - val_mae: 0.0036 - val_mda: 0.5134 - val_rmse: 0.0070 - learning_rate: 0.0012\n",
            "Epoch 20/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3384e-05 - mae: 0.0035 - mda: 0.5072 - rmse: 0.0764\n",
            "Epoch 20: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.3388e-05 - mae: 0.0035 - mda: 0.5072 - rmse: 0.0764 - val_loss: 2.3145e-05 - val_mae: 0.0032 - val_mda: 0.5156 - val_rmse: 0.0068 - learning_rate: 0.0012\n",
            "Epoch 21/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2046e-05 - mae: 0.0034 - mda: 0.5088 - rmse: 0.0767\n",
            "Epoch 21: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.2047e-05 - mae: 0.0034 - mda: 0.5088 - rmse: 0.0767 - val_loss: 4.3845e-05 - val_mae: 0.0045 - val_mda: 0.5160 - val_rmse: 0.0078 - learning_rate: 0.0012\n",
            "Epoch 22/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3220e-05 - mae: 0.0034 - mda: 0.5070 - rmse: 0.0768\n",
            "Epoch 22: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.3206e-05 - mae: 0.0034 - mda: 0.5070 - rmse: 0.0767 - val_loss: 8.9928e-05 - val_mae: 0.0070 - val_mda: 0.5146 - val_rmse: 0.0097 - learning_rate: 6.2500e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9784e-05 - mae: 0.0032 - mda: 0.5078 - rmse: 0.0762\n",
            "Epoch 23: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1.9790e-05 - mae: 0.0032 - mda: 0.5078 - rmse: 0.0762 - val_loss: 2.9360e-05 - val_mae: 0.0040 - val_mda: 0.5156 - val_rmse: 0.0073 - learning_rate: 6.2500e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0288e-05 - mae: 0.0032 - mda: 0.5089 - rmse: 0.0764\n",
            "Epoch 24: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.0303e-05 - mae: 0.0032 - mda: 0.5089 - rmse: 0.0764 - val_loss: 3.8425e-05 - val_mae: 0.0048 - val_mda: 0.5136 - val_rmse: 0.0079 - learning_rate: 6.2500e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2703e-05 - mae: 0.0034 - mda: 0.5087 - rmse: 0.0767\n",
            "Epoch 25: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.2696e-05 - mae: 0.0034 - mda: 0.5087 - rmse: 0.0767 - val_loss: 9.6827e-05 - val_mae: 0.0068 - val_mda: 0.5144 - val_rmse: 0.0098 - learning_rate: 6.2500e-04\n",
            "Epoch 25: early stopping\n",
            "Restoring model weights from the end of the best epoch: 15.\n",
            "Validation Loss: 0.00002, RMSE: 0.00663, MDA: 0.52696, MAE: 0.00318\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0070 - mae: 0.0273 - mda: 0.5058 - rmse: 0.0868\n",
            "Epoch 1: val_loss improved from inf to 0.00011, saving model to best_model.keras\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 0.0070 - mae: 0.0272 - mda: 0.5058 - rmse: 0.0868 - val_loss: 1.0695e-04 - val_mae: 0.0078 - val_mda: 0.5259 - val_rmse: 0.0097 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.7307e-05 - mae: 0.0050 - mda: 0.5066 - rmse: 0.0760\n",
            "Epoch 2: val_loss did not improve from 0.00011\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 4.7307e-05 - mae: 0.0050 - mda: 0.5066 - rmse: 0.0760 - val_loss: 2.3580e-04 - val_mae: 0.0134 - val_mda: 0.5268 - val_rmse: 0.0147 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.5796e-05 - mae: 0.0050 - mda: 0.5075 - rmse: 0.0762\n",
            "Epoch 3: val_loss improved from 0.00011 to 0.00010, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 4.5760e-05 - mae: 0.0050 - mda: 0.5075 - rmse: 0.0762 - val_loss: 9.8112e-05 - val_mae: 0.0066 - val_mda: 0.5267 - val_rmse: 0.0085 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.0830e-05 - mae: 0.0041 - mda: 0.5089 - rmse: 0.0771\n",
            "Epoch 4: val_loss improved from 0.00010 to 0.00006, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 3.0831e-05 - mae: 0.0041 - mda: 0.5089 - rmse: 0.0771 - val_loss: 6.2877e-05 - val_mae: 0.0068 - val_mda: 0.5269 - val_rmse: 0.0088 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2901e-05 - mae: 0.0043 - mda: 0.5096 - rmse: 0.0771\n",
            "Epoch 5: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 3.2898e-05 - mae: 0.0043 - mda: 0.5096 - rmse: 0.0771 - val_loss: 1.1119e-04 - val_mae: 0.0073 - val_mda: 0.5242 - val_rmse: 0.0094 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2117e-05 - mae: 0.0042 - mda: 0.5082 - rmse: 0.0755\n",
            "Epoch 6: val_loss improved from 0.00006 to 0.00006, saving model to best_model.keras\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 3.2115e-05 - mae: 0.0042 - mda: 0.5082 - rmse: 0.0755 - val_loss: 5.8793e-05 - val_mae: 0.0053 - val_mda: 0.5241 - val_rmse: 0.0076 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.1066e-05 - mae: 0.0034 - mda: 0.5086 - rmse: 0.0775\n",
            "Epoch 7: val_loss improved from 0.00006 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.1067e-05 - mae: 0.0034 - mda: 0.5086 - rmse: 0.0775 - val_loss: 2.2572e-05 - val_mae: 0.0036 - val_mda: 0.5251 - val_rmse: 0.0063 - learning_rate: 0.0050\n",
            "Epoch 8/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.5465e-05 - mae: 0.0037 - mda: 0.5093 - rmse: 0.0766\n",
            "Epoch 8: val_loss improved from 0.00002 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.5464e-05 - mae: 0.0037 - mda: 0.5093 - rmse: 0.0766 - val_loss: 1.7917e-05 - val_mae: 0.0032 - val_mda: 0.5218 - val_rmse: 0.0061 - learning_rate: 0.0050\n",
            "Epoch 9/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.5320e-05 - mae: 0.0037 - mda: 0.5089 - rmse: 0.0775\n",
            "Epoch 9: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.5312e-05 - mae: 0.0037 - mda: 0.5089 - rmse: 0.0775 - val_loss: 2.8936e-05 - val_mae: 0.0044 - val_mda: 0.5223 - val_rmse: 0.0072 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.9337e-05 - mae: 0.0040 - mda: 0.5088 - rmse: 0.0763\n",
            "Epoch 10: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.9322e-05 - mae: 0.0040 - mda: 0.5088 - rmse: 0.0763 - val_loss: 1.8491e-04 - val_mae: 0.0106 - val_mda: 0.5208 - val_rmse: 0.0124 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.7059e-05 - mae: 0.0046 - mda: 0.5105 - rmse: 0.0758\n",
            "Epoch 11: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 3.7032e-05 - mae: 0.0046 - mda: 0.5105 - rmse: 0.0758 - val_loss: 1.9337e-04 - val_mae: 0.0121 - val_mda: 0.5205 - val_rmse: 0.0137 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.9608e-05 - mae: 0.0032 - mda: 0.5076 - rmse: 0.0767\n",
            "Epoch 12: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.9610e-05 - mae: 0.0032 - mda: 0.5076 - rmse: 0.0767 - val_loss: 1.0053e-04 - val_mae: 0.0068 - val_mda: 0.5180 - val_rmse: 0.0092 - learning_rate: 0.0025\n",
            "Epoch 13/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.8854e-05 - mae: 0.0032 - mda: 0.5079 - rmse: 0.0762\n",
            "Epoch 13: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.8870e-05 - mae: 0.0032 - mda: 0.5079 - rmse: 0.0762 - val_loss: 1.8960e-05 - val_mae: 0.0034 - val_mda: 0.5188 - val_rmse: 0.0066 - learning_rate: 0.0025\n",
            "Epoch 14/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.2156e-05 - mae: 0.0035 - mda: 0.5077 - rmse: 0.0768\n",
            "Epoch 14: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.2168e-05 - mae: 0.0035 - mda: 0.5077 - rmse: 0.0768 - val_loss: 2.8282e-05 - val_mae: 0.0038 - val_mda: 0.5174 - val_rmse: 0.0069 - learning_rate: 0.0025\n",
            "Epoch 15/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.0763e-05 - mae: 0.0033 - mda: 0.5099 - rmse: 0.0771\n",
            "Epoch 15: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.0770e-05 - mae: 0.0033 - mda: 0.5099 - rmse: 0.0771 - val_loss: 4.9430e-05 - val_mae: 0.0062 - val_mda: 0.5171 - val_rmse: 0.0088 - learning_rate: 0.0025\n",
            "Epoch 16/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.2161e-05 - mae: 0.0034 - mda: 0.5076 - rmse: 0.0772\n",
            "Epoch 16: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.2176e-05 - mae: 0.0034 - mda: 0.5076 - rmse: 0.0772 - val_loss: 7.4726e-05 - val_mae: 0.0066 - val_mda: 0.5164 - val_rmse: 0.0091 - learning_rate: 0.0025\n",
            "Epoch 17/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.8409e-05 - mae: 0.0031 - mda: 0.5094 - rmse: 0.0762\n",
            "Epoch 17: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.8399e-05 - mae: 0.0031 - mda: 0.5094 - rmse: 0.0762 - val_loss: 2.5372e-04 - val_mae: 0.0128 - val_mda: 0.5176 - val_rmse: 0.0147 - learning_rate: 0.0012\n",
            "Epoch 18/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.9063e-05 - mae: 0.0031 - mda: 0.5111 - rmse: 0.0769\n",
            "Epoch 18: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.9061e-05 - mae: 0.0031 - mda: 0.5111 - rmse: 0.0769 - val_loss: 2.0515e-04 - val_mae: 0.0102 - val_mda: 0.5143 - val_rmse: 0.0123 - learning_rate: 0.0012\n",
            "Epoch 18: early stopping\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "Validation Loss: 0.00002, RMSE: 0.00608, MDA: 0.52689, MAE: 0.00320\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m570/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1345e-04 - mae: 0.0130 - mda: 0.5059 - rmse: 0.0763\n",
            "Epoch 1: val_loss improved from inf to 0.00008, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 4.0853e-04 - mae: 0.0129 - mda: 0.5059 - rmse: 0.0763 - val_loss: 8.4597e-05 - val_mae: 0.0074 - val_mda: 0.5250 - val_rmse: 0.0095 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0389e-04 - mae: 0.0074 - mda: 0.5061 - rmse: 0.0768\n",
            "Epoch 2: val_loss improved from 0.00008 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 1.0387e-04 - mae: 0.0074 - mda: 0.5062 - rmse: 0.0768 - val_loss: 3.7609e-05 - val_mae: 0.0047 - val_mda: 0.5254 - val_rmse: 0.0069 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m570/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.3489e-05 - mae: 0.0066 - mda: 0.5080 - rmse: 0.0763\n",
            "Epoch 3: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 8.3515e-05 - mae: 0.0066 - mda: 0.5080 - rmse: 0.0763 - val_loss: 4.1557e-05 - val_mae: 0.0052 - val_mda: 0.5261 - val_rmse: 0.0074 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0069e-05 - mae: 0.0066 - mda: 0.5077 - rmse: 0.0765\n",
            "Epoch 4: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 7.9999e-05 - mae: 0.0066 - mda: 0.5077 - rmse: 0.0765 - val_loss: 2.0901e-04 - val_mae: 0.0114 - val_mda: 0.5263 - val_rmse: 0.0129 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.5569e-05 - mae: 0.0068 - mda: 0.5079 - rmse: 0.0760\n",
            "Epoch 5: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 8.5434e-05 - mae: 0.0067 - mda: 0.5079 - rmse: 0.0760 - val_loss: 1.5594e-04 - val_mae: 0.0092 - val_mda: 0.5267 - val_rmse: 0.0108 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0698e-05 - mae: 0.0061 - mda: 0.5072 - rmse: 0.0764\n",
            "Epoch 6: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 7.0720e-05 - mae: 0.0061 - mda: 0.5072 - rmse: 0.0764 - val_loss: 4.9224e-05 - val_mae: 0.0050 - val_mda: 0.5270 - val_rmse: 0.0070 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.9267e-05 - mae: 0.0056 - mda: 0.5075 - rmse: 0.0754\n",
            "Epoch 7: val_loss improved from 0.00004 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.9400e-05 - mae: 0.0056 - mda: 0.5075 - rmse: 0.0754 - val_loss: 3.2953e-05 - val_mae: 0.0045 - val_mda: 0.5279 - val_rmse: 0.0066 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5640e-05 - mae: 0.0058 - mda: 0.5069 - rmse: 0.0766\n",
            "Epoch 8: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.5583e-05 - mae: 0.0058 - mda: 0.5069 - rmse: 0.0766 - val_loss: 5.3379e-05 - val_mae: 0.0052 - val_mda: 0.5276 - val_rmse: 0.0072 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4412e-05 - mae: 0.0057 - mda: 0.5088 - rmse: 0.0773\n",
            "Epoch 9: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.4452e-05 - mae: 0.0057 - mda: 0.5088 - rmse: 0.0773 - val_loss: 4.0873e-05 - val_mae: 0.0046 - val_mda: 0.5275 - val_rmse: 0.0067 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.1300e-05 - mae: 0.0057 - mda: 0.5071 - rmse: 0.0754\n",
            "Epoch 10: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.1304e-05 - mae: 0.0057 - mda: 0.5071 - rmse: 0.0754 - val_loss: 7.9120e-05 - val_mae: 0.0066 - val_mda: 0.5276 - val_rmse: 0.0085 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2429e-05 - mae: 0.0056 - mda: 0.5081 - rmse: 0.0764\n",
            "Epoch 11: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.2462e-05 - mae: 0.0056 - mda: 0.5081 - rmse: 0.0764 - val_loss: 4.7657e-05 - val_mae: 0.0050 - val_mda: 0.5272 - val_rmse: 0.0070 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.6113e-05 - mae: 0.0054 - mda: 0.5066 - rmse: 0.0764\n",
            "Epoch 12: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.6158e-05 - mae: 0.0054 - mda: 0.5066 - rmse: 0.0764 - val_loss: 4.3089e-05 - val_mae: 0.0048 - val_mda: 0.5274 - val_rmse: 0.0069 - learning_rate: 2.5000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.9968e-05 - mae: 0.0055 - mda: 0.5083 - rmse: 0.0762\n",
            "Epoch 13: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.0003e-05 - mae: 0.0055 - mda: 0.5083 - rmse: 0.0762 - val_loss: 1.5360e-04 - val_mae: 0.0097 - val_mda: 0.5275 - val_rmse: 0.0113 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m570/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.9151e-05 - mae: 0.0055 - mda: 0.5077 - rmse: 0.0761\n",
            "Epoch 14: val_loss improved from 0.00003 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.9109e-05 - mae: 0.0055 - mda: 0.5076 - rmse: 0.0761 - val_loss: 3.1014e-05 - val_mae: 0.0041 - val_mda: 0.5286 - val_rmse: 0.0062 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0252e-05 - mae: 0.0055 - mda: 0.5085 - rmse: 0.0762\n",
            "Epoch 15: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.0220e-05 - mae: 0.0055 - mda: 0.5085 - rmse: 0.0762 - val_loss: 3.6978e-05 - val_mae: 0.0044 - val_mda: 0.5280 - val_rmse: 0.0065 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.7890e-05 - mae: 0.0054 - mda: 0.5079 - rmse: 0.0778\n",
            "Epoch 16: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.7894e-05 - mae: 0.0054 - mda: 0.5079 - rmse: 0.0778 - val_loss: 3.6539e-05 - val_mae: 0.0047 - val_mda: 0.5276 - val_rmse: 0.0068 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.8213e-05 - mae: 0.0054 - mda: 0.5069 - rmse: 0.0767\n",
            "Epoch 17: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.8206e-05 - mae: 0.0054 - mda: 0.5069 - rmse: 0.0767 - val_loss: 8.1681e-05 - val_mae: 0.0069 - val_mda: 0.5282 - val_rmse: 0.0087 - learning_rate: 1.2500e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.3922e-05 - mae: 0.0052 - mda: 0.5061 - rmse: 0.0759\n",
            "Epoch 18: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.3939e-05 - mae: 0.0052 - mda: 0.5062 - rmse: 0.0759 - val_loss: 1.1741e-04 - val_mae: 0.0080 - val_mda: 0.5284 - val_rmse: 0.0097 - learning_rate: 1.2500e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.7246e-05 - mae: 0.0053 - mda: 0.5075 - rmse: 0.0766\n",
            "Epoch 19: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.7239e-05 - mae: 0.0053 - mda: 0.5075 - rmse: 0.0766 - val_loss: 4.8701e-05 - val_mae: 0.0050 - val_mda: 0.5284 - val_rmse: 0.0070 - learning_rate: 1.2500e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5767e-05 - mae: 0.0053 - mda: 0.5102 - rmse: 0.0759\n",
            "Epoch 20: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.5781e-05 - mae: 0.0053 - mda: 0.5102 - rmse: 0.0759 - val_loss: 4.5534e-05 - val_mae: 0.0048 - val_mda: 0.5283 - val_rmse: 0.0069 - learning_rate: 1.2500e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.6110e-05 - mae: 0.0053 - mda: 0.5084 - rmse: 0.0761\n",
            "Epoch 21: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.6112e-05 - mae: 0.0053 - mda: 0.5084 - rmse: 0.0761 - val_loss: 9.0378e-05 - val_mae: 0.0069 - val_mda: 0.5293 - val_rmse: 0.0087 - learning_rate: 1.2500e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.4416e-05 - mae: 0.0052 - mda: 0.5067 - rmse: 0.0767\n",
            "Epoch 22: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.4430e-05 - mae: 0.0052 - mda: 0.5067 - rmse: 0.0767 - val_loss: 7.1690e-05 - val_mae: 0.0061 - val_mda: 0.5293 - val_rmse: 0.0080 - learning_rate: 6.2500e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.3258e-05 - mae: 0.0052 - mda: 0.5068 - rmse: 0.0769\n",
            "Epoch 23: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.3284e-05 - mae: 0.0052 - mda: 0.5068 - rmse: 0.0769 - val_loss: 1.1910e-04 - val_mae: 0.0081 - val_mda: 0.5288 - val_rmse: 0.0098 - learning_rate: 6.2500e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m570/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.9327e-05 - mae: 0.0053 - mda: 0.5069 - rmse: 0.0763\n",
            "Epoch 24: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.9309e-05 - mae: 0.0053 - mda: 0.5069 - rmse: 0.0763 - val_loss: 5.6431e-05 - val_mae: 0.0053 - val_mda: 0.5290 - val_rmse: 0.0073 - learning_rate: 6.2500e-05\n",
            "Epoch 24: early stopping\n",
            "Restoring model weights from the end of the best epoch: 14.\n",
            "Validation Loss: 0.00003, RMSE: 0.00625, MDA: 0.52935, MAE: 0.00413\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9642e-04 - mae: 0.0126 - mda: 0.5064 - rmse: 0.0771\n",
            "Epoch 1: val_loss improved from inf to 0.00009, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 4.9256e-04 - mae: 0.0125 - mda: 0.5065 - rmse: 0.0771 - val_loss: 9.4632e-05 - val_mae: 0.0073 - val_mda: 0.5246 - val_rmse: 0.0093 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.6058e-05 - mae: 0.0058 - mda: 0.5078 - rmse: 0.0761\n",
            "Epoch 2: val_loss did not improve from 0.00009\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 6.6070e-05 - mae: 0.0058 - mda: 0.5078 - rmse: 0.0761 - val_loss: 1.5778e-04 - val_mae: 0.0102 - val_mda: 0.5239 - val_rmse: 0.0120 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.4354e-05 - mae: 0.0053 - mda: 0.5069 - rmse: 0.0754\n",
            "Epoch 3: val_loss improved from 0.00009 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.4376e-05 - mae: 0.0053 - mda: 0.5069 - rmse: 0.0754 - val_loss: 3.8354e-05 - val_mae: 0.0051 - val_mda: 0.5242 - val_rmse: 0.0075 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0464e-05 - mae: 0.0052 - mda: 0.5103 - rmse: 0.0777\n",
            "Epoch 4: val_loss improved from 0.00004 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.0497e-05 - mae: 0.0052 - mda: 0.5103 - rmse: 0.0777 - val_loss: 3.0332e-05 - val_mae: 0.0041 - val_mda: 0.5247 - val_rmse: 0.0066 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.5041e-05 - mae: 0.0055 - mda: 0.5067 - rmse: 0.0767\n",
            "Epoch 5: val_loss improved from 0.00003 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.4989e-05 - mae: 0.0055 - mda: 0.5067 - rmse: 0.0767 - val_loss: 2.0904e-05 - val_mae: 0.0035 - val_mda: 0.5248 - val_rmse: 0.0061 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.7617e-05 - mae: 0.0051 - mda: 0.5068 - rmse: 0.0757\n",
            "Epoch 6: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.7587e-05 - mae: 0.0051 - mda: 0.5068 - rmse: 0.0757 - val_loss: 2.4649e-05 - val_mae: 0.0038 - val_mda: 0.5252 - val_rmse: 0.0064 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.6895e-05 - mae: 0.0044 - mda: 0.5077 - rmse: 0.0767\n",
            "Epoch 7: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.6915e-05 - mae: 0.0044 - mda: 0.5077 - rmse: 0.0767 - val_loss: 3.3326e-05 - val_mae: 0.0043 - val_mda: 0.5252 - val_rmse: 0.0068 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9747e-05 - mae: 0.0045 - mda: 0.5098 - rmse: 0.0770\n",
            "Epoch 8: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.9741e-05 - mae: 0.0045 - mda: 0.5098 - rmse: 0.0770 - val_loss: 2.6186e-05 - val_mae: 0.0038 - val_mda: 0.5257 - val_rmse: 0.0063 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8292e-05 - mae: 0.0044 - mda: 0.5066 - rmse: 0.0773\n",
            "Epoch 9: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.8294e-05 - mae: 0.0044 - mda: 0.5066 - rmse: 0.0773 - val_loss: 2.2711e-05 - val_mae: 0.0036 - val_mda: 0.5258 - val_rmse: 0.0061 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9012e-05 - mae: 0.0045 - mda: 0.5084 - rmse: 0.0760\n",
            "Epoch 10: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.9011e-05 - mae: 0.0045 - mda: 0.5084 - rmse: 0.0760 - val_loss: 4.5560e-05 - val_mae: 0.0049 - val_mda: 0.5265 - val_rmse: 0.0073 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0153e-05 - mae: 0.0046 - mda: 0.5086 - rmse: 0.0771\n",
            "Epoch 11: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.0132e-05 - mae: 0.0046 - mda: 0.5086 - rmse: 0.0771 - val_loss: 2.9595e-05 - val_mae: 0.0041 - val_mda: 0.5268 - val_rmse: 0.0066 - learning_rate: 5.0000e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4002e-05 - mae: 0.0042 - mda: 0.5078 - rmse: 0.0768\n",
            "Epoch 12: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.4004e-05 - mae: 0.0042 - mda: 0.5078 - rmse: 0.0768 - val_loss: 3.0121e-05 - val_mae: 0.0040 - val_mda: 0.5265 - val_rmse: 0.0065 - learning_rate: 2.5000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3184e-05 - mae: 0.0042 - mda: 0.5081 - rmse: 0.0764\n",
            "Epoch 13: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.3186e-05 - mae: 0.0042 - mda: 0.5081 - rmse: 0.0764 - val_loss: 6.7441e-05 - val_mae: 0.0061 - val_mda: 0.5268 - val_rmse: 0.0082 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3437e-05 - mae: 0.0041 - mda: 0.5079 - rmse: 0.0766\n",
            "Epoch 14: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.3440e-05 - mae: 0.0041 - mda: 0.5079 - rmse: 0.0766 - val_loss: 7.8479e-05 - val_mae: 0.0067 - val_mda: 0.5260 - val_rmse: 0.0088 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5649e-05 - mae: 0.0043 - mda: 0.5076 - rmse: 0.0762\n",
            "Epoch 15: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.5616e-05 - mae: 0.0043 - mda: 0.5076 - rmse: 0.0762 - val_loss: 5.3306e-05 - val_mae: 0.0053 - val_mda: 0.5266 - val_rmse: 0.0075 - learning_rate: 2.5000e-04\n",
            "Epoch 15: early stopping\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "Validation Loss: 0.00002, RMSE: 0.00611, MDA: 0.52684, MAE: 0.00345\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.8493e-04 - mae: 0.0092 - mda: 0.5064 - rmse: 0.0777\n",
            "Epoch 1: val_loss improved from inf to 0.00007, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.8400e-04 - mae: 0.0091 - mda: 0.5064 - rmse: 0.0777 - val_loss: 7.4178e-05 - val_mae: 0.0069 - val_mda: 0.5240 - val_rmse: 0.0091 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.8148e-05 - mae: 0.0055 - mda: 0.5082 - rmse: 0.0768\n",
            "Epoch 2: val_loss did not improve from 0.00007\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 5.8105e-05 - mae: 0.0055 - mda: 0.5082 - rmse: 0.0768 - val_loss: 2.0000e-04 - val_mae: 0.0116 - val_mda: 0.5248 - val_rmse: 0.0133 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.3491e-05 - mae: 0.0049 - mda: 0.5085 - rmse: 0.0769\n",
            "Epoch 3: val_loss did not improve from 0.00007\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 4.3493e-05 - mae: 0.0049 - mda: 0.5085 - rmse: 0.0769 - val_loss: 1.1090e-04 - val_mae: 0.0082 - val_mda: 0.5231 - val_rmse: 0.0102 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.2191e-05 - mae: 0.0047 - mda: 0.5085 - rmse: 0.0758\n",
            "Epoch 4: val_loss improved from 0.00007 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 4.2183e-05 - mae: 0.0047 - mda: 0.5085 - rmse: 0.0758 - val_loss: 2.0570e-05 - val_mae: 0.0034 - val_mda: 0.5239 - val_rmse: 0.0063 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3236e-05 - mae: 0.0042 - mda: 0.5070 - rmse: 0.0765\n",
            "Epoch 5: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 3.3238e-05 - mae: 0.0042 - mda: 0.5070 - rmse: 0.0765 - val_loss: 2.4501e-04 - val_mae: 0.0121 - val_mda: 0.5251 - val_rmse: 0.0140 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.4080e-05 - mae: 0.0043 - mda: 0.5074 - rmse: 0.0763\n",
            "Epoch 6: val_loss improved from 0.00002 to 0.00002, saving model to best_model.keras\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 3.4078e-05 - mae: 0.0043 - mda: 0.5074 - rmse: 0.0763 - val_loss: 1.7482e-05 - val_mae: 0.0031 - val_mda: 0.5235 - val_rmse: 0.0061 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.5398e-05 - mae: 0.0036 - mda: 0.5081 - rmse: 0.0771\n",
            "Epoch 7: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.5395e-05 - mae: 0.0036 - mda: 0.5081 - rmse: 0.0771 - val_loss: 4.8435e-05 - val_mae: 0.0051 - val_mda: 0.5251 - val_rmse: 0.0076 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.5844e-05 - mae: 0.0036 - mda: 0.5076 - rmse: 0.0762\n",
            "Epoch 8: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.5837e-05 - mae: 0.0036 - mda: 0.5077 - rmse: 0.0762 - val_loss: 6.0743e-05 - val_mae: 0.0059 - val_mda: 0.5250 - val_rmse: 0.0082 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.3600e-05 - mae: 0.0035 - mda: 0.5065 - rmse: 0.0766\n",
            "Epoch 9: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.3606e-05 - mae: 0.0035 - mda: 0.5065 - rmse: 0.0766 - val_loss: 7.6991e-05 - val_mae: 0.0074 - val_mda: 0.5251 - val_rmse: 0.0096 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2531e-05 - mae: 0.0040 - mda: 0.5087 - rmse: 0.0768\n",
            "Epoch 10: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 3.2497e-05 - mae: 0.0040 - mda: 0.5087 - rmse: 0.0768 - val_loss: 1.8438e-05 - val_mae: 0.0032 - val_mda: 0.5236 - val_rmse: 0.0061 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.3213e-05 - mae: 0.0035 - mda: 0.5088 - rmse: 0.0757\n",
            "Epoch 11: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.3220e-05 - mae: 0.0035 - mda: 0.5088 - rmse: 0.0757 - val_loss: 1.1098e-04 - val_mae: 0.0087 - val_mda: 0.5247 - val_rmse: 0.0106 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.3513e-05 - mae: 0.0034 - mda: 0.5077 - rmse: 0.0763\n",
            "Epoch 12: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.3511e-05 - mae: 0.0034 - mda: 0.5077 - rmse: 0.0763 - val_loss: 3.2529e-05 - val_mae: 0.0047 - val_mda: 0.5246 - val_rmse: 0.0073 - learning_rate: 2.5000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.1578e-05 - mae: 0.0033 - mda: 0.5071 - rmse: 0.0781\n",
            "Epoch 13: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.1577e-05 - mae: 0.0033 - mda: 0.5071 - rmse: 0.0781 - val_loss: 3.0752e-05 - val_mae: 0.0041 - val_mda: 0.5248 - val_rmse: 0.0067 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.0107e-05 - mae: 0.0032 - mda: 0.5078 - rmse: 0.0771\n",
            "Epoch 14: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.0109e-05 - mae: 0.0032 - mda: 0.5078 - rmse: 0.0771 - val_loss: 1.7848e-05 - val_mae: 0.0032 - val_mda: 0.5247 - val_rmse: 0.0061 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.0337e-05 - mae: 0.0033 - mda: 0.5079 - rmse: 0.0762\n",
            "Epoch 15: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.0337e-05 - mae: 0.0033 - mda: 0.5079 - rmse: 0.0762 - val_loss: 3.5178e-05 - val_mae: 0.0042 - val_mda: 0.5242 - val_rmse: 0.0069 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.1715e-05 - mae: 0.0033 - mda: 0.5092 - rmse: 0.0759\n",
            "Epoch 16: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.1716e-05 - mae: 0.0033 - mda: 0.5092 - rmse: 0.0759 - val_loss: 1.0441e-04 - val_mae: 0.0077 - val_mda: 0.5239 - val_rmse: 0.0098 - learning_rate: 2.5000e-04\n",
            "Epoch 16: early stopping\n",
            "Restoring model weights from the end of the best epoch: 6.\n",
            "Validation Loss: 0.00002, RMSE: 0.00606, MDA: 0.52514, MAE: 0.00312\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0016 - mae: 0.0196 - mda: 0.5069 - rmse: 0.0812\n",
            "Epoch 1: val_loss improved from inf to 0.00022, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0016 - mae: 0.0195 - mda: 0.5069 - rmse: 0.0812 - val_loss: 2.2437e-04 - val_mae: 0.0133 - val_mda: 0.5266 - val_rmse: 0.0148 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0133e-04 - mae: 0.0074 - mda: 0.5063 - rmse: 0.0767\n",
            "Epoch 2: val_loss did not improve from 0.00022\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 1.0132e-04 - mae: 0.0074 - mda: 0.5063 - rmse: 0.0767 - val_loss: 2.4000e-04 - val_mae: 0.0103 - val_mda: 0.5279 - val_rmse: 0.0118 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1025e-05 - mae: 0.0070 - mda: 0.5067 - rmse: 0.0774\n",
            "Epoch 3: val_loss improved from 0.00022 to 0.00007, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 9.0952e-05 - mae: 0.0070 - mda: 0.5067 - rmse: 0.0773 - val_loss: 6.6995e-05 - val_mae: 0.0065 - val_mda: 0.5278 - val_rmse: 0.0086 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.3732e-05 - mae: 0.0067 - mda: 0.5068 - rmse: 0.0768\n",
            "Epoch 4: val_loss did not improve from 0.00007\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 8.3823e-05 - mae: 0.0067 - mda: 0.5069 - rmse: 0.0768 - val_loss: 1.1421e-04 - val_mae: 0.0070 - val_mda: 0.5281 - val_rmse: 0.0089 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.9698e-05 - mae: 0.0073 - mda: 0.5076 - rmse: 0.0766\n",
            "Epoch 5: val_loss did not improve from 0.00007\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 9.9550e-05 - mae: 0.0073 - mda: 0.5076 - rmse: 0.0766 - val_loss: 2.5896e-04 - val_mae: 0.0120 - val_mda: 0.5272 - val_rmse: 0.0134 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.5349e-05 - mae: 0.0067 - mda: 0.5075 - rmse: 0.0769\n",
            "Epoch 6: val_loss improved from 0.00007 to 0.00006, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 8.5360e-05 - mae: 0.0067 - mda: 0.5075 - rmse: 0.0769 - val_loss: 5.9450e-05 - val_mae: 0.0064 - val_mda: 0.5240 - val_rmse: 0.0085 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.4711e-05 - mae: 0.0072 - mda: 0.5063 - rmse: 0.0769\n",
            "Epoch 7: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 9.4848e-05 - mae: 0.0072 - mda: 0.5064 - rmse: 0.0769 - val_loss: 4.7849e-04 - val_mae: 0.0151 - val_mda: 0.5217 - val_rmse: 0.0164 - learning_rate: 0.0100\n",
            "Epoch 8/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.8131e-05 - mae: 0.0073 - mda: 0.5080 - rmse: 0.0763\n",
            "Epoch 8: val_loss did not improve from 0.00006\n",
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 9.8128e-05 - mae: 0.0073 - mda: 0.5080 - rmse: 0.0763 - val_loss: 1.7370e-04 - val_mae: 0.0085 - val_mda: 0.5194 - val_rmse: 0.0103 - learning_rate: 0.0100\n",
            "Epoch 9/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5828e-05 - mae: 0.0063 - mda: 0.5074 - rmse: 0.0770\n",
            "Epoch 9: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 7.5797e-05 - mae: 0.0063 - mda: 0.5074 - rmse: 0.0770 - val_loss: 3.2915e-04 - val_mae: 0.0117 - val_mda: 0.5161 - val_rmse: 0.0133 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3045e-05 - mae: 0.0061 - mda: 0.5071 - rmse: 0.0761\n",
            "Epoch 10: val_loss improved from 0.00006 to 0.00005, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 7.3126e-05 - mae: 0.0061 - mda: 0.5072 - rmse: 0.0762 - val_loss: 5.3907e-05 - val_mae: 0.0051 - val_mda: 0.5164 - val_rmse: 0.0076 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6242e-05 - mae: 0.0064 - mda: 0.5070 - rmse: 0.0763\n",
            "Epoch 11: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 7.6204e-05 - mae: 0.0064 - mda: 0.5070 - rmse: 0.0763 - val_loss: 1.2259e-04 - val_mae: 0.0075 - val_mda: 0.5147 - val_rmse: 0.0096 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.7665e-05 - mae: 0.0064 - mda: 0.5079 - rmse: 0.0758\n",
            "Epoch 12: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 7.7689e-05 - mae: 0.0064 - mda: 0.5078 - rmse: 0.0758 - val_loss: 3.8918e-04 - val_mae: 0.0128 - val_mda: 0.5171 - val_rmse: 0.0144 - learning_rate: 0.0050\n",
            "Epoch 13/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1394e-05 - mae: 0.0067 - mda: 0.5072 - rmse: 0.0762\n",
            "Epoch 13: val_loss did not improve from 0.00005\n",
            "\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 8.1339e-05 - mae: 0.0067 - mda: 0.5072 - rmse: 0.0762 - val_loss: 3.4431e-04 - val_mae: 0.0103 - val_mda: 0.5190 - val_rmse: 0.0124 - learning_rate: 0.0050\n",
            "Epoch 14/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3289e-05 - mae: 0.0058 - mda: 0.5072 - rmse: 0.0763\n",
            "Epoch 14: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.3311e-05 - mae: 0.0058 - mda: 0.5072 - rmse: 0.0763 - val_loss: 3.4039e-04 - val_mae: 0.0130 - val_mda: 0.5150 - val_rmse: 0.0147 - learning_rate: 0.0025\n",
            "Epoch 15/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m570/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4914e-05 - mae: 0.0058 - mda: 0.5069 - rmse: 0.0765\n",
            "Epoch 15: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.4877e-05 - mae: 0.0058 - mda: 0.5069 - rmse: 0.0765 - val_loss: 1.8259e-04 - val_mae: 0.0097 - val_mda: 0.5167 - val_rmse: 0.0116 - learning_rate: 0.0025\n",
            "Epoch 16/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7557e-05 - mae: 0.0060 - mda: 0.5066 - rmse: 0.0767\n",
            "Epoch 16: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.7548e-05 - mae: 0.0060 - mda: 0.5066 - rmse: 0.0767 - val_loss: 2.3413e-04 - val_mae: 0.0110 - val_mda: 0.5153 - val_rmse: 0.0129 - learning_rate: 0.0025\n",
            "Epoch 17/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5443e-05 - mae: 0.0059 - mda: 0.5074 - rmse: 0.0772\n",
            "Epoch 17: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.5413e-05 - mae: 0.0059 - mda: 0.5074 - rmse: 0.0772 - val_loss: 5.7463e-05 - val_mae: 0.0053 - val_mda: 0.5162 - val_rmse: 0.0078 - learning_rate: 0.0025\n",
            "Epoch 18/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8431e-05 - mae: 0.0060 - mda: 0.5081 - rmse: 0.0768\n",
            "Epoch 18: val_loss did not improve from 0.00005\n",
            "\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.8364e-05 - mae: 0.0060 - mda: 0.5081 - rmse: 0.0768 - val_loss: 6.9396e-04 - val_mae: 0.0205 - val_mda: 0.5151 - val_rmse: 0.0216 - learning_rate: 0.0025\n",
            "Epoch 19/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.1902e-05 - mae: 0.0056 - mda: 0.5078 - rmse: 0.0764\n",
            "Epoch 19: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.1824e-05 - mae: 0.0056 - mda: 0.5078 - rmse: 0.0764 - val_loss: 2.6334e-04 - val_mae: 0.0120 - val_mda: 0.5146 - val_rmse: 0.0137 - learning_rate: 0.0012\n",
            "Epoch 20/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3375e-05 - mae: 0.0057 - mda: 0.5078 - rmse: 0.0764\n",
            "Epoch 20: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.3283e-05 - mae: 0.0056 - mda: 0.5078 - rmse: 0.0764 - val_loss: 2.9909e-04 - val_mae: 0.0140 - val_mda: 0.5143 - val_rmse: 0.0155 - learning_rate: 0.0012\n",
            "Epoch 20: early stopping\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "Validation Loss: 0.00005, RMSE: 0.00760, MDA: 0.52810, MAE: 0.00510\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0013 - mae: 0.0174 - mda: 0.5027 - rmse: 0.0805\n",
            "Epoch 1: val_loss improved from inf to 0.00012, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0013 - mae: 0.0174 - mda: 0.5027 - rmse: 0.0805 - val_loss: 1.2246e-04 - val_mae: 0.0077 - val_mda: 0.5258 - val_rmse: 0.0095 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.6849e-05 - mae: 0.0060 - mda: 0.5076 - rmse: 0.0760\n",
            "Epoch 2: val_loss did not improve from 0.00012\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 6.6854e-05 - mae: 0.0060 - mda: 0.5076 - rmse: 0.0760 - val_loss: 1.7448e-04 - val_mae: 0.0093 - val_mda: 0.5266 - val_rmse: 0.0110 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.1083e-05 - mae: 0.0053 - mda: 0.5080 - rmse: 0.0763\n",
            "Epoch 3: val_loss did not improve from 0.00012\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.1129e-05 - mae: 0.0053 - mda: 0.5080 - rmse: 0.0763 - val_loss: 5.5564e-04 - val_mae: 0.0182 - val_mda: 0.5277 - val_rmse: 0.0194 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.9135e-05 - mae: 0.0065 - mda: 0.5073 - rmse: 0.0772\n",
            "Epoch 4: val_loss improved from 0.00012 to 0.00008, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 7.9077e-05 - mae: 0.0065 - mda: 0.5073 - rmse: 0.0772 - val_loss: 8.0830e-05 - val_mae: 0.0064 - val_mda: 0.5254 - val_rmse: 0.0084 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.5662e-05 - mae: 0.0060 - mda: 0.5065 - rmse: 0.0773\n",
            "Epoch 5: val_loss did not improve from 0.00008\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 6.5672e-05 - mae: 0.0060 - mda: 0.5065 - rmse: 0.0773 - val_loss: 1.3137e-04 - val_mae: 0.0076 - val_mda: 0.5226 - val_rmse: 0.0095 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.8721e-05 - mae: 0.0067 - mda: 0.5064 - rmse: 0.0771\n",
            "Epoch 6: val_loss did not improve from 0.00008\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 7.8642e-05 - mae: 0.0066 - mda: 0.5064 - rmse: 0.0771 - val_loss: 3.7884e-04 - val_mae: 0.0169 - val_mda: 0.5187 - val_rmse: 0.0182 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3336e-05 - mae: 0.0052 - mda: 0.5064 - rmse: 0.0766\n",
            "Epoch 7: val_loss improved from 0.00008 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.3297e-05 - mae: 0.0052 - mda: 0.5065 - rmse: 0.0766 - val_loss: 2.2434e-05 - val_mae: 0.0036 - val_mda: 0.5179 - val_rmse: 0.0065 - learning_rate: 0.0050\n",
            "Epoch 8/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.4320e-05 - mae: 0.0055 - mda: 0.5071 - rmse: 0.0766\n",
            "Epoch 8: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.4329e-05 - mae: 0.0055 - mda: 0.5071 - rmse: 0.0766 - val_loss: 7.7159e-04 - val_mae: 0.0221 - val_mda: 0.5171 - val_rmse: 0.0231 - learning_rate: 0.0050\n",
            "Epoch 9/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.2693e-05 - mae: 0.0054 - mda: 0.5065 - rmse: 0.0768\n",
            "Epoch 9: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.2719e-05 - mae: 0.0054 - mda: 0.5065 - rmse: 0.0768 - val_loss: 1.8930e-04 - val_mae: 0.0089 - val_mda: 0.5177 - val_rmse: 0.0109 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0202e-05 - mae: 0.0052 - mda: 0.5065 - rmse: 0.0757\n",
            "Epoch 10: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.0217e-05 - mae: 0.0052 - mda: 0.5065 - rmse: 0.0757 - val_loss: 4.3823e-05 - val_mae: 0.0055 - val_mda: 0.5161 - val_rmse: 0.0080 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.7578e-05 - mae: 0.0055 - mda: 0.5073 - rmse: 0.0766\n",
            "Epoch 11: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.7559e-05 - mae: 0.0055 - mda: 0.5073 - rmse: 0.0766 - val_loss: 4.0739e-04 - val_mae: 0.0148 - val_mda: 0.5149 - val_rmse: 0.0162 - learning_rate: 0.0050\n",
            "Epoch 12/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.6046e-05 - mae: 0.0055 - mda: 0.5067 - rmse: 0.0760\n",
            "Epoch 12: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.6039e-05 - mae: 0.0055 - mda: 0.5068 - rmse: 0.0760 - val_loss: 4.8067e-04 - val_mae: 0.0163 - val_mda: 0.5194 - val_rmse: 0.0177 - learning_rate: 0.0050\n",
            "Epoch 13/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2093e-05 - mae: 0.0047 - mda: 0.5073 - rmse: 0.0762\n",
            "Epoch 13: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.2079e-05 - mae: 0.0047 - mda: 0.5073 - rmse: 0.0762 - val_loss: 3.4230e-05 - val_mae: 0.0043 - val_mda: 0.5164 - val_rmse: 0.0071 - learning_rate: 0.0025\n",
            "Epoch 14/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.3233e-05 - mae: 0.0048 - mda: 0.5088 - rmse: 0.0771\n",
            "Epoch 14: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.3240e-05 - mae: 0.0048 - mda: 0.5088 - rmse: 0.0771 - val_loss: 1.7541e-04 - val_mae: 0.0113 - val_mda: 0.5165 - val_rmse: 0.0133 - learning_rate: 0.0025\n",
            "Epoch 15/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9864e-05 - mae: 0.0046 - mda: 0.5072 - rmse: 0.0754\n",
            "Epoch 15: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.9874e-05 - mae: 0.0046 - mda: 0.5072 - rmse: 0.0754 - val_loss: 2.9275e-05 - val_mae: 0.0040 - val_mda: 0.5183 - val_rmse: 0.0070 - learning_rate: 0.0025\n",
            "Epoch 16/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.3770e-05 - mae: 0.0048 - mda: 0.5080 - rmse: 0.0772\n",
            "Epoch 16: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.3762e-05 - mae: 0.0048 - mda: 0.5080 - rmse: 0.0772 - val_loss: 4.4877e-05 - val_mae: 0.0056 - val_mda: 0.5155 - val_rmse: 0.0083 - learning_rate: 0.0025\n",
            "Epoch 17/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.7191e-05 - mae: 0.0050 - mda: 0.5086 - rmse: 0.0764\n",
            "Epoch 17: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.7128e-05 - mae: 0.0050 - mda: 0.5086 - rmse: 0.0764 - val_loss: 6.5882e-05 - val_mae: 0.0066 - val_mda: 0.5168 - val_rmse: 0.0091 - learning_rate: 0.0025\n",
            "Epoch 17: early stopping\n",
            "Restoring model weights from the end of the best epoch: 7.\n",
            "Validation Loss: 0.00002, RMSE: 0.00649, MDA: 0.52774, MAE: 0.00365\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0103 - mae: 0.0318 - mda: 0.5073 - rmse: 0.0914\n",
            "Epoch 1: val_loss improved from inf to 0.00006, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - loss: 0.0103 - mae: 0.0317 - mda: 0.5073 - rmse: 0.0913 - val_loss: 6.1590e-05 - val_mae: 0.0056 - val_mda: 0.5250 - val_rmse: 0.0079 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 7.1657e-05 - mae: 0.0063 - mda: 0.5078 - rmse: 0.0773\n",
            "Epoch 2: val_loss improved from 0.00006 to 0.00005, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 7.1622e-05 - mae: 0.0063 - mda: 0.5078 - rmse: 0.0773 - val_loss: 5.2356e-05 - val_mae: 0.0056 - val_mda: 0.5259 - val_rmse: 0.0078 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.1486e-05 - mae: 0.0053 - mda: 0.5080 - rmse: 0.0762\n",
            "Epoch 3: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 5.1484e-05 - mae: 0.0053 - mda: 0.5080 - rmse: 0.0762 - val_loss: 2.4975e-04 - val_mae: 0.0124 - val_mda: 0.5258 - val_rmse: 0.0139 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.9596e-05 - mae: 0.0052 - mda: 0.5070 - rmse: 0.0767\n",
            "Epoch 4: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 4.9575e-05 - mae: 0.0052 - mda: 0.5070 - rmse: 0.0767 - val_loss: 8.7015e-05 - val_mae: 0.0081 - val_mda: 0.5260 - val_rmse: 0.0100 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8585e-05 - mae: 0.0046 - mda: 0.5076 - rmse: 0.0763\n",
            "Epoch 5: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 3.8596e-05 - mae: 0.0046 - mda: 0.5076 - rmse: 0.0763 - val_loss: 2.7589e-04 - val_mae: 0.0138 - val_mda: 0.5258 - val_rmse: 0.0152 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 4.0140e-05 - mae: 0.0047 - mda: 0.5093 - rmse: 0.0765\n",
            "Epoch 6: val_loss did not improve from 0.00005\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 4.0140e-05 - mae: 0.0047 - mda: 0.5092 - rmse: 0.0765 - val_loss: 1.0753e-04 - val_mae: 0.0091 - val_mda: 0.5271 - val_rmse: 0.0109 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1920e-05 - mae: 0.0041 - mda: 0.5081 - rmse: 0.0762\n",
            "Epoch 7: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 3.1919e-05 - mae: 0.0041 - mda: 0.5081 - rmse: 0.0762 - val_loss: 2.0731e-04 - val_mae: 0.0110 - val_mda: 0.5258 - val_rmse: 0.0126 - learning_rate: 0.0050\n",
            "Epoch 8/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.2471e-05 - mae: 0.0041 - mda: 0.5076 - rmse: 0.0763\n",
            "Epoch 8: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 3.2475e-05 - mae: 0.0041 - mda: 0.5076 - rmse: 0.0763 - val_loss: 6.8211e-05 - val_mae: 0.0063 - val_mda: 0.5269 - val_rmse: 0.0084 - learning_rate: 0.0050\n",
            "Epoch 9/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5933e-05 - mae: 0.0044 - mda: 0.5081 - rmse: 0.0760\n",
            "Epoch 9: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 3.5942e-05 - mae: 0.0044 - mda: 0.5081 - rmse: 0.0760 - val_loss: 1.7431e-04 - val_mae: 0.0092 - val_mda: 0.5273 - val_rmse: 0.0110 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.6771e-05 - mae: 0.0044 - mda: 0.5084 - rmse: 0.0769\n",
            "Epoch 10: val_loss improved from 0.00005 to 0.00005, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 3.6769e-05 - mae: 0.0044 - mda: 0.5084 - rmse: 0.0769 - val_loss: 4.9241e-05 - val_mae: 0.0059 - val_mda: 0.5261 - val_rmse: 0.0082 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5.2398e-05 - mae: 0.0055 - mda: 0.5093 - rmse: 0.0766\n",
            "Epoch 11: val_loss did not improve from 0.00005\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 5.2413e-05 - mae: 0.0055 - mda: 0.5093 - rmse: 0.0766 - val_loss: 7.6626e-05 - val_mae: 0.0067 - val_mda: 0.5242 - val_rmse: 0.0089 - learning_rate: 0.0050\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.8123e-05 - mae: 0.0039 - mda: 0.5083 - rmse: 0.0772\n",
            "Epoch 12: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 2.8123e-05 - mae: 0.0039 - mda: 0.5083 - rmse: 0.0772 - val_loss: 1.6468e-04 - val_mae: 0.0093 - val_mda: 0.5249 - val_rmse: 0.0112 - learning_rate: 0.0025\n",
            "Epoch 13/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.1597e-05 - mae: 0.0041 - mda: 0.5080 - rmse: 0.0766\n",
            "Epoch 13: val_loss improved from 0.00005 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 3.1600e-05 - mae: 0.0041 - mda: 0.5080 - rmse: 0.0766 - val_loss: 2.2200e-05 - val_mae: 0.0036 - val_mda: 0.5246 - val_rmse: 0.0064 - learning_rate: 0.0025\n",
            "Epoch 14/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.5746e-05 - mae: 0.0044 - mda: 0.5076 - rmse: 0.0765\n",
            "Epoch 14: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 3.5748e-05 - mae: 0.0044 - mda: 0.5076 - rmse: 0.0765 - val_loss: 9.2559e-05 - val_mae: 0.0073 - val_mda: 0.5229 - val_rmse: 0.0094 - learning_rate: 0.0025\n",
            "Epoch 15/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.9479e-05 - mae: 0.0040 - mda: 0.5060 - rmse: 0.0754\n",
            "Epoch 15: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.9498e-05 - mae: 0.0040 - mda: 0.5060 - rmse: 0.0754 - val_loss: 1.1600e-04 - val_mae: 0.0084 - val_mda: 0.5217 - val_rmse: 0.0103 - learning_rate: 0.0025\n",
            "Epoch 16/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.8988e-05 - mae: 0.0046 - mda: 0.5066 - rmse: 0.0766\n",
            "Epoch 16: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 3.8989e-05 - mae: 0.0046 - mda: 0.5066 - rmse: 0.0766 - val_loss: 3.1432e-04 - val_mae: 0.0140 - val_mda: 0.5206 - val_rmse: 0.0155 - learning_rate: 0.0025\n",
            "Epoch 17/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.7564e-05 - mae: 0.0038 - mda: 0.5093 - rmse: 0.0769\n",
            "Epoch 17: val_loss improved from 0.00002 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.7556e-05 - mae: 0.0038 - mda: 0.5093 - rmse: 0.0769 - val_loss: 2.1062e-05 - val_mae: 0.0035 - val_mda: 0.5195 - val_rmse: 0.0064 - learning_rate: 0.0012\n",
            "Epoch 18/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.6546e-05 - mae: 0.0037 - mda: 0.5082 - rmse: 0.0761\n",
            "Epoch 18: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 2.6550e-05 - mae: 0.0037 - mda: 0.5082 - rmse: 0.0761 - val_loss: 2.0791e-04 - val_mae: 0.0119 - val_mda: 0.5197 - val_rmse: 0.0135 - learning_rate: 0.0012\n",
            "Epoch 19/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.7711e-05 - mae: 0.0038 - mda: 0.5083 - rmse: 0.0765\n",
            "Epoch 19: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.7717e-05 - mae: 0.0038 - mda: 0.5083 - rmse: 0.0765 - val_loss: 1.3759e-04 - val_mae: 0.0087 - val_mda: 0.5187 - val_rmse: 0.0107 - learning_rate: 0.0012\n",
            "Epoch 20/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.5988e-05 - mae: 0.0037 - mda: 0.5100 - rmse: 0.0767\n",
            "Epoch 20: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 2.5998e-05 - mae: 0.0037 - mda: 0.5100 - rmse: 0.0767 - val_loss: 4.5059e-04 - val_mae: 0.0170 - val_mda: 0.5174 - val_rmse: 0.0183 - learning_rate: 0.0012\n",
            "Epoch 21/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.7763e-05 - mae: 0.0038 - mda: 0.5080 - rmse: 0.0761\n",
            "Epoch 21: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 2.7757e-05 - mae: 0.0038 - mda: 0.5080 - rmse: 0.0761 - val_loss: 5.6924e-05 - val_mae: 0.0053 - val_mda: 0.5184 - val_rmse: 0.0079 - learning_rate: 0.0012\n",
            "Epoch 22/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.3262e-05 - mae: 0.0034 - mda: 0.5101 - rmse: 0.0769\n",
            "Epoch 22: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - loss: 2.3258e-05 - mae: 0.0034 - mda: 0.5101 - rmse: 0.0769 - val_loss: 2.5063e-05 - val_mae: 0.0039 - val_mda: 0.5170 - val_rmse: 0.0069 - learning_rate: 6.2500e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.0532e-05 - mae: 0.0032 - mda: 0.5095 - rmse: 0.0760\n",
            "Epoch 23: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 2.0535e-05 - mae: 0.0032 - mda: 0.5095 - rmse: 0.0760 - val_loss: 1.7849e-04 - val_mae: 0.0101 - val_mda: 0.5151 - val_rmse: 0.0121 - learning_rate: 6.2500e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.1785e-05 - mae: 0.0033 - mda: 0.5069 - rmse: 0.0775\n",
            "Epoch 24: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 2.1784e-05 - mae: 0.0033 - mda: 0.5069 - rmse: 0.0775 - val_loss: 2.1949e-05 - val_mae: 0.0035 - val_mda: 0.5156 - val_rmse: 0.0067 - learning_rate: 6.2500e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.0759e-05 - mae: 0.0033 - mda: 0.5071 - rmse: 0.0765\n",
            "Epoch 25: val_loss improved from 0.00002 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 2.0761e-05 - mae: 0.0033 - mda: 0.5071 - rmse: 0.0765 - val_loss: 1.6191e-05 - val_mae: 0.0030 - val_mda: 0.5166 - val_rmse: 0.0063 - learning_rate: 6.2500e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.3939e-05 - mae: 0.0035 - mda: 0.5073 - rmse: 0.0763\n",
            "Epoch 26: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 2.3934e-05 - mae: 0.0035 - mda: 0.5073 - rmse: 0.0763 - val_loss: 3.3315e-05 - val_mae: 0.0045 - val_mda: 0.5164 - val_rmse: 0.0074 - learning_rate: 6.2500e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.9861e-05 - mae: 0.0032 - mda: 0.5088 - rmse: 0.0764\n",
            "Epoch 27: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.9862e-05 - mae: 0.0032 - mda: 0.5088 - rmse: 0.0764 - val_loss: 4.4793e-05 - val_mae: 0.0049 - val_mda: 0.5156 - val_rmse: 0.0078 - learning_rate: 3.1250e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.8867e-05 - mae: 0.0031 - mda: 0.5082 - rmse: 0.0763\n",
            "Epoch 28: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 1.8870e-05 - mae: 0.0031 - mda: 0.5082 - rmse: 0.0763 - val_loss: 2.2157e-05 - val_mae: 0.0037 - val_mda: 0.5144 - val_rmse: 0.0070 - learning_rate: 3.1250e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.9693e-05 - mae: 0.0031 - mda: 0.5094 - rmse: 0.0768\n",
            "Epoch 29: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.9693e-05 - mae: 0.0031 - mda: 0.5094 - rmse: 0.0768 - val_loss: 1.7515e-05 - val_mae: 0.0032 - val_mda: 0.5148 - val_rmse: 0.0066 - learning_rate: 3.1250e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.7964e-05 - mae: 0.0030 - mda: 0.5073 - rmse: 0.0764\n",
            "Epoch 30: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.7966e-05 - mae: 0.0030 - mda: 0.5073 - rmse: 0.0764 - val_loss: 9.7084e-05 - val_mae: 0.0078 - val_mda: 0.5147 - val_rmse: 0.0101 - learning_rate: 3.1250e-04\n",
            "Epoch 31/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.9323e-05 - mae: 0.0031 - mda: 0.5071 - rmse: 0.0763\n",
            "Epoch 31: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.9324e-05 - mae: 0.0031 - mda: 0.5071 - rmse: 0.0763 - val_loss: 2.5252e-05 - val_mae: 0.0037 - val_mda: 0.5145 - val_rmse: 0.0070 - learning_rate: 3.1250e-04\n",
            "Epoch 32/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.7783e-05 - mae: 0.0030 - mda: 0.5071 - rmse: 0.0771\n",
            "Epoch 32: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - loss: 1.7781e-05 - mae: 0.0030 - mda: 0.5071 - rmse: 0.0771 - val_loss: 3.7400e-05 - val_mae: 0.0043 - val_mda: 0.5139 - val_rmse: 0.0074 - learning_rate: 1.5625e-04\n",
            "Epoch 33/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.7276e-05 - mae: 0.0030 - mda: 0.5079 - rmse: 0.0771\n",
            "Epoch 33: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - loss: 1.7278e-05 - mae: 0.0030 - mda: 0.5079 - rmse: 0.0771 - val_loss: 8.6141e-05 - val_mae: 0.0065 - val_mda: 0.5135 - val_rmse: 0.0091 - learning_rate: 1.5625e-04\n",
            "Epoch 34/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.7336e-05 - mae: 0.0030 - mda: 0.5093 - rmse: 0.0769\n",
            "Epoch 34: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 1.7335e-05 - mae: 0.0030 - mda: 0.5093 - rmse: 0.0769 - val_loss: 1.7406e-05 - val_mae: 0.0031 - val_mda: 0.5135 - val_rmse: 0.0067 - learning_rate: 1.5625e-04\n",
            "Epoch 35/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.7603e-05 - mae: 0.0030 - mda: 0.5078 - rmse: 0.0765\n",
            "Epoch 35: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 1.7601e-05 - mae: 0.0030 - mda: 0.5078 - rmse: 0.0765 - val_loss: 1.6769e-05 - val_mae: 0.0029 - val_mda: 0.5132 - val_rmse: 0.0065 - learning_rate: 1.5625e-04\n",
            "Epoch 35: early stopping\n",
            "Restoring model weights from the end of the best epoch: 25.\n",
            "Validation Loss: 0.00002, RMSE: 0.00627, MDA: 0.52734, MAE: 0.00295\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.6857e-04 - mae: 0.0121 - mda: 0.5024 - rmse: 0.0773\n",
            "Epoch 1: val_loss improved from inf to 0.00005, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.6385e-04 - mae: 0.0120 - mda: 0.5024 - rmse: 0.0774 - val_loss: 5.0305e-05 - val_mae: 0.0052 - val_mda: 0.5130 - val_rmse: 0.0095 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.4129e-05 - mae: 0.0057 - mda: 0.5028 - rmse: 0.0788\n",
            "Epoch 2: val_loss improved from 0.00005 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 7.3978e-05 - mae: 0.0057 - mda: 0.5028 - rmse: 0.0788 - val_loss: 2.5958e-05 - val_mae: 0.0039 - val_mda: 0.5139 - val_rmse: 0.0087 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4697e-05 - mae: 0.0052 - mda: 0.5033 - rmse: 0.0782\n",
            "Epoch 3: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.4701e-05 - mae: 0.0052 - mda: 0.5033 - rmse: 0.0782 - val_loss: 1.6660e-04 - val_mae: 0.0106 - val_mda: 0.5130 - val_rmse: 0.0138 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8633e-05 - mae: 0.0050 - mda: 0.5039 - rmse: 0.0791\n",
            "Epoch 4: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.8605e-05 - mae: 0.0050 - mda: 0.5039 - rmse: 0.0791 - val_loss: 6.5410e-05 - val_mae: 0.0062 - val_mda: 0.5135 - val_rmse: 0.0103 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.1088e-05 - mae: 0.0051 - mda: 0.5023 - rmse: 0.0798\n",
            "Epoch 5: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.0983e-05 - mae: 0.0051 - mda: 0.5023 - rmse: 0.0798 - val_loss: 8.6758e-05 - val_mae: 0.0072 - val_mda: 0.5144 - val_rmse: 0.0110 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7562e-05 - mae: 0.0050 - mda: 0.5035 - rmse: 0.0787\n",
            "Epoch 6: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.7555e-05 - mae: 0.0050 - mda: 0.5035 - rmse: 0.0787 - val_loss: 5.0088e-05 - val_mae: 0.0052 - val_mda: 0.5143 - val_rmse: 0.0096 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2801e-05 - mae: 0.0046 - mda: 0.5029 - rmse: 0.0788\n",
            "Epoch 7: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.2792e-05 - mae: 0.0046 - mda: 0.5029 - rmse: 0.0788 - val_loss: 4.2385e-05 - val_mae: 0.0046 - val_mda: 0.5146 - val_rmse: 0.0092 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.3182e-05 - mae: 0.0047 - mda: 0.5040 - rmse: 0.0784\n",
            "Epoch 8: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.3181e-05 - mae: 0.0047 - mda: 0.5040 - rmse: 0.0784 - val_loss: 3.2227e-05 - val_mae: 0.0044 - val_mda: 0.5149 - val_rmse: 0.0091 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1228e-05 - mae: 0.0046 - mda: 0.5026 - rmse: 0.0782\n",
            "Epoch 9: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.1231e-05 - mae: 0.0046 - mda: 0.5026 - rmse: 0.0782 - val_loss: 3.7978e-05 - val_mae: 0.0044 - val_mda: 0.5141 - val_rmse: 0.0090 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.5287e-05 - mae: 0.0048 - mda: 0.5042 - rmse: 0.0793\n",
            "Epoch 10: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.5255e-05 - mae: 0.0048 - mda: 0.5042 - rmse: 0.0793 - val_loss: 7.1272e-05 - val_mae: 0.0062 - val_mda: 0.5144 - val_rmse: 0.0103 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.8902e-05 - mae: 0.0045 - mda: 0.5041 - rmse: 0.0778\n",
            "Epoch 11: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3.8911e-05 - mae: 0.0045 - mda: 0.5041 - rmse: 0.0778 - val_loss: 5.3743e-05 - val_mae: 0.0054 - val_mda: 0.5141 - val_rmse: 0.0096 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1668e-05 - mae: 0.0046 - mda: 0.5034 - rmse: 0.0790\n",
            "Epoch 12: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.1647e-05 - mae: 0.0046 - mda: 0.5034 - rmse: 0.0790 - val_loss: 3.8833e-05 - val_mae: 0.0044 - val_mda: 0.5145 - val_rmse: 0.0090 - learning_rate: 2.5000e-04\n",
            "Epoch 12: early stopping\n",
            "Restoring model weights from the end of the best epoch: 2.\n",
            "Validation Loss: 0.00003, RMSE: 0.00865, MDA: 0.51492, MAE: 0.00385\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.7685e-04 - mae: 0.0111 - mda: 0.5032 - rmse: 0.0768\n",
            "Epoch 1: val_loss improved from inf to 0.00007, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 3.7268e-04 - mae: 0.0110 - mda: 0.5032 - rmse: 0.0768 - val_loss: 7.3888e-05 - val_mae: 0.0066 - val_mda: 0.5133 - val_rmse: 0.0107 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.6504e-05 - mae: 0.0046 - mda: 0.5032 - rmse: 0.0777\n",
            "Epoch 2: val_loss improved from 0.00007 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.6496e-05 - mae: 0.0046 - mda: 0.5032 - rmse: 0.0777 - val_loss: 4.4003e-05 - val_mae: 0.0051 - val_mda: 0.5130 - val_rmse: 0.0096 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.6689e-05 - mae: 0.0042 - mda: 0.5042 - rmse: 0.0772\n",
            "Epoch 3: val_loss improved from 0.00004 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.6717e-05 - mae: 0.0042 - mda: 0.5042 - rmse: 0.0772 - val_loss: 2.9148e-05 - val_mae: 0.0040 - val_mda: 0.5124 - val_rmse: 0.0089 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.3558e-05 - mae: 0.0039 - mda: 0.5035 - rmse: 0.0784\n",
            "Epoch 4: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.3563e-05 - mae: 0.0039 - mda: 0.5035 - rmse: 0.0784 - val_loss: 5.3854e-05 - val_mae: 0.0057 - val_mda: 0.5128 - val_rmse: 0.0101 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.3183e-05 - mae: 0.0040 - mda: 0.5029 - rmse: 0.0776\n",
            "Epoch 5: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.3200e-05 - mae: 0.0040 - mda: 0.5029 - rmse: 0.0776 - val_loss: 5.3646e-05 - val_mae: 0.0054 - val_mda: 0.5117 - val_rmse: 0.0099 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.1634e-05 - mae: 0.0040 - mda: 0.5032 - rmse: 0.0780\n",
            "Epoch 6: val_loss improved from 0.00003 to 0.00002, saving model to best_model.keras\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.1651e-05 - mae: 0.0040 - mda: 0.5032 - rmse: 0.0780 - val_loss: 2.1047e-05 - val_mae: 0.0035 - val_mda: 0.5116 - val_rmse: 0.0088 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.5781e-05 - mae: 0.0036 - mda: 0.5039 - rmse: 0.0776\n",
            "Epoch 7: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.5791e-05 - mae: 0.0036 - mda: 0.5039 - rmse: 0.0776 - val_loss: 5.7897e-05 - val_mae: 0.0055 - val_mda: 0.5119 - val_rmse: 0.0100 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.5999e-05 - mae: 0.0037 - mda: 0.5036 - rmse: 0.0788\n",
            "Epoch 8: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.5995e-05 - mae: 0.0037 - mda: 0.5036 - rmse: 0.0788 - val_loss: 3.8408e-05 - val_mae: 0.0043 - val_mda: 0.5117 - val_rmse: 0.0093 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.5202e-05 - mae: 0.0035 - mda: 0.5037 - rmse: 0.0792\n",
            "Epoch 9: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.5208e-05 - mae: 0.0035 - mda: 0.5037 - rmse: 0.0791 - val_loss: 5.5292e-05 - val_mae: 0.0058 - val_mda: 0.5121 - val_rmse: 0.0102 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.7183e-05 - mae: 0.0038 - mda: 0.5037 - rmse: 0.0778\n",
            "Epoch 10: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.7173e-05 - mae: 0.0038 - mda: 0.5037 - rmse: 0.0778 - val_loss: 8.0944e-05 - val_mae: 0.0067 - val_mda: 0.5114 - val_rmse: 0.0109 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.4610e-05 - mae: 0.0036 - mda: 0.5025 - rmse: 0.0777\n",
            "Epoch 11: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.4618e-05 - mae: 0.0036 - mda: 0.5025 - rmse: 0.0777 - val_loss: 2.5863e-05 - val_mae: 0.0036 - val_mda: 0.5116 - val_rmse: 0.0088 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.3620e-05 - mae: 0.0034 - mda: 0.5030 - rmse: 0.0775\n",
            "Epoch 12: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.3619e-05 - mae: 0.0034 - mda: 0.5030 - rmse: 0.0775 - val_loss: 5.0117e-05 - val_mae: 0.0052 - val_mda: 0.5117 - val_rmse: 0.0098 - learning_rate: 2.5000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.3139e-05 - mae: 0.0034 - mda: 0.5041 - rmse: 0.0776\n",
            "Epoch 13: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.3131e-05 - mae: 0.0034 - mda: 0.5041 - rmse: 0.0776 - val_loss: 2.3885e-05 - val_mae: 0.0035 - val_mda: 0.5118 - val_rmse: 0.0088 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.4327e-05 - mae: 0.0035 - mda: 0.5032 - rmse: 0.0789\n",
            "Epoch 14: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.4320e-05 - mae: 0.0035 - mda: 0.5032 - rmse: 0.0789 - val_loss: 1.2207e-04 - val_mae: 0.0085 - val_mda: 0.5111 - val_rmse: 0.0123 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.3301e-05 - mae: 0.0034 - mda: 0.5036 - rmse: 0.0791\n",
            "Epoch 15: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.3288e-05 - mae: 0.0034 - mda: 0.5036 - rmse: 0.0791 - val_loss: 2.3892e-05 - val_mae: 0.0035 - val_mda: 0.5117 - val_rmse: 0.0087 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.3142e-05 - mae: 0.0034 - mda: 0.5041 - rmse: 0.0795\n",
            "Epoch 16: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.3134e-05 - mae: 0.0034 - mda: 0.5041 - rmse: 0.0795 - val_loss: 8.5540e-05 - val_mae: 0.0071 - val_mda: 0.5115 - val_rmse: 0.0112 - learning_rate: 2.5000e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: early stopping\n",
            "Restoring model weights from the end of the best epoch: 6.\n",
            "Validation Loss: 0.00002, RMSE: 0.00874, MDA: 0.51335, MAE: 0.00348\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.6265e-04 - mae: 0.0077 - mda: 0.5039 - rmse: 0.0792\n",
            "Epoch 1: val_loss improved from inf to 0.00005, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 1.6168e-04 - mae: 0.0077 - mda: 0.5039 - rmse: 0.0792 - val_loss: 5.3493e-05 - val_mae: 0.0062 - val_mda: 0.5124 - val_rmse: 0.0106 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.5198e-05 - mae: 0.0041 - mda: 0.5037 - rmse: 0.0784\n",
            "Epoch 2: val_loss improved from 0.00005 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 3.5197e-05 - mae: 0.0041 - mda: 0.5037 - rmse: 0.0784 - val_loss: 3.2081e-05 - val_mae: 0.0045 - val_mda: 0.5108 - val_rmse: 0.0095 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.2679e-05 - mae: 0.0040 - mda: 0.5037 - rmse: 0.0786\n",
            "Epoch 3: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 3.2667e-05 - mae: 0.0040 - mda: 0.5037 - rmse: 0.0786 - val_loss: 6.5494e-05 - val_mae: 0.0064 - val_mda: 0.5096 - val_rmse: 0.0108 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.4878e-05 - mae: 0.0035 - mda: 0.5043 - rmse: 0.0789\n",
            "Epoch 4: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.4879e-05 - mae: 0.0035 - mda: 0.5043 - rmse: 0.0789 - val_loss: 4.3794e-05 - val_mae: 0.0052 - val_mda: 0.5104 - val_rmse: 0.0102 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.2411e-05 - mae: 0.0034 - mda: 0.5038 - rmse: 0.0782\n",
            "Epoch 5: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 2.2420e-05 - mae: 0.0034 - mda: 0.5038 - rmse: 0.0782 - val_loss: 5.1921e-05 - val_mae: 0.0058 - val_mda: 0.5100 - val_rmse: 0.0106 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.1332e-05 - mae: 0.0033 - mda: 0.5035 - rmse: 0.0785\n",
            "Epoch 6: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 2.1333e-05 - mae: 0.0033 - mda: 0.5035 - rmse: 0.0785 - val_loss: 8.9398e-05 - val_mae: 0.0078 - val_mda: 0.5082 - val_rmse: 0.0120 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.9508e-05 - mae: 0.0030 - mda: 0.5026 - rmse: 0.0786\n",
            "Epoch 7: val_loss improved from 0.00003 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 1.9504e-05 - mae: 0.0030 - mda: 0.5026 - rmse: 0.0786 - val_loss: 1.6990e-05 - val_mae: 0.0031 - val_mda: 0.5092 - val_rmse: 0.0087 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.6948e-05 - mae: 0.0029 - mda: 0.5035 - rmse: 0.0781\n",
            "Epoch 8: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 1.6952e-05 - mae: 0.0029 - mda: 0.5035 - rmse: 0.0781 - val_loss: 6.0871e-05 - val_mae: 0.0063 - val_mda: 0.5095 - val_rmse: 0.0108 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.6611e-05 - mae: 0.0028 - mda: 0.5028 - rmse: 0.0786\n",
            "Epoch 9: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 1.6613e-05 - mae: 0.0028 - mda: 0.5028 - rmse: 0.0786 - val_loss: 3.0263e-05 - val_mae: 0.0041 - val_mda: 0.5097 - val_rmse: 0.0093 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.7331e-05 - mae: 0.0030 - mda: 0.5054 - rmse: 0.0784\n",
            "Epoch 10: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 1.7332e-05 - mae: 0.0030 - mda: 0.5054 - rmse: 0.0784 - val_loss: 3.3000e-05 - val_mae: 0.0046 - val_mda: 0.5096 - val_rmse: 0.0098 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.8616e-05 - mae: 0.0030 - mda: 0.5028 - rmse: 0.0787\n",
            "Epoch 11: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 1.8608e-05 - mae: 0.0030 - mda: 0.5028 - rmse: 0.0787 - val_loss: 1.7933e-05 - val_mae: 0.0032 - val_mda: 0.5091 - val_rmse: 0.0088 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.4810e-05 - mae: 0.0027 - mda: 0.5040 - rmse: 0.0780\n",
            "Epoch 12: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 1.4812e-05 - mae: 0.0027 - mda: 0.5040 - rmse: 0.0780 - val_loss: 1.7938e-05 - val_mae: 0.0031 - val_mda: 0.5098 - val_rmse: 0.0088 - learning_rate: 2.5000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.4360e-05 - mae: 0.0027 - mda: 0.5041 - rmse: 0.0787\n",
            "Epoch 13: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 1.4362e-05 - mae: 0.0027 - mda: 0.5041 - rmse: 0.0787 - val_loss: 6.3660e-05 - val_mae: 0.0064 - val_mda: 0.5098 - val_rmse: 0.0108 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.5295e-05 - mae: 0.0028 - mda: 0.5044 - rmse: 0.0783\n",
            "Epoch 14: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 1.5296e-05 - mae: 0.0028 - mda: 0.5044 - rmse: 0.0783 - val_loss: 1.7820e-05 - val_mae: 0.0031 - val_mda: 0.5097 - val_rmse: 0.0088 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.4441e-05 - mae: 0.0026 - mda: 0.5037 - rmse: 0.0785\n",
            "Epoch 15: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 1.4443e-05 - mae: 0.0026 - mda: 0.5037 - rmse: 0.0785 - val_loss: 4.2914e-05 - val_mae: 0.0050 - val_mda: 0.5098 - val_rmse: 0.0099 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.4347e-05 - mae: 0.0026 - mda: 0.5048 - rmse: 0.0787\n",
            "Epoch 16: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 1.4348e-05 - mae: 0.0026 - mda: 0.5048 - rmse: 0.0787 - val_loss: 6.2313e-05 - val_mae: 0.0065 - val_mda: 0.5097 - val_rmse: 0.0109 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.4779e-05 - mae: 0.0027 - mda: 0.5040 - rmse: 0.0778\n",
            "Epoch 17: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 1.4777e-05 - mae: 0.0027 - mda: 0.5040 - rmse: 0.0778 - val_loss: 1.9607e-05 - val_mae: 0.0032 - val_mda: 0.5099 - val_rmse: 0.0088 - learning_rate: 1.2500e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: early stopping\n",
            "Restoring model weights from the end of the best epoch: 7.\n",
            "Validation Loss: 0.00002, RMSE: 0.00874, MDA: 0.51238, MAE: 0.00306\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0015 - mae: 0.0203 - mda: 0.5022 - rmse: 0.0817\n",
            "Epoch 1: val_loss improved from inf to 0.00005, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0015 - mae: 0.0201 - mda: 0.5022 - rmse: 0.0816 - val_loss: 5.1118e-05 - val_mae: 0.0054 - val_mda: 0.5133 - val_rmse: 0.0097 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2225e-05 - mae: 0.0057 - mda: 0.5037 - rmse: 0.0780\n",
            "Epoch 2: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6.2198e-05 - mae: 0.0057 - mda: 0.5037 - rmse: 0.0780 - val_loss: 1.4943e-04 - val_mae: 0.0098 - val_mda: 0.5145 - val_rmse: 0.0131 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5805e-05 - mae: 0.0055 - mda: 0.5032 - rmse: 0.0791\n",
            "Epoch 3: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.5804e-05 - mae: 0.0055 - mda: 0.5032 - rmse: 0.0791 - val_loss: 3.4176e-04 - val_mae: 0.0147 - val_mda: 0.5146 - val_rmse: 0.0174 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5430e-05 - mae: 0.0055 - mda: 0.5042 - rmse: 0.0782\n",
            "Epoch 4: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.5580e-05 - mae: 0.0055 - mda: 0.5042 - rmse: 0.0782 - val_loss: 4.7662e-04 - val_mae: 0.0178 - val_mda: 0.5153 - val_rmse: 0.0202 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.0727e-05 - mae: 0.0057 - mda: 0.5031 - rmse: 0.0780\n",
            "Epoch 5: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6.0716e-05 - mae: 0.0057 - mda: 0.5031 - rmse: 0.0780 - val_loss: 5.1226e-05 - val_mae: 0.0053 - val_mda: 0.5151 - val_rmse: 0.0094 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8490e-05 - mae: 0.0050 - mda: 0.5033 - rmse: 0.0779\n",
            "Epoch 6: val_loss did not improve from 0.00005\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.8511e-05 - mae: 0.0050 - mda: 0.5033 - rmse: 0.0779 - val_loss: 1.0879e-04 - val_mae: 0.0070 - val_mda: 0.5147 - val_rmse: 0.0108 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.4409e-05 - mae: 0.0049 - mda: 0.5030 - rmse: 0.0779\n",
            "Epoch 7: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.4369e-05 - mae: 0.0049 - mda: 0.5031 - rmse: 0.0779 - val_loss: 9.0938e-05 - val_mae: 0.0062 - val_mda: 0.5148 - val_rmse: 0.0103 - learning_rate: 0.0050\n",
            "Epoch 8/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.4497e-05 - mae: 0.0049 - mda: 0.5034 - rmse: 0.0788\n",
            "Epoch 8: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.4515e-05 - mae: 0.0049 - mda: 0.5034 - rmse: 0.0787 - val_loss: 1.0322e-04 - val_mae: 0.0068 - val_mda: 0.5140 - val_rmse: 0.0107 - learning_rate: 0.0050\n",
            "Epoch 9/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1902e-05 - mae: 0.0046 - mda: 0.5031 - rmse: 0.0785\n",
            "Epoch 9: val_loss improved from 0.00005 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.1907e-05 - mae: 0.0046 - mda: 0.5031 - rmse: 0.0785 - val_loss: 2.9428e-05 - val_mae: 0.0041 - val_mda: 0.5146 - val_rmse: 0.0088 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2724e-05 - mae: 0.0047 - mda: 0.5047 - rmse: 0.0777\n",
            "Epoch 10: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.2740e-05 - mae: 0.0047 - mda: 0.5047 - rmse: 0.0777 - val_loss: 3.2573e-04 - val_mae: 0.0140 - val_mda: 0.5149 - val_rmse: 0.0168 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.0960e-05 - mae: 0.0047 - mda: 0.5033 - rmse: 0.0783\n",
            "Epoch 11: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.1017e-05 - mae: 0.0047 - mda: 0.5032 - rmse: 0.0784 - val_loss: 3.8053e-05 - val_mae: 0.0046 - val_mda: 0.5139 - val_rmse: 0.0091 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.9695e-05 - mae: 0.0046 - mda: 0.5035 - rmse: 0.0792\n",
            "Epoch 12: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3.9666e-05 - mae: 0.0046 - mda: 0.5035 - rmse: 0.0792 - val_loss: 8.1467e-05 - val_mae: 0.0063 - val_mda: 0.5139 - val_rmse: 0.0104 - learning_rate: 0.0025\n",
            "Epoch 13/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1659e-05 - mae: 0.0046 - mda: 0.5028 - rmse: 0.0785\n",
            "Epoch 13: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.1663e-05 - mae: 0.0046 - mda: 0.5028 - rmse: 0.0785 - val_loss: 2.3699e-04 - val_mae: 0.0121 - val_mda: 0.5140 - val_rmse: 0.0150 - learning_rate: 0.0025\n",
            "Epoch 14/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7663e-05 - mae: 0.0044 - mda: 0.5029 - rmse: 0.0786\n",
            "Epoch 14: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3.7701e-05 - mae: 0.0044 - mda: 0.5029 - rmse: 0.0786 - val_loss: 1.3268e-04 - val_mae: 0.0083 - val_mda: 0.5134 - val_rmse: 0.0120 - learning_rate: 0.0025\n",
            "Epoch 15/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1444e-05 - mae: 0.0046 - mda: 0.5030 - rmse: 0.0790\n",
            "Epoch 15: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.1440e-05 - mae: 0.0046 - mda: 0.5030 - rmse: 0.0790 - val_loss: 1.3783e-04 - val_mae: 0.0089 - val_mda: 0.5140 - val_rmse: 0.0124 - learning_rate: 0.0025\n",
            "Epoch 16/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.9787e-05 - mae: 0.0046 - mda: 0.5029 - rmse: 0.0788\n",
            "Epoch 16: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3.9777e-05 - mae: 0.0046 - mda: 0.5029 - rmse: 0.0788 - val_loss: 1.3004e-04 - val_mae: 0.0080 - val_mda: 0.5122 - val_rmse: 0.0118 - learning_rate: 0.0025\n",
            "Epoch 17/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.5949e-05 - mae: 0.0043 - mda: 0.5037 - rmse: 0.0782\n",
            "Epoch 17: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3.5964e-05 - mae: 0.0043 - mda: 0.5037 - rmse: 0.0782 - val_loss: 6.2826e-05 - val_mae: 0.0055 - val_mda: 0.5121 - val_rmse: 0.0099 - learning_rate: 0.0012\n",
            "Epoch 18/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7184e-05 - mae: 0.0043 - mda: 0.5036 - rmse: 0.0785\n",
            "Epoch 18: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3.7207e-05 - mae: 0.0043 - mda: 0.5036 - rmse: 0.0785 - val_loss: 5.7244e-05 - val_mae: 0.0054 - val_mda: 0.5118 - val_rmse: 0.0098 - learning_rate: 0.0012\n",
            "Epoch 19/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7080e-05 - mae: 0.0043 - mda: 0.5031 - rmse: 0.0783\n",
            "Epoch 19: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3.7087e-05 - mae: 0.0043 - mda: 0.5031 - rmse: 0.0783 - val_loss: 1.3227e-04 - val_mae: 0.0091 - val_mda: 0.5114 - val_rmse: 0.0126 - learning_rate: 0.0012\n",
            "Epoch 19: early stopping\n",
            "Restoring model weights from the end of the best epoch: 9.\n",
            "Validation Loss: 0.00003, RMSE: 0.00883, MDA: 0.51533, MAE: 0.00407\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0051 - mae: 0.0310 - mda: 0.5019 - rmse: 0.0882\n",
            "Epoch 1: val_loss improved from inf to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0050 - mae: 0.0308 - mda: 0.5019 - rmse: 0.0881 - val_loss: 3.6927e-05 - val_mae: 0.0044 - val_mda: 0.5143 - val_rmse: 0.0091 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.7625e-05 - mae: 0.0055 - mda: 0.5025 - rmse: 0.0791\n",
            "Epoch 2: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 5.7539e-05 - mae: 0.0055 - mda: 0.5025 - rmse: 0.0791 - val_loss: 3.7720e-05 - val_mae: 0.0048 - val_mda: 0.5140 - val_rmse: 0.0093 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.4080e-05 - mae: 0.0048 - mda: 0.5038 - rmse: 0.0789\n",
            "Epoch 3: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.4029e-05 - mae: 0.0048 - mda: 0.5037 - rmse: 0.0789 - val_loss: 3.8422e-05 - val_mae: 0.0047 - val_mda: 0.5153 - val_rmse: 0.0091 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.5705e-05 - mae: 0.0043 - mda: 0.5043 - rmse: 0.0779\n",
            "Epoch 4: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.5679e-05 - mae: 0.0043 - mda: 0.5043 - rmse: 0.0780 - val_loss: 2.1594e-04 - val_mae: 0.0112 - val_mda: 0.5153 - val_rmse: 0.0143 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.8401e-05 - mae: 0.0045 - mda: 0.5033 - rmse: 0.0781\n",
            "Epoch 5: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.8381e-05 - mae: 0.0045 - mda: 0.5033 - rmse: 0.0781 - val_loss: 1.6080e-04 - val_mae: 0.0086 - val_mda: 0.5153 - val_rmse: 0.0122 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.8850e-05 - mae: 0.0045 - mda: 0.5035 - rmse: 0.0781\n",
            "Epoch 6: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.8805e-05 - mae: 0.0045 - mda: 0.5035 - rmse: 0.0781 - val_loss: 5.2220e-04 - val_mae: 0.0188 - val_mda: 0.5154 - val_rmse: 0.0211 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.1659e-05 - mae: 0.0040 - mda: 0.5042 - rmse: 0.0773\n",
            "Epoch 7: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.1619e-05 - mae: 0.0040 - mda: 0.5042 - rmse: 0.0773 - val_loss: 8.4265e-05 - val_mae: 0.0059 - val_mda: 0.5151 - val_rmse: 0.0102 - learning_rate: 0.0050\n",
            "Epoch 8/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.5824e-05 - mae: 0.0037 - mda: 0.5037 - rmse: 0.0793\n",
            "Epoch 8: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.5823e-05 - mae: 0.0037 - mda: 0.5037 - rmse: 0.0792 - val_loss: 4.8632e-05 - val_mae: 0.0055 - val_mda: 0.5154 - val_rmse: 0.0098 - learning_rate: 0.0050\n",
            "Epoch 9/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.9469e-05 - mae: 0.0039 - mda: 0.5029 - rmse: 0.0787\n",
            "Epoch 9: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.9446e-05 - mae: 0.0039 - mda: 0.5029 - rmse: 0.0787 - val_loss: 4.4201e-05 - val_mae: 0.0050 - val_mda: 0.5151 - val_rmse: 0.0095 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.0492e-05 - mae: 0.0040 - mda: 0.5038 - rmse: 0.0783\n",
            "Epoch 10: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.0473e-05 - mae: 0.0040 - mda: 0.5038 - rmse: 0.0783 - val_loss: 1.3022e-04 - val_mae: 0.0073 - val_mda: 0.5134 - val_rmse: 0.0112 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.7643e-05 - mae: 0.0038 - mda: 0.5031 - rmse: 0.0768\n",
            "Epoch 11: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.7683e-05 - mae: 0.0038 - mda: 0.5031 - rmse: 0.0769 - val_loss: 7.0770e-05 - val_mae: 0.0058 - val_mda: 0.5137 - val_rmse: 0.0101 - learning_rate: 0.0050\n",
            "Epoch 11: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "Validation Loss: 0.00004, RMSE: 0.00911, MDA: 0.51545, MAE: 0.00438\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0121 - mae: 0.0404 - mda: 0.5020 - rmse: 0.0964\n",
            "Epoch 1: val_loss improved from inf to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - loss: 0.0120 - mae: 0.0402 - mda: 0.5020 - rmse: 0.0963 - val_loss: 2.9234e-05 - val_mae: 0.0042 - val_mda: 0.5128 - val_rmse: 0.0086 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 5.7360e-05 - mae: 0.0055 - mda: 0.5024 - rmse: 0.0780\n",
            "Epoch 2: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 5.7340e-05 - mae: 0.0055 - mda: 0.5024 - rmse: 0.0780 - val_loss: 2.9792e-05 - val_mae: 0.0042 - val_mda: 0.5149 - val_rmse: 0.0087 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.8471e-05 - mae: 0.0044 - mda: 0.5040 - rmse: 0.0782\n",
            "Epoch 3: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 3.8459e-05 - mae: 0.0044 - mda: 0.5040 - rmse: 0.0782 - val_loss: 3.1376e-05 - val_mae: 0.0042 - val_mda: 0.5146 - val_rmse: 0.0087 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.6258e-05 - mae: 0.0044 - mda: 0.5045 - rmse: 0.0785\n",
            "Epoch 4: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 3.6244e-05 - mae: 0.0044 - mda: 0.5045 - rmse: 0.0785 - val_loss: 4.0224e-05 - val_mae: 0.0046 - val_mda: 0.5147 - val_rmse: 0.0090 - learning_rate: 0.0100\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.1529e-05 - mae: 0.0041 - mda: 0.5033 - rmse: 0.0787\n",
            "Epoch 5: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 3.1524e-05 - mae: 0.0041 - mda: 0.5033 - rmse: 0.0787 - val_loss: 3.8166e-04 - val_mae: 0.0157 - val_mda: 0.5139 - val_rmse: 0.0184 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.6828e-05 - mae: 0.0045 - mda: 0.5034 - rmse: 0.0791\n",
            "Epoch 6: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 3.6808e-05 - mae: 0.0045 - mda: 0.5034 - rmse: 0.0791 - val_loss: 5.2272e-05 - val_mae: 0.0056 - val_mda: 0.5145 - val_rmse: 0.0098 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.8877e-05 - mae: 0.0038 - mda: 0.5039 - rmse: 0.0788\n",
            "Epoch 7: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 2.8864e-05 - mae: 0.0038 - mda: 0.5039 - rmse: 0.0788 - val_loss: 3.6006e-05 - val_mae: 0.0043 - val_mda: 0.5137 - val_rmse: 0.0089 - learning_rate: 0.0050\n",
            "Epoch 8/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.3996e-05 - mae: 0.0035 - mda: 0.5037 - rmse: 0.0781\n",
            "Epoch 8: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 2.3992e-05 - mae: 0.0035 - mda: 0.5037 - rmse: 0.0781 - val_loss: 7.4111e-05 - val_mae: 0.0060 - val_mda: 0.5136 - val_rmse: 0.0102 - learning_rate: 0.0050\n",
            "Epoch 9/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.3214e-05 - mae: 0.0034 - mda: 0.5051 - rmse: 0.0788\n",
            "Epoch 9: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 2.3208e-05 - mae: 0.0034 - mda: 0.5051 - rmse: 0.0788 - val_loss: 2.0223e-04 - val_mae: 0.0110 - val_mda: 0.5130 - val_rmse: 0.0142 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.2211e-05 - mae: 0.0034 - mda: 0.5042 - rmse: 0.0784\n",
            "Epoch 10: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 2.2212e-05 - mae: 0.0034 - mda: 0.5042 - rmse: 0.0784 - val_loss: 3.3228e-05 - val_mae: 0.0041 - val_mda: 0.5133 - val_rmse: 0.0088 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.3306e-05 - mae: 0.0035 - mda: 0.5051 - rmse: 0.0781\n",
            "Epoch 11: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 2.3308e-05 - mae: 0.0035 - mda: 0.5051 - rmse: 0.0781 - val_loss: 4.4404e-05 - val_mae: 0.0046 - val_mda: 0.5126 - val_rmse: 0.0093 - learning_rate: 0.0050\n",
            "Epoch 11: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "Validation Loss: 0.00003, RMSE: 0.00865, MDA: 0.51486, MAE: 0.00410\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.3986e-04 - mae: 0.0133 - mda: 0.5029 - rmse: 0.0779\n",
            "Epoch 1: val_loss improved from inf to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.3681e-04 - mae: 0.0132 - mda: 0.5029 - rmse: 0.0779 - val_loss: 2.8614e-05 - val_mae: 0.0040 - val_mda: 0.5138 - val_rmse: 0.0089 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0095e-04 - mae: 0.0068 - mda: 0.5031 - rmse: 0.0790\n",
            "Epoch 2: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.0085e-04 - mae: 0.0068 - mda: 0.5031 - rmse: 0.0790 - val_loss: 7.3320e-05 - val_mae: 0.0062 - val_mda: 0.5134 - val_rmse: 0.0104 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.1141e-05 - mae: 0.0065 - mda: 0.5033 - rmse: 0.0781\n",
            "Epoch 3: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 8.1134e-05 - mae: 0.0065 - mda: 0.5033 - rmse: 0.0781 - val_loss: 1.4583e-04 - val_mae: 0.0100 - val_mda: 0.5138 - val_rmse: 0.0133 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.2221e-05 - mae: 0.0065 - mda: 0.5037 - rmse: 0.0774\n",
            "Epoch 4: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 8.2188e-05 - mae: 0.0065 - mda: 0.5037 - rmse: 0.0774 - val_loss: 3.7189e-05 - val_mae: 0.0045 - val_mda: 0.5148 - val_rmse: 0.0091 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.4276e-05 - mae: 0.0062 - mda: 0.5042 - rmse: 0.0784\n",
            "Epoch 5: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 7.4286e-05 - mae: 0.0062 - mda: 0.5042 - rmse: 0.0784 - val_loss: 2.4994e-04 - val_mae: 0.0139 - val_mda: 0.5141 - val_rmse: 0.0166 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.0723e-05 - mae: 0.0066 - mda: 0.5030 - rmse: 0.0780\n",
            "Epoch 6: val_loss improved from 0.00003 to 0.00003, saving model to best_model.keras\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 8.0652e-05 - mae: 0.0066 - mda: 0.5029 - rmse: 0.0780 - val_loss: 2.8187e-05 - val_mae: 0.0041 - val_mda: 0.5149 - val_rmse: 0.0088 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.8338e-05 - mae: 0.0058 - mda: 0.5031 - rmse: 0.0784\n",
            "Epoch 7: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6.8278e-05 - mae: 0.0058 - mda: 0.5031 - rmse: 0.0784 - val_loss: 1.0179e-04 - val_mae: 0.0077 - val_mda: 0.5147 - val_rmse: 0.0114 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.3623e-05 - mae: 0.0056 - mda: 0.5033 - rmse: 0.0780\n",
            "Epoch 8: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6.3603e-05 - mae: 0.0056 - mda: 0.5033 - rmse: 0.0780 - val_loss: 2.9751e-05 - val_mae: 0.0041 - val_mda: 0.5150 - val_rmse: 0.0088 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.8389e-05 - mae: 0.0055 - mda: 0.5039 - rmse: 0.0773\n",
            "Epoch 9: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.8452e-05 - mae: 0.0055 - mda: 0.5039 - rmse: 0.0773 - val_loss: 9.1621e-05 - val_mae: 0.0082 - val_mda: 0.5153 - val_rmse: 0.0119 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6545e-05 - mae: 0.0058 - mda: 0.5029 - rmse: 0.0784\n",
            "Epoch 10: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6.6491e-05 - mae: 0.0058 - mda: 0.5029 - rmse: 0.0784 - val_loss: 4.2099e-05 - val_mae: 0.0049 - val_mda: 0.5154 - val_rmse: 0.0093 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2478e-05 - mae: 0.0057 - mda: 0.5034 - rmse: 0.0784\n",
            "Epoch 11: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6.2452e-05 - mae: 0.0057 - mda: 0.5034 - rmse: 0.0784 - val_loss: 1.1639e-04 - val_mae: 0.0081 - val_mda: 0.5162 - val_rmse: 0.0117 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6654e-05 - mae: 0.0054 - mda: 0.5041 - rmse: 0.0770\n",
            "Epoch 12: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.6691e-05 - mae: 0.0054 - mda: 0.5041 - rmse: 0.0770 - val_loss: 8.6337e-05 - val_mae: 0.0068 - val_mda: 0.5157 - val_rmse: 0.0106 - learning_rate: 2.5000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5875e-05 - mae: 0.0054 - mda: 0.5030 - rmse: 0.0778\n",
            "Epoch 13: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.5928e-05 - mae: 0.0054 - mda: 0.5030 - rmse: 0.0778 - val_loss: 1.0531e-04 - val_mae: 0.0075 - val_mda: 0.5157 - val_rmse: 0.0111 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4013e-05 - mae: 0.0056 - mda: 0.5021 - rmse: 0.0784\n",
            "Epoch 14: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6.3957e-05 - mae: 0.0056 - mda: 0.5021 - rmse: 0.0784 - val_loss: 6.5927e-05 - val_mae: 0.0059 - val_mda: 0.5159 - val_rmse: 0.0099 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.1445e-05 - mae: 0.0056 - mda: 0.5038 - rmse: 0.0793\n",
            "Epoch 15: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6.1443e-05 - mae: 0.0056 - mda: 0.5038 - rmse: 0.0793 - val_loss: 3.1879e-05 - val_mae: 0.0041 - val_mda: 0.5157 - val_rmse: 0.0086 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5275e-05 - mae: 0.0053 - mda: 0.5036 - rmse: 0.0791\n",
            "Epoch 16: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.5317e-05 - mae: 0.0053 - mda: 0.5036 - rmse: 0.0791 - val_loss: 7.6005e-05 - val_mae: 0.0064 - val_mda: 0.5162 - val_rmse: 0.0103 - learning_rate: 2.5000e-04\n",
            "Epoch 16: early stopping\n",
            "Restoring model weights from the end of the best epoch: 6.\n",
            "Validation Loss: 0.00003, RMSE: 0.00861, MDA: 0.51619, MAE: 0.00401\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.5635e-04 - mae: 0.0135 - mda: 0.5018 - rmse: 0.0783\n",
            "Epoch 1: val_loss improved from inf to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 5.5154e-04 - mae: 0.0135 - mda: 0.5018 - rmse: 0.0783 - val_loss: 3.1192e-05 - val_mae: 0.0042 - val_mda: 0.5129 - val_rmse: 0.0091 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.6427e-05 - mae: 0.0061 - mda: 0.5036 - rmse: 0.0779\n",
            "Epoch 2: val_loss improved from 0.00003 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 7.6346e-05 - mae: 0.0061 - mda: 0.5036 - rmse: 0.0779 - val_loss: 2.1346e-05 - val_mae: 0.0035 - val_mda: 0.5131 - val_rmse: 0.0086 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.7070e-05 - mae: 0.0053 - mda: 0.5036 - rmse: 0.0779\n",
            "Epoch 3: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 5.7029e-05 - mae: 0.0053 - mda: 0.5036 - rmse: 0.0779 - val_loss: 2.3206e-05 - val_mae: 0.0037 - val_mda: 0.5136 - val_rmse: 0.0088 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.0499e-05 - mae: 0.0051 - mda: 0.5031 - rmse: 0.0797\n",
            "Epoch 4: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 5.0473e-05 - mae: 0.0051 - mda: 0.5031 - rmse: 0.0796 - val_loss: 7.2839e-05 - val_mae: 0.0070 - val_mda: 0.5127 - val_rmse: 0.0111 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.0487e-05 - mae: 0.0051 - mda: 0.5039 - rmse: 0.0783\n",
            "Epoch 5: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 5.0485e-05 - mae: 0.0051 - mda: 0.5039 - rmse: 0.0783 - val_loss: 2.0191e-04 - val_mae: 0.0127 - val_mda: 0.5131 - val_rmse: 0.0156 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.0429e-05 - mae: 0.0051 - mda: 0.5035 - rmse: 0.0790\n",
            "Epoch 6: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 5.0333e-05 - mae: 0.0051 - mda: 0.5035 - rmse: 0.0790 - val_loss: 2.4892e-05 - val_mae: 0.0038 - val_mda: 0.5128 - val_rmse: 0.0088 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.0932e-05 - mae: 0.0046 - mda: 0.5027 - rmse: 0.0787\n",
            "Epoch 7: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.0909e-05 - mae: 0.0046 - mda: 0.5027 - rmse: 0.0787 - val_loss: 2.7126e-05 - val_mae: 0.0039 - val_mda: 0.5125 - val_rmse: 0.0088 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.9236e-05 - mae: 0.0045 - mda: 0.5027 - rmse: 0.0781\n",
            "Epoch 8: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.9227e-05 - mae: 0.0045 - mda: 0.5027 - rmse: 0.0781 - val_loss: 2.5136e-05 - val_mae: 0.0037 - val_mda: 0.5129 - val_rmse: 0.0087 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.8550e-05 - mae: 0.0044 - mda: 0.5022 - rmse: 0.0783\n",
            "Epoch 9: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.8544e-05 - mae: 0.0044 - mda: 0.5022 - rmse: 0.0783 - val_loss: 8.2212e-05 - val_mae: 0.0072 - val_mda: 0.5122 - val_rmse: 0.0112 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.7478e-05 - mae: 0.0044 - mda: 0.5025 - rmse: 0.0775\n",
            "Epoch 10: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.7490e-05 - mae: 0.0044 - mda: 0.5025 - rmse: 0.0775 - val_loss: 3.2767e-05 - val_mae: 0.0041 - val_mda: 0.5132 - val_rmse: 0.0090 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.2396e-05 - mae: 0.0047 - mda: 0.5034 - rmse: 0.0786\n",
            "Epoch 11: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.2356e-05 - mae: 0.0047 - mda: 0.5034 - rmse: 0.0786 - val_loss: 4.5638e-05 - val_mae: 0.0050 - val_mda: 0.5128 - val_rmse: 0.0095 - learning_rate: 5.0000e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.5466e-05 - mae: 0.0043 - mda: 0.5029 - rmse: 0.0786\n",
            "Epoch 12: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 3.5467e-05 - mae: 0.0043 - mda: 0.5029 - rmse: 0.0786 - val_loss: 3.1193e-05 - val_mae: 0.0042 - val_mda: 0.5130 - val_rmse: 0.0090 - learning_rate: 2.5000e-04\n",
            "Epoch 12: early stopping\n",
            "Restoring model weights from the end of the best epoch: 2.\n",
            "Validation Loss: 0.00002, RMSE: 0.00864, MDA: 0.51365, MAE: 0.00346\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.0503e-04 - mae: 0.0099 - mda: 0.5027 - rmse: 0.0779\n",
            "Epoch 1: val_loss improved from inf to 0.00005, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 3.0372e-04 - mae: 0.0099 - mda: 0.5027 - rmse: 0.0779 - val_loss: 4.9669e-05 - val_mae: 0.0053 - val_mda: 0.5113 - val_rmse: 0.0098 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.3104e-05 - mae: 0.0050 - mda: 0.5038 - rmse: 0.0790\n",
            "Epoch 2: val_loss improved from 0.00005 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 5.3087e-05 - mae: 0.0050 - mda: 0.5038 - rmse: 0.0790 - val_loss: 3.3716e-05 - val_mae: 0.0044 - val_mda: 0.5109 - val_rmse: 0.0093 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.6691e-05 - mae: 0.0043 - mda: 0.5037 - rmse: 0.0772\n",
            "Epoch 3: val_loss improved from 0.00003 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 3.6706e-05 - mae: 0.0043 - mda: 0.5037 - rmse: 0.0772 - val_loss: 2.4877e-05 - val_mae: 0.0039 - val_mda: 0.5108 - val_rmse: 0.0091 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.2257e-05 - mae: 0.0047 - mda: 0.5032 - rmse: 0.0781\n",
            "Epoch 4: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 4.2257e-05 - mae: 0.0047 - mda: 0.5032 - rmse: 0.0781 - val_loss: 9.0417e-05 - val_mae: 0.0071 - val_mda: 0.5112 - val_rmse: 0.0113 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.5579e-05 - mae: 0.0043 - mda: 0.5026 - rmse: 0.0782\n",
            "Epoch 5: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 3.5583e-05 - mae: 0.0043 - mda: 0.5026 - rmse: 0.0782 - val_loss: 3.3360e-05 - val_mae: 0.0047 - val_mda: 0.5106 - val_rmse: 0.0097 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.2391e-05 - mae: 0.0041 - mda: 0.5034 - rmse: 0.0787\n",
            "Epoch 6: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 3.2392e-05 - mae: 0.0041 - mda: 0.5034 - rmse: 0.0787 - val_loss: 3.9891e-05 - val_mae: 0.0048 - val_mda: 0.5104 - val_rmse: 0.0097 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.5635e-05 - mae: 0.0036 - mda: 0.5031 - rmse: 0.0791\n",
            "Epoch 7: val_loss improved from 0.00002 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.5621e-05 - mae: 0.0036 - mda: 0.5031 - rmse: 0.0791 - val_loss: 1.7855e-05 - val_mae: 0.0031 - val_mda: 0.5102 - val_rmse: 0.0087 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.7523e-05 - mae: 0.0037 - mda: 0.5029 - rmse: 0.0787\n",
            "Epoch 8: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.7516e-05 - mae: 0.0037 - mda: 0.5029 - rmse: 0.0787 - val_loss: 2.7564e-05 - val_mae: 0.0040 - val_mda: 0.5104 - val_rmse: 0.0091 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.3609e-05 - mae: 0.0035 - mda: 0.5035 - rmse: 0.0785\n",
            "Epoch 9: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 2.3621e-05 - mae: 0.0035 - mda: 0.5035 - rmse: 0.0785 - val_loss: 5.5558e-05 - val_mae: 0.0054 - val_mda: 0.5108 - val_rmse: 0.0100 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.4798e-05 - mae: 0.0036 - mda: 0.5036 - rmse: 0.0785\n",
            "Epoch 10: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.4800e-05 - mae: 0.0036 - mda: 0.5036 - rmse: 0.0785 - val_loss: 2.5646e-05 - val_mae: 0.0037 - val_mda: 0.5111 - val_rmse: 0.0090 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.4116e-05 - mae: 0.0035 - mda: 0.5034 - rmse: 0.0784\n",
            "Epoch 11: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.4122e-05 - mae: 0.0035 - mda: 0.5034 - rmse: 0.0784 - val_loss: 1.4188e-04 - val_mae: 0.0098 - val_mda: 0.5115 - val_rmse: 0.0134 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.2411e-05 - mae: 0.0034 - mda: 0.5039 - rmse: 0.0795\n",
            "Epoch 12: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 2.2405e-05 - mae: 0.0034 - mda: 0.5039 - rmse: 0.0795 - val_loss: 2.1592e-05 - val_mae: 0.0034 - val_mda: 0.5112 - val_rmse: 0.0088 - learning_rate: 2.5000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.2320e-05 - mae: 0.0033 - mda: 0.5036 - rmse: 0.0787\n",
            "Epoch 13: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.2319e-05 - mae: 0.0033 - mda: 0.5036 - rmse: 0.0787 - val_loss: 5.1441e-05 - val_mae: 0.0055 - val_mda: 0.5116 - val_rmse: 0.0101 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.0926e-05 - mae: 0.0033 - mda: 0.5031 - rmse: 0.0783\n",
            "Epoch 14: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.0920e-05 - mae: 0.0033 - mda: 0.5031 - rmse: 0.0783 - val_loss: 2.7557e-05 - val_mae: 0.0039 - val_mda: 0.5117 - val_rmse: 0.0090 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.0739e-05 - mae: 0.0032 - mda: 0.5036 - rmse: 0.0786\n",
            "Epoch 15: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 2.0737e-05 - mae: 0.0032 - mda: 0.5036 - rmse: 0.0786 - val_loss: 1.9711e-05 - val_mae: 0.0033 - val_mda: 0.5111 - val_rmse: 0.0087 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.0199e-05 - mae: 0.0032 - mda: 0.5041 - rmse: 0.0783\n",
            "Epoch 16: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.0208e-05 - mae: 0.0032 - mda: 0.5041 - rmse: 0.0783 - val_loss: 4.4967e-05 - val_mae: 0.0055 - val_mda: 0.5108 - val_rmse: 0.0102 - learning_rate: 2.5000e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.9803e-05 - mae: 0.0032 - mda: 0.5034 - rmse: 0.0778\n",
            "Epoch 17: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.9803e-05 - mae: 0.0032 - mda: 0.5034 - rmse: 0.0778 - val_loss: 2.4555e-05 - val_mae: 0.0036 - val_mda: 0.5117 - val_rmse: 0.0089 - learning_rate: 1.2500e-04\n",
            "Epoch 17: early stopping\n",
            "Restoring model weights from the end of the best epoch: 7.\n",
            "Validation Loss: 0.00002, RMSE: 0.00868, MDA: 0.51172, MAE: 0.00315\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0029 - mae: 0.0277 - mda: 0.5021 - rmse: 0.0852\n",
            "Epoch 1: val_loss improved from inf to 0.00009, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0029 - mae: 0.0274 - mda: 0.5021 - rmse: 0.0851 - val_loss: 8.6385e-05 - val_mae: 0.0074 - val_mda: 0.5145 - val_rmse: 0.0111 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0435e-04 - mae: 0.0075 - mda: 0.5028 - rmse: 0.0780\n",
            "Epoch 2: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.0424e-04 - mae: 0.0075 - mda: 0.5028 - rmse: 0.0780 - val_loss: 1.7029e-04 - val_mae: 0.0097 - val_mda: 0.5167 - val_rmse: 0.0128 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.5431e-05 - mae: 0.0067 - mda: 0.5037 - rmse: 0.0777\n",
            "Epoch 3: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 8.5377e-05 - mae: 0.0067 - mda: 0.5037 - rmse: 0.0777 - val_loss: 2.6955e-04 - val_mae: 0.0117 - val_mda: 0.5180 - val_rmse: 0.0146 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.7442e-05 - mae: 0.0064 - mda: 0.5032 - rmse: 0.0772\n",
            "Epoch 4: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 7.7454e-05 - mae: 0.0064 - mda: 0.5032 - rmse: 0.0772 - val_loss: 1.0966e-04 - val_mae: 0.0079 - val_mda: 0.5185 - val_rmse: 0.0112 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.1250e-05 - mae: 0.0066 - mda: 0.5022 - rmse: 0.0787\n",
            "Epoch 5: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 8.1196e-05 - mae: 0.0066 - mda: 0.5022 - rmse: 0.0787 - val_loss: 1.1683e-04 - val_mae: 0.0081 - val_mda: 0.5178 - val_rmse: 0.0114 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.7416e-05 - mae: 0.0064 - mda: 0.5020 - rmse: 0.0793\n",
            "Epoch 6: val_loss did not improve from 0.00009\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 7.7321e-05 - mae: 0.0064 - mda: 0.5020 - rmse: 0.0793 - val_loss: 3.3604e-04 - val_mae: 0.0140 - val_mda: 0.5180 - val_rmse: 0.0166 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4870e-05 - mae: 0.0057 - mda: 0.5025 - rmse: 0.0786\n",
            "Epoch 7: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6.4847e-05 - mae: 0.0057 - mda: 0.5025 - rmse: 0.0786 - val_loss: 1.5716e-04 - val_mae: 0.0092 - val_mda: 0.5183 - val_rmse: 0.0124 - learning_rate: 0.0050\n",
            "Epoch 8/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4929e-05 - mae: 0.0058 - mda: 0.5027 - rmse: 0.0786\n",
            "Epoch 8: val_loss improved from 0.00009 to 0.00007, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6.4939e-05 - mae: 0.0058 - mda: 0.5027 - rmse: 0.0786 - val_loss: 7.2826e-05 - val_mae: 0.0059 - val_mda: 0.5172 - val_rmse: 0.0097 - learning_rate: 0.0050\n",
            "Epoch 9/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4275e-05 - mae: 0.0057 - mda: 0.5035 - rmse: 0.0786\n",
            "Epoch 9: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6.4292e-05 - mae: 0.0057 - mda: 0.5035 - rmse: 0.0786 - val_loss: 1.8294e-04 - val_mae: 0.0099 - val_mda: 0.5165 - val_rmse: 0.0131 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.9343e-05 - mae: 0.0061 - mda: 0.5034 - rmse: 0.0774\n",
            "Epoch 10: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6.9331e-05 - mae: 0.0061 - mda: 0.5034 - rmse: 0.0774 - val_loss: 4.0535e-04 - val_mae: 0.0163 - val_mda: 0.5173 - val_rmse: 0.0187 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1254e-05 - mae: 0.0062 - mda: 0.5027 - rmse: 0.0777\n",
            "Epoch 11: val_loss improved from 0.00007 to 0.00006, saving model to best_model.keras\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 7.1245e-05 - mae: 0.0062 - mda: 0.5027 - rmse: 0.0777 - val_loss: 5.7060e-05 - val_mae: 0.0062 - val_mda: 0.5167 - val_rmse: 0.0102 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9794e-05 - mae: 0.0055 - mda: 0.5034 - rmse: 0.0783\n",
            "Epoch 12: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.9804e-05 - mae: 0.0055 - mda: 0.5034 - rmse: 0.0783 - val_loss: 3.0100e-04 - val_mae: 0.0128 - val_mda: 0.5173 - val_rmse: 0.0156 - learning_rate: 0.0025\n",
            "Epoch 13/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.0165e-05 - mae: 0.0056 - mda: 0.5027 - rmse: 0.0774\n",
            "Epoch 13: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6.0212e-05 - mae: 0.0056 - mda: 0.5027 - rmse: 0.0774 - val_loss: 4.6388e-04 - val_mae: 0.0172 - val_mda: 0.5170 - val_rmse: 0.0196 - learning_rate: 0.0025\n",
            "Epoch 14/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.3059e-05 - mae: 0.0057 - mda: 0.5031 - rmse: 0.0780\n",
            "Epoch 14: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6.2978e-05 - mae: 0.0057 - mda: 0.5031 - rmse: 0.0780 - val_loss: 1.1350e-04 - val_mae: 0.0092 - val_mda: 0.5176 - val_rmse: 0.0126 - learning_rate: 0.0025\n",
            "Epoch 15/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.1227e-05 - mae: 0.0060 - mda: 0.5037 - rmse: 0.0786\n",
            "Epoch 15: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 7.1192e-05 - mae: 0.0060 - mda: 0.5037 - rmse: 0.0786 - val_loss: 9.0564e-05 - val_mae: 0.0064 - val_mda: 0.5170 - val_rmse: 0.0102 - learning_rate: 0.0025\n",
            "Epoch 16/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.5047e-05 - mae: 0.0057 - mda: 0.5021 - rmse: 0.0784\n",
            "Epoch 16: val_loss did not improve from 0.00006\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6.5020e-05 - mae: 0.0057 - mda: 0.5021 - rmse: 0.0784 - val_loss: 4.5470e-04 - val_mae: 0.0150 - val_mda: 0.5158 - val_rmse: 0.0178 - learning_rate: 0.0025\n",
            "Epoch 17/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.0894e-05 - mae: 0.0054 - mda: 0.5029 - rmse: 0.0779\n",
            "Epoch 17: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6.0868e-05 - mae: 0.0054 - mda: 0.5029 - rmse: 0.0779 - val_loss: 2.0525e-04 - val_mae: 0.0097 - val_mda: 0.5156 - val_rmse: 0.0130 - learning_rate: 0.0012\n",
            "Epoch 18/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.1205e-05 - mae: 0.0055 - mda: 0.5023 - rmse: 0.0778\n",
            "Epoch 18: val_loss improved from 0.00006 to 0.00006, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6.1160e-05 - mae: 0.0055 - mda: 0.5023 - rmse: 0.0778 - val_loss: 5.5836e-05 - val_mae: 0.0051 - val_mda: 0.5156 - val_rmse: 0.0094 - learning_rate: 0.0012\n",
            "Epoch 19/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.8605e-05 - mae: 0.0055 - mda: 0.5033 - rmse: 0.0785\n",
            "Epoch 19: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.8600e-05 - mae: 0.0055 - mda: 0.5033 - rmse: 0.0785 - val_loss: 1.2128e-04 - val_mae: 0.0075 - val_mda: 0.5159 - val_rmse: 0.0112 - learning_rate: 0.0012\n",
            "Epoch 20/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6736e-05 - mae: 0.0054 - mda: 0.5039 - rmse: 0.0783\n",
            "Epoch 20: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.6753e-05 - mae: 0.0054 - mda: 0.5039 - rmse: 0.0783 - val_loss: 8.8803e-05 - val_mae: 0.0064 - val_mda: 0.5156 - val_rmse: 0.0103 - learning_rate: 0.0012\n",
            "Epoch 21/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.8664e-05 - mae: 0.0055 - mda: 0.5037 - rmse: 0.0780\n",
            "Epoch 21: val_loss improved from 0.00006 to 0.00004, saving model to best_model.keras\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.8683e-05 - mae: 0.0055 - mda: 0.5037 - rmse: 0.0780 - val_loss: 4.1542e-05 - val_mae: 0.0046 - val_mda: 0.5150 - val_rmse: 0.0090 - learning_rate: 0.0012\n",
            "Epoch 22/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9318e-05 - mae: 0.0054 - mda: 0.5039 - rmse: 0.0788\n",
            "Epoch 22: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.9309e-05 - mae: 0.0054 - mda: 0.5039 - rmse: 0.0788 - val_loss: 3.1893e-04 - val_mae: 0.0139 - val_mda: 0.5155 - val_rmse: 0.0166 - learning_rate: 6.2500e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6347e-05 - mae: 0.0053 - mda: 0.5027 - rmse: 0.0779\n",
            "Epoch 23: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.6353e-05 - mae: 0.0053 - mda: 0.5027 - rmse: 0.0779 - val_loss: 2.3366e-04 - val_mae: 0.0113 - val_mda: 0.5155 - val_rmse: 0.0144 - learning_rate: 6.2500e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.6906e-05 - mae: 0.0053 - mda: 0.5022 - rmse: 0.0781\n",
            "Epoch 24: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.6910e-05 - mae: 0.0053 - mda: 0.5022 - rmse: 0.0781 - val_loss: 1.8590e-04 - val_mae: 0.0099 - val_mda: 0.5153 - val_rmse: 0.0132 - learning_rate: 6.2500e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.7390e-05 - mae: 0.0054 - mda: 0.5030 - rmse: 0.0788\n",
            "Epoch 25: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.7380e-05 - mae: 0.0054 - mda: 0.5030 - rmse: 0.0788 - val_loss: 3.3357e-04 - val_mae: 0.0136 - val_mda: 0.5150 - val_rmse: 0.0165 - learning_rate: 6.2500e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4721e-05 - mae: 0.0053 - mda: 0.5027 - rmse: 0.0779\n",
            "Epoch 26: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.4771e-05 - mae: 0.0053 - mda: 0.5027 - rmse: 0.0779 - val_loss: 1.4964e-04 - val_mae: 0.0089 - val_mda: 0.5150 - val_rmse: 0.0124 - learning_rate: 6.2500e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5817e-05 - mae: 0.0053 - mda: 0.5046 - rmse: 0.0788\n",
            "Epoch 27: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.5813e-05 - mae: 0.0053 - mda: 0.5046 - rmse: 0.0788 - val_loss: 1.9722e-04 - val_mae: 0.0103 - val_mda: 0.5151 - val_rmse: 0.0135 - learning_rate: 3.1250e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.5891e-05 - mae: 0.0053 - mda: 0.5033 - rmse: 0.0786\n",
            "Epoch 28: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.5881e-05 - mae: 0.0053 - mda: 0.5033 - rmse: 0.0786 - val_loss: 1.5963e-04 - val_mae: 0.0086 - val_mda: 0.5148 - val_rmse: 0.0122 - learning_rate: 3.1250e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.1995e-05 - mae: 0.0051 - mda: 0.5026 - rmse: 0.0782\n",
            "Epoch 29: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.2011e-05 - mae: 0.0051 - mda: 0.5026 - rmse: 0.0782 - val_loss: 8.2317e-05 - val_mae: 0.0063 - val_mda: 0.5145 - val_rmse: 0.0103 - learning_rate: 3.1250e-04\n",
            "Epoch 30/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.4259e-05 - mae: 0.0052 - mda: 0.5029 - rmse: 0.0786\n",
            "Epoch 30: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.4260e-05 - mae: 0.0052 - mda: 0.5029 - rmse: 0.0786 - val_loss: 1.9147e-04 - val_mae: 0.0100 - val_mda: 0.5144 - val_rmse: 0.0133 - learning_rate: 3.1250e-04\n",
            "Epoch 31/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.3461e-05 - mae: 0.0051 - mda: 0.5032 - rmse: 0.0788\n",
            "Epoch 31: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.3462e-05 - mae: 0.0051 - mda: 0.5032 - rmse: 0.0788 - val_loss: 1.2765e-04 - val_mae: 0.0081 - val_mda: 0.5144 - val_rmse: 0.0117 - learning_rate: 3.1250e-04\n",
            "Epoch 31: early stopping\n",
            "Restoring model weights from the end of the best epoch: 21.\n",
            "Validation Loss: 0.00004, RMSE: 0.00903, MDA: 0.51849, MAE: 0.00461\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0031 - mae: 0.0265 - mda: 0.5012 - rmse: 0.0849\n",
            "Epoch 1: val_loss improved from inf to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0030 - mae: 0.0264 - mda: 0.5012 - rmse: 0.0848 - val_loss: 3.0751e-05 - val_mae: 0.0042 - val_mda: 0.5135 - val_rmse: 0.0089 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.4995e-05 - mae: 0.0063 - mda: 0.5023 - rmse: 0.0783\n",
            "Epoch 2: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 7.4886e-05 - mae: 0.0063 - mda: 0.5023 - rmse: 0.0783 - val_loss: 1.1155e-04 - val_mae: 0.0091 - val_mda: 0.5151 - val_rmse: 0.0127 - learning_rate: 0.0100\n",
            "Epoch 3/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.2886e-05 - mae: 0.0057 - mda: 0.5034 - rmse: 0.0782\n",
            "Epoch 3: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 6.2797e-05 - mae: 0.0057 - mda: 0.5034 - rmse: 0.0782 - val_loss: 6.6415e-05 - val_mae: 0.0059 - val_mda: 0.5156 - val_rmse: 0.0099 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.0111e-05 - mae: 0.0052 - mda: 0.5028 - rmse: 0.0791\n",
            "Epoch 4: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 5.0137e-05 - mae: 0.0052 - mda: 0.5028 - rmse: 0.0791 - val_loss: 1.4014e-04 - val_mae: 0.0095 - val_mda: 0.5159 - val_rmse: 0.0128 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.6334e-05 - mae: 0.0056 - mda: 0.5027 - rmse: 0.0782\n",
            "Epoch 5: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 5.6313e-05 - mae: 0.0056 - mda: 0.5027 - rmse: 0.0782 - val_loss: 2.3223e-04 - val_mae: 0.0108 - val_mda: 0.5153 - val_rmse: 0.0140 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.4761e-05 - mae: 0.0049 - mda: 0.5030 - rmse: 0.0776\n",
            "Epoch 6: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 4.4809e-05 - mae: 0.0049 - mda: 0.5030 - rmse: 0.0776 - val_loss: 7.5556e-05 - val_mae: 0.0057 - val_mda: 0.5148 - val_rmse: 0.0100 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.9571e-05 - mae: 0.0045 - mda: 0.5038 - rmse: 0.0790\n",
            "Epoch 7: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.9558e-05 - mae: 0.0045 - mda: 0.5038 - rmse: 0.0790 - val_loss: 1.2842e-04 - val_mae: 0.0077 - val_mda: 0.5145 - val_rmse: 0.0115 - learning_rate: 0.0050\n",
            "Epoch 8/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.7132e-05 - mae: 0.0044 - mda: 0.5034 - rmse: 0.0777\n",
            "Epoch 8: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.7211e-05 - mae: 0.0044 - mda: 0.5034 - rmse: 0.0777 - val_loss: 5.1892e-05 - val_mae: 0.0049 - val_mda: 0.5145 - val_rmse: 0.0094 - learning_rate: 0.0050\n",
            "Epoch 9/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.9831e-05 - mae: 0.0052 - mda: 0.5036 - rmse: 0.0784\n",
            "Epoch 9: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.9743e-05 - mae: 0.0052 - mda: 0.5036 - rmse: 0.0784 - val_loss: 4.1480e-05 - val_mae: 0.0046 - val_mda: 0.5148 - val_rmse: 0.0092 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.8263e-05 - mae: 0.0045 - mda: 0.5021 - rmse: 0.0778\n",
            "Epoch 10: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.8289e-05 - mae: 0.0045 - mda: 0.5021 - rmse: 0.0779 - val_loss: 3.7679e-05 - val_mae: 0.0049 - val_mda: 0.5136 - val_rmse: 0.0095 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.0031e-05 - mae: 0.0045 - mda: 0.5037 - rmse: 0.0778\n",
            "Epoch 11: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.0034e-05 - mae: 0.0045 - mda: 0.5037 - rmse: 0.0778 - val_loss: 5.3302e-04 - val_mae: 0.0187 - val_mda: 0.5144 - val_rmse: 0.0211 - learning_rate: 0.0050\n",
            "Epoch 11: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "Validation Loss: 0.00003, RMSE: 0.00892, MDA: 0.51588, MAE: 0.00422\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0079 - mae: 0.0347 - mda: 0.5026 - rmse: 0.0922\n",
            "Epoch 1: val_loss improved from inf to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 0.0078 - mae: 0.0346 - mda: 0.5026 - rmse: 0.0922 - val_loss: 4.4085e-05 - val_mae: 0.0055 - val_mda: 0.5116 - val_rmse: 0.0100 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 6.2467e-05 - mae: 0.0058 - mda: 0.5037 - rmse: 0.0786\n",
            "Epoch 2: val_loss improved from 0.00004 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 6.2447e-05 - mae: 0.0058 - mda: 0.5037 - rmse: 0.0786 - val_loss: 3.7320e-05 - val_mae: 0.0048 - val_mda: 0.5146 - val_rmse: 0.0093 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 5.0326e-05 - mae: 0.0053 - mda: 0.5038 - rmse: 0.0781\n",
            "Epoch 3: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 5.0327e-05 - mae: 0.0053 - mda: 0.5038 - rmse: 0.0781 - val_loss: 1.9282e-04 - val_mae: 0.0124 - val_mda: 0.5144 - val_rmse: 0.0154 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.7606e-05 - mae: 0.0051 - mda: 0.5036 - rmse: 0.0785\n",
            "Epoch 4: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 4.7573e-05 - mae: 0.0051 - mda: 0.5036 - rmse: 0.0785 - val_loss: 5.7068e-05 - val_mae: 0.0053 - val_mda: 0.5137 - val_rmse: 0.0095 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.5183e-05 - mae: 0.0043 - mda: 0.5041 - rmse: 0.0784\n",
            "Epoch 5: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 3.5187e-05 - mae: 0.0043 - mda: 0.5041 - rmse: 0.0784 - val_loss: 3.9752e-05 - val_mae: 0.0050 - val_mda: 0.5143 - val_rmse: 0.0094 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.5562e-05 - mae: 0.0044 - mda: 0.5041 - rmse: 0.0777\n",
            "Epoch 6: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - loss: 3.5560e-05 - mae: 0.0044 - mda: 0.5041 - rmse: 0.0777 - val_loss: 2.0043e-04 - val_mae: 0.0110 - val_mda: 0.5148 - val_rmse: 0.0142 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 3.1218e-05 - mae: 0.0040 - mda: 0.5029 - rmse: 0.0789\n",
            "Epoch 7: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - loss: 3.1220e-05 - mae: 0.0040 - mda: 0.5030 - rmse: 0.0789 - val_loss: 3.8525e-05 - val_mae: 0.0049 - val_mda: 0.5145 - val_rmse: 0.0094 - learning_rate: 0.0050\n",
            "Epoch 8/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.8400e-05 - mae: 0.0038 - mda: 0.5037 - rmse: 0.0787\n",
            "Epoch 8: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - loss: 2.8391e-05 - mae: 0.0038 - mda: 0.5037 - rmse: 0.0787 - val_loss: 4.5088e-05 - val_mae: 0.0047 - val_mda: 0.5150 - val_rmse: 0.0091 - learning_rate: 0.0050\n",
            "Epoch 9/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.8440e-05 - mae: 0.0038 - mda: 0.5034 - rmse: 0.0784\n",
            "Epoch 9: val_loss improved from 0.00004 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.8441e-05 - mae: 0.0038 - mda: 0.5034 - rmse: 0.0784 - val_loss: 3.3735e-05 - val_mae: 0.0043 - val_mda: 0.5150 - val_rmse: 0.0089 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.1194e-05 - mae: 0.0040 - mda: 0.5023 - rmse: 0.0785\n",
            "Epoch 10: val_loss improved from 0.00003 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 3.1193e-05 - mae: 0.0040 - mda: 0.5024 - rmse: 0.0785 - val_loss: 3.1628e-05 - val_mae: 0.0043 - val_mda: 0.5152 - val_rmse: 0.0089 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 3.2111e-05 - mae: 0.0041 - mda: 0.5034 - rmse: 0.0789\n",
            "Epoch 11: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 3.2102e-05 - mae: 0.0041 - mda: 0.5034 - rmse: 0.0789 - val_loss: 1.4188e-04 - val_mae: 0.0089 - val_mda: 0.5138 - val_rmse: 0.0124 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.6341e-05 - mae: 0.0037 - mda: 0.5033 - rmse: 0.0780\n",
            "Epoch 12: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.6325e-05 - mae: 0.0037 - mda: 0.5033 - rmse: 0.0780 - val_loss: 6.8672e-05 - val_mae: 0.0057 - val_mda: 0.5145 - val_rmse: 0.0100 - learning_rate: 0.0025\n",
            "Epoch 13/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.5262e-05 - mae: 0.0035 - mda: 0.5036 - rmse: 0.0791\n",
            "Epoch 13: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.5253e-05 - mae: 0.0035 - mda: 0.5036 - rmse: 0.0791 - val_loss: 3.9346e-05 - val_mae: 0.0049 - val_mda: 0.5140 - val_rmse: 0.0094 - learning_rate: 0.0025\n",
            "Epoch 14/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.5557e-05 - mae: 0.0036 - mda: 0.5046 - rmse: 0.0781\n",
            "Epoch 14: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.5565e-05 - mae: 0.0036 - mda: 0.5046 - rmse: 0.0781 - val_loss: 8.2373e-05 - val_mae: 0.0062 - val_mda: 0.5133 - val_rmse: 0.0104 - learning_rate: 0.0025\n",
            "Epoch 15/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.6443e-05 - mae: 0.0037 - mda: 0.5037 - rmse: 0.0784\n",
            "Epoch 15: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.6430e-05 - mae: 0.0037 - mda: 0.5037 - rmse: 0.0784 - val_loss: 1.6294e-04 - val_mae: 0.0094 - val_mda: 0.5129 - val_rmse: 0.0129 - learning_rate: 0.0025\n",
            "Epoch 16/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.5568e-05 - mae: 0.0037 - mda: 0.5032 - rmse: 0.0786\n",
            "Epoch 16: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - loss: 2.5576e-05 - mae: 0.0037 - mda: 0.5032 - rmse: 0.0786 - val_loss: 2.7272e-04 - val_mae: 0.0127 - val_mda: 0.5130 - val_rmse: 0.0158 - learning_rate: 0.0025\n",
            "Epoch 17/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.4261e-05 - mae: 0.0035 - mda: 0.5040 - rmse: 0.0783\n",
            "Epoch 17: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step - loss: 2.4258e-05 - mae: 0.0035 - mda: 0.5040 - rmse: 0.0783 - val_loss: 1.0960e-04 - val_mae: 0.0075 - val_mda: 0.5131 - val_rmse: 0.0114 - learning_rate: 0.0012\n",
            "Epoch 18/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.2390e-05 - mae: 0.0033 - mda: 0.5049 - rmse: 0.0782\n",
            "Epoch 18: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.2396e-05 - mae: 0.0033 - mda: 0.5049 - rmse: 0.0782 - val_loss: 1.0818e-04 - val_mae: 0.0073 - val_mda: 0.5134 - val_rmse: 0.0113 - learning_rate: 0.0012\n",
            "Epoch 19/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.4684e-05 - mae: 0.0035 - mda: 0.5032 - rmse: 0.0791\n",
            "Epoch 19: val_loss improved from 0.00003 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.4680e-05 - mae: 0.0035 - mda: 0.5032 - rmse: 0.0791 - val_loss: 3.0185e-05 - val_mae: 0.0041 - val_mda: 0.5130 - val_rmse: 0.0090 - learning_rate: 0.0012\n",
            "Epoch 20/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.5559e-05 - mae: 0.0036 - mda: 0.5030 - rmse: 0.0784\n",
            "Epoch 20: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 2.5546e-05 - mae: 0.0036 - mda: 0.5030 - rmse: 0.0784 - val_loss: 1.0438e-04 - val_mae: 0.0072 - val_mda: 0.5129 - val_rmse: 0.0112 - learning_rate: 0.0012\n",
            "Epoch 21/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.3617e-05 - mae: 0.0035 - mda: 0.5032 - rmse: 0.0771\n",
            "Epoch 21: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 2.3630e-05 - mae: 0.0035 - mda: 0.5032 - rmse: 0.0771 - val_loss: 4.7253e-05 - val_mae: 0.0047 - val_mda: 0.5128 - val_rmse: 0.0093 - learning_rate: 0.0012\n",
            "Epoch 22/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.3494e-05 - mae: 0.0034 - mda: 0.5033 - rmse: 0.0788\n",
            "Epoch 22: val_loss improved from 0.00003 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.3480e-05 - mae: 0.0034 - mda: 0.5033 - rmse: 0.0788 - val_loss: 2.7416e-05 - val_mae: 0.0038 - val_mda: 0.5127 - val_rmse: 0.0088 - learning_rate: 6.2500e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.0756e-05 - mae: 0.0033 - mda: 0.5036 - rmse: 0.0781\n",
            "Epoch 23: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 2.0758e-05 - mae: 0.0033 - mda: 0.5036 - rmse: 0.0781 - val_loss: 3.5784e-05 - val_mae: 0.0043 - val_mda: 0.5130 - val_rmse: 0.0092 - learning_rate: 6.2500e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.1971e-05 - mae: 0.0033 - mda: 0.5034 - rmse: 0.0777\n",
            "Epoch 24: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 2.1976e-05 - mae: 0.0033 - mda: 0.5034 - rmse: 0.0777 - val_loss: 1.0054e-04 - val_mae: 0.0069 - val_mda: 0.5121 - val_rmse: 0.0110 - learning_rate: 6.2500e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.2382e-05 - mae: 0.0034 - mda: 0.5030 - rmse: 0.0786\n",
            "Epoch 25: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.2379e-05 - mae: 0.0034 - mda: 0.5030 - rmse: 0.0786 - val_loss: 1.1125e-04 - val_mae: 0.0072 - val_mda: 0.5124 - val_rmse: 0.0113 - learning_rate: 6.2500e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.2081e-05 - mae: 0.0033 - mda: 0.5031 - rmse: 0.0788\n",
            "Epoch 26: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.2082e-05 - mae: 0.0033 - mda: 0.5031 - rmse: 0.0788 - val_loss: 1.2422e-04 - val_mae: 0.0076 - val_mda: 0.5125 - val_rmse: 0.0116 - learning_rate: 6.2500e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.1466e-05 - mae: 0.0033 - mda: 0.5031 - rmse: 0.0779\n",
            "Epoch 27: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.1467e-05 - mae: 0.0033 - mda: 0.5031 - rmse: 0.0779 - val_loss: 4.4977e-05 - val_mae: 0.0045 - val_mda: 0.5124 - val_rmse: 0.0093 - learning_rate: 3.1250e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.0869e-05 - mae: 0.0032 - mda: 0.5029 - rmse: 0.0787\n",
            "Epoch 28: val_loss improved from 0.00003 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.0868e-05 - mae: 0.0032 - mda: 0.5029 - rmse: 0.0787 - val_loss: 2.7028e-05 - val_mae: 0.0040 - val_mda: 0.5120 - val_rmse: 0.0090 - learning_rate: 3.1250e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.0840e-05 - mae: 0.0032 - mda: 0.5027 - rmse: 0.0791\n",
            "Epoch 29: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.0836e-05 - mae: 0.0032 - mda: 0.5027 - rmse: 0.0791 - val_loss: 1.9447e-04 - val_mae: 0.0102 - val_mda: 0.5108 - val_rmse: 0.0137 - learning_rate: 3.1250e-04\n",
            "Epoch 30/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.9508e-05 - mae: 0.0031 - mda: 0.5046 - rmse: 0.0780\n",
            "Epoch 30: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 1.9508e-05 - mae: 0.0031 - mda: 0.5046 - rmse: 0.0780 - val_loss: 1.4171e-04 - val_mae: 0.0087 - val_mda: 0.5108 - val_rmse: 0.0125 - learning_rate: 3.1250e-04\n",
            "Epoch 31/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.0544e-05 - mae: 0.0032 - mda: 0.5030 - rmse: 0.0781\n",
            "Epoch 31: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.0543e-05 - mae: 0.0032 - mda: 0.5030 - rmse: 0.0781 - val_loss: 6.5950e-05 - val_mae: 0.0057 - val_mda: 0.5107 - val_rmse: 0.0101 - learning_rate: 3.1250e-04\n",
            "Epoch 32/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.0437e-05 - mae: 0.0032 - mda: 0.5029 - rmse: 0.0778\n",
            "Epoch 32: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.0432e-05 - mae: 0.0032 - mda: 0.5029 - rmse: 0.0779 - val_loss: 4.4430e-05 - val_mae: 0.0045 - val_mda: 0.5109 - val_rmse: 0.0094 - learning_rate: 1.5625e-04\n",
            "Epoch 33/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.8751e-05 - mae: 0.0031 - mda: 0.5038 - rmse: 0.0783\n",
            "Epoch 33: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 1.8754e-05 - mae: 0.0031 - mda: 0.5038 - rmse: 0.0783 - val_loss: 6.5921e-05 - val_mae: 0.0054 - val_mda: 0.5105 - val_rmse: 0.0100 - learning_rate: 1.5625e-04\n",
            "Epoch 34/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.0730e-05 - mae: 0.0032 - mda: 0.5026 - rmse: 0.0792\n",
            "Epoch 34: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 2.0726e-05 - mae: 0.0032 - mda: 0.5026 - rmse: 0.0792 - val_loss: 2.8823e-05 - val_mae: 0.0039 - val_mda: 0.5104 - val_rmse: 0.0090 - learning_rate: 1.5625e-04\n",
            "Epoch 35/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.0744e-05 - mae: 0.0032 - mda: 0.5031 - rmse: 0.0787\n",
            "Epoch 35: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 2.0730e-05 - mae: 0.0032 - mda: 0.5031 - rmse: 0.0787 - val_loss: 1.4139e-04 - val_mae: 0.0086 - val_mda: 0.5101 - val_rmse: 0.0124 - learning_rate: 1.5625e-04\n",
            "Epoch 36/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.0113e-05 - mae: 0.0031 - mda: 0.5043 - rmse: 0.0779\n",
            "Epoch 36: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 2.0114e-05 - mae: 0.0031 - mda: 0.5043 - rmse: 0.0779 - val_loss: 5.8350e-05 - val_mae: 0.0052 - val_mda: 0.5101 - val_rmse: 0.0098 - learning_rate: 1.5625e-04\n",
            "Epoch 37/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.8962e-05 - mae: 0.0031 - mda: 0.5025 - rmse: 0.0780\n",
            "Epoch 37: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.8967e-05 - mae: 0.0031 - mda: 0.5025 - rmse: 0.0780 - val_loss: 8.8528e-05 - val_mae: 0.0064 - val_mda: 0.5098 - val_rmse: 0.0108 - learning_rate: 7.8125e-05\n",
            "Epoch 38/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.8005e-05 - mae: 0.0030 - mda: 0.5025 - rmse: 0.0780\n",
            "Epoch 38: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.8007e-05 - mae: 0.0030 - mda: 0.5025 - rmse: 0.0780 - val_loss: 5.2831e-05 - val_mae: 0.0048 - val_mda: 0.5099 - val_rmse: 0.0096 - learning_rate: 7.8125e-05\n",
            "Epoch 38: early stopping\n",
            "Restoring model weights from the end of the best epoch: 28.\n",
            "Validation Loss: 0.00003, RMSE: 0.00880, MDA: 0.51517, MAE: 0.00380\n",
            "--------------------------------------------------\n",
            "Best GRU Parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 128}, Validation Loss: 1.4037846085557248e-05\n"
          ]
        }
      ],
      "source": [
        "print(\"Tuning GRU...\")\n",
        "best_gru_model, best_gru_params, best_gru_loss, gru_results = tune_model(build_gru, X_train, y_train, X_val, y_val, param_grid)\n",
        "print(f\"Best GRU Parameters: {best_gru_params}, Validation Loss: {best_gru_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4831d12e",
      "metadata": {
        "id": "4831d12e"
      },
      "source": [
        "### Tuning LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "130b3dd5",
      "metadata": {
        "scrolled": true,
        "id": "130b3dd5",
        "outputId": "1e924fd3-188c-41e3-d70d-2727c9f198a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning LSTM...\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5031e-04 - mae: 0.0101 - mda: 0.5066 - rmse: 0.0759\n",
            "Epoch 1: val_loss improved from inf to 0.00021, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 3.4885e-04 - mae: 0.0101 - mda: 0.5066 - rmse: 0.0759 - val_loss: 2.0594e-04 - val_mae: 0.0095 - val_mda: 0.5219 - val_rmse: 0.0108 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.9406e-05 - mae: 0.0052 - mda: 0.5066 - rmse: 0.0764\n",
            "Epoch 2: val_loss improved from 0.00021 to 0.00015, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.9352e-05 - mae: 0.0052 - mda: 0.5066 - rmse: 0.0764 - val_loss: 1.5467e-04 - val_mae: 0.0085 - val_mda: 0.5220 - val_rmse: 0.0099 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m572/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.0945e-05 - mae: 0.0050 - mda: 0.5080 - rmse: 0.0768\n",
            "Epoch 3: val_loss did not improve from 0.00015\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.0984e-05 - mae: 0.0050 - mda: 0.5080 - rmse: 0.0768 - val_loss: 2.5221e-04 - val_mae: 0.0109 - val_mda: 0.5236 - val_rmse: 0.0123 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.0557e-05 - mae: 0.0051 - mda: 0.5085 - rmse: 0.0759\n",
            "Epoch 4: val_loss improved from 0.00015 to 0.00007, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.0579e-05 - mae: 0.0051 - mda: 0.5085 - rmse: 0.0759 - val_loss: 6.6944e-05 - val_mae: 0.0057 - val_mda: 0.5233 - val_rmse: 0.0075 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2316e-05 - mae: 0.0052 - mda: 0.5066 - rmse: 0.0765\n",
            "Epoch 5: val_loss did not improve from 0.00007\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.2268e-05 - mae: 0.0052 - mda: 0.5066 - rmse: 0.0765 - val_loss: 1.0180e-04 - val_mae: 0.0070 - val_mda: 0.5232 - val_rmse: 0.0087 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m570/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5584e-05 - mae: 0.0049 - mda: 0.5069 - rmse: 0.0761\n",
            "Epoch 6: val_loss improved from 0.00007 to 0.00006, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.5598e-05 - mae: 0.0049 - mda: 0.5069 - rmse: 0.0761 - val_loss: 5.7959e-05 - val_mae: 0.0056 - val_mda: 0.5236 - val_rmse: 0.0074 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m568/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.8610e-05 - mae: 0.0050 - mda: 0.5078 - rmse: 0.0770\n",
            "Epoch 7: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.8556e-05 - mae: 0.0050 - mda: 0.5078 - rmse: 0.0770 - val_loss: 1.1087e-04 - val_mae: 0.0071 - val_mda: 0.5250 - val_rmse: 0.0089 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m573/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5433e-05 - mae: 0.0049 - mda: 0.5072 - rmse: 0.0766\n",
            "Epoch 8: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.5451e-05 - mae: 0.0049 - mda: 0.5073 - rmse: 0.0766 - val_loss: 1.8563e-04 - val_mae: 0.0096 - val_mda: 0.5251 - val_rmse: 0.0112 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4234e-05 - mae: 0.0048 - mda: 0.5084 - rmse: 0.0765\n",
            "Epoch 9: val_loss did not improve from 0.00006\n",
            "\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.4234e-05 - mae: 0.0048 - mda: 0.5084 - rmse: 0.0765 - val_loss: 7.5940e-05 - val_mae: 0.0068 - val_mda: 0.5250 - val_rmse: 0.0088 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2227e-05 - mae: 0.0047 - mda: 0.5080 - rmse: 0.0761\n",
            "Epoch 10: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.2248e-05 - mae: 0.0047 - mda: 0.5080 - rmse: 0.0761 - val_loss: 8.1780e-05 - val_mae: 0.0061 - val_mda: 0.5249 - val_rmse: 0.0081 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m572/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1092e-05 - mae: 0.0046 - mda: 0.5076 - rmse: 0.0765\n",
            "Epoch 11: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.1072e-05 - mae: 0.0046 - mda: 0.5076 - rmse: 0.0765 - val_loss: 6.7600e-05 - val_mae: 0.0057 - val_mda: 0.5253 - val_rmse: 0.0078 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m570/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0920e-05 - mae: 0.0045 - mda: 0.5067 - rmse: 0.0765\n",
            "Epoch 12: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.0922e-05 - mae: 0.0045 - mda: 0.5067 - rmse: 0.0765 - val_loss: 2.0043e-04 - val_mae: 0.0104 - val_mda: 0.5244 - val_rmse: 0.0121 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5476e-05 - mae: 0.0043 - mda: 0.5070 - rmse: 0.0757\n",
            "Epoch 13: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.5502e-05 - mae: 0.0043 - mda: 0.5070 - rmse: 0.0757 - val_loss: 7.6823e-05 - val_mae: 0.0059 - val_mda: 0.5245 - val_rmse: 0.0080 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m570/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1296e-05 - mae: 0.0046 - mda: 0.5078 - rmse: 0.0768\n",
            "Epoch 14: val_loss did not improve from 0.00006\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.1288e-05 - mae: 0.0046 - mda: 0.5078 - rmse: 0.0768 - val_loss: 1.9916e-04 - val_mae: 0.0102 - val_mda: 0.5252 - val_rmse: 0.0119 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m570/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8338e-05 - mae: 0.0043 - mda: 0.5073 - rmse: 0.0764\n",
            "Epoch 15: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.8332e-05 - mae: 0.0043 - mda: 0.5073 - rmse: 0.0764 - val_loss: 7.9450e-05 - val_mae: 0.0060 - val_mda: 0.5255 - val_rmse: 0.0080 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m573/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4928e-05 - mae: 0.0042 - mda: 0.5094 - rmse: 0.0757\n",
            "Epoch 16: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.4959e-05 - mae: 0.0042 - mda: 0.5094 - rmse: 0.0757 - val_loss: 1.7588e-04 - val_mae: 0.0090 - val_mda: 0.5249 - val_rmse: 0.0108 - learning_rate: 2.5000e-04\n",
            "Epoch 16: early stopping\n",
            "Restoring model weights from the end of the best epoch: 6.\n",
            "Validation Loss: 0.00006, RMSE: 0.00744, MDA: 0.52554, MAE: 0.00555\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6230e-04 - mae: 0.0088 - mda: 0.5071 - rmse: 0.0752\n",
            "Epoch 1: val_loss improved from inf to 0.00048, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 2.5979e-04 - mae: 0.0087 - mda: 0.5071 - rmse: 0.0752 - val_loss: 4.7651e-04 - val_mae: 0.0170 - val_mda: 0.5217 - val_rmse: 0.0181 - learning_rate: 0.0010\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.3906e-05 - mae: 0.0043 - mda: 0.5086 - rmse: 0.0760\n",
            "Epoch 2: val_loss improved from 0.00048 to 0.00006, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.3900e-05 - mae: 0.0043 - mda: 0.5086 - rmse: 0.0760 - val_loss: 6.2380e-05 - val_mae: 0.0064 - val_mda: 0.5228 - val_rmse: 0.0081 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5787e-05 - mae: 0.0042 - mda: 0.5091 - rmse: 0.0764\n",
            "Epoch 3: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.5789e-05 - mae: 0.0042 - mda: 0.5091 - rmse: 0.0764 - val_loss: 1.4699e-04 - val_mae: 0.0094 - val_mda: 0.5231 - val_rmse: 0.0111 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7751e-05 - mae: 0.0043 - mda: 0.5079 - rmse: 0.0762\n",
            "Epoch 4: val_loss improved from 0.00006 to 0.00006, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.7741e-05 - mae: 0.0043 - mda: 0.5079 - rmse: 0.0762 - val_loss: 5.5092e-05 - val_mae: 0.0053 - val_mda: 0.5238 - val_rmse: 0.0072 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1876e-05 - mae: 0.0041 - mda: 0.5074 - rmse: 0.0766\n",
            "Epoch 5: val_loss improved from 0.00006 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.1871e-05 - mae: 0.0041 - mda: 0.5074 - rmse: 0.0766 - val_loss: 4.2577e-05 - val_mae: 0.0047 - val_mda: 0.5254 - val_rmse: 0.0069 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0972e-05 - mae: 0.0041 - mda: 0.5072 - rmse: 0.0764\n",
            "Epoch 6: val_loss improved from 0.00004 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.0969e-05 - mae: 0.0041 - mda: 0.5072 - rmse: 0.0764 - val_loss: 3.0432e-05 - val_mae: 0.0042 - val_mda: 0.5255 - val_rmse: 0.0065 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.6424e-05 - mae: 0.0044 - mda: 0.5079 - rmse: 0.0769\n",
            "Epoch 7: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.6395e-05 - mae: 0.0043 - mda: 0.5079 - rmse: 0.0769 - val_loss: 1.1518e-04 - val_mae: 0.0079 - val_mda: 0.5250 - val_rmse: 0.0099 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7077e-05 - mae: 0.0037 - mda: 0.5104 - rmse: 0.0776\n",
            "Epoch 8: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.7075e-05 - mae: 0.0037 - mda: 0.5104 - rmse: 0.0776 - val_loss: 3.2340e-05 - val_mae: 0.0045 - val_mda: 0.5244 - val_rmse: 0.0068 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6787e-05 - mae: 0.0037 - mda: 0.5088 - rmse: 0.0770\n",
            "Epoch 9: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.6784e-05 - mae: 0.0037 - mda: 0.5088 - rmse: 0.0770 - val_loss: 4.0877e-05 - val_mae: 0.0046 - val_mda: 0.5246 - val_rmse: 0.0069 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5915e-05 - mae: 0.0036 - mda: 0.5072 - rmse: 0.0773\n",
            "Epoch 10: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.5918e-05 - mae: 0.0036 - mda: 0.5072 - rmse: 0.0773 - val_loss: 6.1108e-05 - val_mae: 0.0057 - val_mda: 0.5245 - val_rmse: 0.0078 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6524e-05 - mae: 0.0036 - mda: 0.5074 - rmse: 0.0770\n",
            "Epoch 11: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.6522e-05 - mae: 0.0036 - mda: 0.5074 - rmse: 0.0770 - val_loss: 5.1207e-05 - val_mae: 0.0053 - val_mda: 0.5247 - val_rmse: 0.0076 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6215e-05 - mae: 0.0036 - mda: 0.5082 - rmse: 0.0761\n",
            "Epoch 12: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.6237e-05 - mae: 0.0036 - mda: 0.5082 - rmse: 0.0761 - val_loss: 6.5750e-05 - val_mae: 0.0057 - val_mda: 0.5241 - val_rmse: 0.0079 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3731e-05 - mae: 0.0034 - mda: 0.5077 - rmse: 0.0768\n",
            "Epoch 13: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.3734e-05 - mae: 0.0034 - mda: 0.5077 - rmse: 0.0768 - val_loss: 8.9671e-05 - val_mae: 0.0068 - val_mda: 0.5238 - val_rmse: 0.0089 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3387e-05 - mae: 0.0034 - mda: 0.5087 - rmse: 0.0764\n",
            "Epoch 14: val_loss improved from 0.00003 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.3385e-05 - mae: 0.0034 - mda: 0.5087 - rmse: 0.0764 - val_loss: 3.0396e-05 - val_mae: 0.0040 - val_mda: 0.5241 - val_rmse: 0.0066 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2613e-05 - mae: 0.0034 - mda: 0.5077 - rmse: 0.0765\n",
            "Epoch 15: val_loss improved from 0.00003 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.2621e-05 - mae: 0.0034 - mda: 0.5077 - rmse: 0.0765 - val_loss: 2.6955e-05 - val_mae: 0.0039 - val_mda: 0.5238 - val_rmse: 0.0064 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1961e-05 - mae: 0.0033 - mda: 0.5093 - rmse: 0.0769\n",
            "Epoch 16: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.1960e-05 - mae: 0.0033 - mda: 0.5093 - rmse: 0.0769 - val_loss: 1.8422e-04 - val_mae: 0.0107 - val_mda: 0.5234 - val_rmse: 0.0124 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3651e-05 - mae: 0.0034 - mda: 0.5091 - rmse: 0.0763\n",
            "Epoch 17: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.3652e-05 - mae: 0.0034 - mda: 0.5091 - rmse: 0.0763 - val_loss: 3.1325e-05 - val_mae: 0.0041 - val_mda: 0.5239 - val_rmse: 0.0066 - learning_rate: 2.5000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2971e-05 - mae: 0.0034 - mda: 0.5104 - rmse: 0.0767\n",
            "Epoch 18: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.2968e-05 - mae: 0.0034 - mda: 0.5104 - rmse: 0.0767 - val_loss: 5.1069e-05 - val_mae: 0.0050 - val_mda: 0.5246 - val_rmse: 0.0074 - learning_rate: 1.2500e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0796e-05 - mae: 0.0032 - mda: 0.5077 - rmse: 0.0770\n",
            "Epoch 19: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.0801e-05 - mae: 0.0032 - mda: 0.5077 - rmse: 0.0770 - val_loss: 4.3286e-05 - val_mae: 0.0047 - val_mda: 0.5246 - val_rmse: 0.0071 - learning_rate: 1.2500e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1523e-05 - mae: 0.0033 - mda: 0.5088 - rmse: 0.0773\n",
            "Epoch 20: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.1522e-05 - mae: 0.0033 - mda: 0.5088 - rmse: 0.0773 - val_loss: 4.8090e-05 - val_mae: 0.0049 - val_mda: 0.5242 - val_rmse: 0.0073 - learning_rate: 1.2500e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0423e-05 - mae: 0.0032 - mda: 0.5069 - rmse: 0.0761\n",
            "Epoch 21: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.0427e-05 - mae: 0.0032 - mda: 0.5070 - rmse: 0.0761 - val_loss: 7.3009e-05 - val_mae: 0.0063 - val_mda: 0.5242 - val_rmse: 0.0085 - learning_rate: 1.2500e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2584e-05 - mae: 0.0033 - mda: 0.5094 - rmse: 0.0771\n",
            "Epoch 22: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.2574e-05 - mae: 0.0033 - mda: 0.5094 - rmse: 0.0771 - val_loss: 4.0078e-05 - val_mae: 0.0045 - val_mda: 0.5245 - val_rmse: 0.0070 - learning_rate: 1.2500e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9765e-05 - mae: 0.0031 - mda: 0.5074 - rmse: 0.0772\n",
            "Epoch 23: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1.9764e-05 - mae: 0.0031 - mda: 0.5074 - rmse: 0.0772 - val_loss: 4.1013e-05 - val_mae: 0.0046 - val_mda: 0.5244 - val_rmse: 0.0071 - learning_rate: 6.2500e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1138e-05 - mae: 0.0032 - mda: 0.5088 - rmse: 0.0769\n",
            "Epoch 24: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.1134e-05 - mae: 0.0032 - mda: 0.5088 - rmse: 0.0769 - val_loss: 4.2632e-05 - val_mae: 0.0046 - val_mda: 0.5240 - val_rmse: 0.0071 - learning_rate: 6.2500e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0697e-05 - mae: 0.0032 - mda: 0.5086 - rmse: 0.0766\n",
            "Epoch 25: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.0699e-05 - mae: 0.0032 - mda: 0.5086 - rmse: 0.0766 - val_loss: 6.3228e-05 - val_mae: 0.0056 - val_mda: 0.5240 - val_rmse: 0.0080 - learning_rate: 6.2500e-05\n",
            "Epoch 25: early stopping\n",
            "Restoring model weights from the end of the best epoch: 15.\n",
            "Validation Loss: 0.00003, RMSE: 0.00641, MDA: 0.52546, MAE: 0.00386\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.9278e-04 - mae: 0.0072 - mda: 0.5091 - rmse: 0.0761\n",
            "Epoch 1: val_loss improved from inf to 0.00017, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 1.9236e-04 - mae: 0.0072 - mda: 0.5091 - rmse: 0.0761 - val_loss: 1.7153e-04 - val_mae: 0.0105 - val_mda: 0.5236 - val_rmse: 0.0120 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2929e-05 - mae: 0.0039 - mda: 0.5101 - rmse: 0.0764\n",
            "Epoch 2: val_loss improved from 0.00017 to 0.00005, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 3.2933e-05 - mae: 0.0039 - mda: 0.5101 - rmse: 0.0764 - val_loss: 4.8655e-05 - val_mae: 0.0057 - val_mda: 0.5256 - val_rmse: 0.0077 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.9624e-05 - mae: 0.0039 - mda: 0.5088 - rmse: 0.0759\n",
            "Epoch 3: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.9621e-05 - mae: 0.0039 - mda: 0.5088 - rmse: 0.0759 - val_loss: 1.2425e-04 - val_mae: 0.0087 - val_mda: 0.5256 - val_rmse: 0.0106 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.7793e-05 - mae: 0.0038 - mda: 0.5101 - rmse: 0.0767\n",
            "Epoch 4: val_loss improved from 0.00005 to 0.00005, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.7785e-05 - mae: 0.0038 - mda: 0.5101 - rmse: 0.0767 - val_loss: 4.8126e-05 - val_mae: 0.0049 - val_mda: 0.5243 - val_rmse: 0.0072 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.4005e-05 - mae: 0.0035 - mda: 0.5091 - rmse: 0.0766\n",
            "Epoch 5: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 2.4006e-05 - mae: 0.0035 - mda: 0.5091 - rmse: 0.0766 - val_loss: 1.9735e-04 - val_mae: 0.0132 - val_mda: 0.5244 - val_rmse: 0.0147 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.5909e-05 - mae: 0.0036 - mda: 0.5075 - rmse: 0.0766\n",
            "Epoch 6: val_loss improved from 0.00005 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.5894e-05 - mae: 0.0036 - mda: 0.5075 - rmse: 0.0767 - val_loss: 3.9849e-05 - val_mae: 0.0045 - val_mda: 0.5251 - val_rmse: 0.0070 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.5251e-05 - mae: 0.0036 - mda: 0.5081 - rmse: 0.0772\n",
            "Epoch 7: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.5250e-05 - mae: 0.0036 - mda: 0.5081 - rmse: 0.0772 - val_loss: 1.1884e-04 - val_mae: 0.0084 - val_mda: 0.5251 - val_rmse: 0.0105 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.8186e-05 - mae: 0.0030 - mda: 0.5080 - rmse: 0.0760\n",
            "Epoch 8: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.8186e-05 - mae: 0.0030 - mda: 0.5080 - rmse: 0.0760 - val_loss: 8.8910e-05 - val_mae: 0.0068 - val_mda: 0.5249 - val_rmse: 0.0090 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.7157e-05 - mae: 0.0029 - mda: 0.5093 - rmse: 0.0767\n",
            "Epoch 9: val_loss improved from 0.00004 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 1.7162e-05 - mae: 0.0029 - mda: 0.5093 - rmse: 0.0767 - val_loss: 3.8349e-05 - val_mae: 0.0044 - val_mda: 0.5245 - val_rmse: 0.0070 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.9872e-05 - mae: 0.0032 - mda: 0.5083 - rmse: 0.0773\n",
            "Epoch 10: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.9869e-05 - mae: 0.0032 - mda: 0.5083 - rmse: 0.0773 - val_loss: 8.7770e-05 - val_mae: 0.0068 - val_mda: 0.5243 - val_rmse: 0.0091 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.8453e-05 - mae: 0.0030 - mda: 0.5088 - rmse: 0.0766\n",
            "Epoch 11: val_loss improved from 0.00004 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 1.8451e-05 - mae: 0.0030 - mda: 0.5088 - rmse: 0.0766 - val_loss: 2.6864e-05 - val_mae: 0.0038 - val_mda: 0.5238 - val_rmse: 0.0066 - learning_rate: 5.0000e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7637e-05 - mae: 0.0030 - mda: 0.5077 - rmse: 0.0775\n",
            "Epoch 12: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.7641e-05 - mae: 0.0030 - mda: 0.5077 - rmse: 0.0775 - val_loss: 9.8068e-05 - val_mae: 0.0087 - val_mda: 0.5233 - val_rmse: 0.0108 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6307e-05 - mae: 0.0028 - mda: 0.5100 - rmse: 0.0772\n",
            "Epoch 13: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 1.6302e-05 - mae: 0.0028 - mda: 0.5100 - rmse: 0.0772 - val_loss: 5.0476e-05 - val_mae: 0.0054 - val_mda: 0.5239 - val_rmse: 0.0079 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4427e-05 - mae: 0.0027 - mda: 0.5086 - rmse: 0.0759\n",
            "Epoch 14: val_loss improved from 0.00003 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 1.4427e-05 - mae: 0.0027 - mda: 0.5086 - rmse: 0.0759 - val_loss: 2.5408e-05 - val_mae: 0.0036 - val_mda: 0.5241 - val_rmse: 0.0065 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5102e-05 - mae: 0.0027 - mda: 0.5100 - rmse: 0.0761\n",
            "Epoch 15: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 1.5103e-05 - mae: 0.0027 - mda: 0.5100 - rmse: 0.0761 - val_loss: 4.6656e-05 - val_mae: 0.0050 - val_mda: 0.5230 - val_rmse: 0.0076 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5560e-05 - mae: 0.0028 - mda: 0.5078 - rmse: 0.0771\n",
            "Epoch 16: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 1.5560e-05 - mae: 0.0028 - mda: 0.5078 - rmse: 0.0771 - val_loss: 6.1254e-05 - val_mae: 0.0058 - val_mda: 0.5227 - val_rmse: 0.0083 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.4533e-05 - mae: 0.0027 - mda: 0.5092 - rmse: 0.0761\n",
            "Epoch 17: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.4533e-05 - mae: 0.0027 - mda: 0.5092 - rmse: 0.0761 - val_loss: 7.2190e-05 - val_mae: 0.0063 - val_mda: 0.5237 - val_rmse: 0.0087 - learning_rate: 2.5000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.3835e-05 - mae: 0.0026 - mda: 0.5085 - rmse: 0.0768\n",
            "Epoch 18: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.3837e-05 - mae: 0.0026 - mda: 0.5085 - rmse: 0.0768 - val_loss: 6.4235e-05 - val_mae: 0.0060 - val_mda: 0.5235 - val_rmse: 0.0085 - learning_rate: 1.2500e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.4120e-05 - mae: 0.0026 - mda: 0.5088 - rmse: 0.0769\n",
            "Epoch 19: val_loss improved from 0.00003 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.4120e-05 - mae: 0.0026 - mda: 0.5088 - rmse: 0.0769 - val_loss: 2.3317e-05 - val_mae: 0.0035 - val_mda: 0.5239 - val_rmse: 0.0064 - learning_rate: 1.2500e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.3633e-05 - mae: 0.0026 - mda: 0.5083 - rmse: 0.0768\n",
            "Epoch 20: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.3634e-05 - mae: 0.0026 - mda: 0.5083 - rmse: 0.0768 - val_loss: 4.8930e-05 - val_mae: 0.0051 - val_mda: 0.5236 - val_rmse: 0.0077 - learning_rate: 1.2500e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.4347e-05 - mae: 0.0026 - mda: 0.5073 - rmse: 0.0772\n",
            "Epoch 21: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.4347e-05 - mae: 0.0026 - mda: 0.5073 - rmse: 0.0772 - val_loss: 2.5415e-05 - val_mae: 0.0036 - val_mda: 0.5231 - val_rmse: 0.0066 - learning_rate: 1.2500e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.3706e-05 - mae: 0.0026 - mda: 0.5078 - rmse: 0.0770\n",
            "Epoch 22: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.3708e-05 - mae: 0.0026 - mda: 0.5078 - rmse: 0.0770 - val_loss: 3.6537e-05 - val_mae: 0.0044 - val_mda: 0.5234 - val_rmse: 0.0072 - learning_rate: 1.2500e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.2407e-05 - mae: 0.0025 - mda: 0.5088 - rmse: 0.0768\n",
            "Epoch 23: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 1.2408e-05 - mae: 0.0025 - mda: 0.5088 - rmse: 0.0768 - val_loss: 6.9786e-05 - val_mae: 0.0064 - val_mda: 0.5227 - val_rmse: 0.0088 - learning_rate: 6.2500e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.3143e-05 - mae: 0.0025 - mda: 0.5096 - rmse: 0.0767\n",
            "Epoch 24: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 1.3144e-05 - mae: 0.0025 - mda: 0.5096 - rmse: 0.0767 - val_loss: 3.2776e-05 - val_mae: 0.0041 - val_mda: 0.5225 - val_rmse: 0.0069 - learning_rate: 6.2500e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.2429e-05 - mae: 0.0025 - mda: 0.5088 - rmse: 0.0779\n",
            "Epoch 25: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 1.2429e-05 - mae: 0.0025 - mda: 0.5088 - rmse: 0.0779 - val_loss: 4.2719e-05 - val_mae: 0.0048 - val_mda: 0.5227 - val_rmse: 0.0075 - learning_rate: 6.2500e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.3022e-05 - mae: 0.0025 - mda: 0.5092 - rmse: 0.0766\n",
            "Epoch 26: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.3023e-05 - mae: 0.0025 - mda: 0.5092 - rmse: 0.0766 - val_loss: 3.7497e-05 - val_mae: 0.0045 - val_mda: 0.5226 - val_rmse: 0.0072 - learning_rate: 6.2500e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.2546e-05 - mae: 0.0025 - mda: 0.5087 - rmse: 0.0768\n",
            "Epoch 27: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.2547e-05 - mae: 0.0025 - mda: 0.5087 - rmse: 0.0768 - val_loss: 4.5577e-05 - val_mae: 0.0049 - val_mda: 0.5222 - val_rmse: 0.0076 - learning_rate: 6.2500e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.1958e-05 - mae: 0.0024 - mda: 0.5093 - rmse: 0.0767\n",
            "Epoch 28: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.1959e-05 - mae: 0.0024 - mda: 0.5092 - rmse: 0.0767 - val_loss: 3.3223e-05 - val_mae: 0.0042 - val_mda: 0.5222 - val_rmse: 0.0070 - learning_rate: 3.1250e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.1509e-05 - mae: 0.0024 - mda: 0.5086 - rmse: 0.0764\n",
            "Epoch 29: val_loss improved from 0.00002 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.1512e-05 - mae: 0.0024 - mda: 0.5086 - rmse: 0.0764 - val_loss: 2.0389e-05 - val_mae: 0.0033 - val_mda: 0.5217 - val_rmse: 0.0063 - learning_rate: 3.1250e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.2251e-05 - mae: 0.0024 - mda: 0.5099 - rmse: 0.0769\n",
            "Epoch 30: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.2252e-05 - mae: 0.0024 - mda: 0.5099 - rmse: 0.0769 - val_loss: 3.8971e-05 - val_mae: 0.0046 - val_mda: 0.5213 - val_rmse: 0.0074 - learning_rate: 3.1250e-05\n",
            "Epoch 31/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.2626e-05 - mae: 0.0024 - mda: 0.5084 - rmse: 0.0769\n",
            "Epoch 31: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.2625e-05 - mae: 0.0024 - mda: 0.5084 - rmse: 0.0769 - val_loss: 2.9127e-05 - val_mae: 0.0039 - val_mda: 0.5212 - val_rmse: 0.0068 - learning_rate: 3.1250e-05\n",
            "Epoch 32/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.2458e-05 - mae: 0.0025 - mda: 0.5076 - rmse: 0.0773\n",
            "Epoch 32: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 1.2459e-05 - mae: 0.0025 - mda: 0.5076 - rmse: 0.0773 - val_loss: 3.5858e-05 - val_mae: 0.0044 - val_mda: 0.5213 - val_rmse: 0.0072 - learning_rate: 3.1250e-05\n",
            "Epoch 33/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.2617e-05 - mae: 0.0025 - mda: 0.5092 - rmse: 0.0775\n",
            "Epoch 33: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 1.2614e-05 - mae: 0.0025 - mda: 0.5092 - rmse: 0.0775 - val_loss: 3.3886e-05 - val_mae: 0.0043 - val_mda: 0.5214 - val_rmse: 0.0071 - learning_rate: 1.5625e-05\n",
            "Epoch 34/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.1958e-05 - mae: 0.0024 - mda: 0.5089 - rmse: 0.0763\n",
            "Epoch 34: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - loss: 1.1960e-05 - mae: 0.0024 - mda: 0.5090 - rmse: 0.0763 - val_loss: 3.8385e-05 - val_mae: 0.0045 - val_mda: 0.5215 - val_rmse: 0.0073 - learning_rate: 1.5625e-05\n",
            "Epoch 35/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.2187e-05 - mae: 0.0024 - mda: 0.5091 - rmse: 0.0761\n",
            "Epoch 35: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 1.2188e-05 - mae: 0.0024 - mda: 0.5091 - rmse: 0.0761 - val_loss: 3.5605e-05 - val_mae: 0.0044 - val_mda: 0.5216 - val_rmse: 0.0072 - learning_rate: 1.5625e-05\n",
            "Epoch 36/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.3105e-05 - mae: 0.0024 - mda: 0.5084 - rmse: 0.0767\n",
            "Epoch 36: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.3103e-05 - mae: 0.0024 - mda: 0.5084 - rmse: 0.0767 - val_loss: 3.3574e-05 - val_mae: 0.0042 - val_mda: 0.5216 - val_rmse: 0.0070 - learning_rate: 1.5625e-05\n",
            "Epoch 37/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.2344e-05 - mae: 0.0024 - mda: 0.5073 - rmse: 0.0776\n",
            "Epoch 37: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 37: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 1.2343e-05 - mae: 0.0024 - mda: 0.5073 - rmse: 0.0776 - val_loss: 2.4262e-05 - val_mae: 0.0036 - val_mda: 0.5216 - val_rmse: 0.0065 - learning_rate: 1.5625e-05\n",
            "Epoch 38/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.2249e-05 - mae: 0.0024 - mda: 0.5095 - rmse: 0.0779\n",
            "Epoch 38: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.2249e-05 - mae: 0.0024 - mda: 0.5095 - rmse: 0.0779 - val_loss: 2.9418e-05 - val_mae: 0.0039 - val_mda: 0.5213 - val_rmse: 0.0068 - learning_rate: 7.8125e-06\n",
            "Epoch 39/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.2077e-05 - mae: 0.0024 - mda: 0.5105 - rmse: 0.0776\n",
            "Epoch 39: val_loss did not improve from 0.00002\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.2077e-05 - mae: 0.0024 - mda: 0.5105 - rmse: 0.0776 - val_loss: 3.3455e-05 - val_mae: 0.0042 - val_mda: 0.5213 - val_rmse: 0.0071 - learning_rate: 7.8125e-06\n",
            "Epoch 39: early stopping\n",
            "Restoring model weights from the end of the best epoch: 29.\n",
            "Validation Loss: 0.00002, RMSE: 0.00634, MDA: 0.52565, MAE: 0.00333\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2601e-04 - mae: 0.0122 - mda: 0.5074 - rmse: 0.0771\n",
            "Epoch 1: val_loss improved from inf to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 5.2356e-04 - mae: 0.0122 - mda: 0.5074 - rmse: 0.0771 - val_loss: 4.3417e-05 - val_mae: 0.0048 - val_mda: 0.5237 - val_rmse: 0.0067 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m573/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.7311e-05 - mae: 0.0056 - mda: 0.5070 - rmse: 0.0752\n",
            "Epoch 2: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.7391e-05 - mae: 0.0056 - mda: 0.5070 - rmse: 0.0752 - val_loss: 3.7098e-04 - val_mae: 0.0115 - val_mda: 0.5255 - val_rmse: 0.0132 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3030e-05 - mae: 0.0058 - mda: 0.5071 - rmse: 0.0758\n",
            "Epoch 3: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.3056e-05 - mae: 0.0058 - mda: 0.5071 - rmse: 0.0758 - val_loss: 2.1868e-04 - val_mae: 0.0094 - val_mda: 0.5258 - val_rmse: 0.0113 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.8055e-05 - mae: 0.0066 - mda: 0.5078 - rmse: 0.0767\n",
            "Epoch 4: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 7.7943e-05 - mae: 0.0066 - mda: 0.5077 - rmse: 0.0767 - val_loss: 2.4397e-04 - val_mae: 0.0142 - val_mda: 0.5245 - val_rmse: 0.0156 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4576e-05 - mae: 0.0060 - mda: 0.5096 - rmse: 0.0764\n",
            "Epoch 5: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.4536e-05 - mae: 0.0060 - mda: 0.5096 - rmse: 0.0764 - val_loss: 5.2380e-04 - val_mae: 0.0127 - val_mda: 0.5245 - val_rmse: 0.0147 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3788e-05 - mae: 0.0059 - mda: 0.5078 - rmse: 0.0765\n",
            "Epoch 6: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.3791e-05 - mae: 0.0059 - mda: 0.5078 - rmse: 0.0765 - val_loss: 6.5327e-04 - val_mae: 0.0173 - val_mda: 0.5238 - val_rmse: 0.0188 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m568/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6479e-05 - mae: 0.0049 - mda: 0.5074 - rmse: 0.0758\n",
            "Epoch 7: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 4.6468e-05 - mae: 0.0049 - mda: 0.5074 - rmse: 0.0758 - val_loss: 3.0413e-04 - val_mae: 0.0100 - val_mda: 0.5252 - val_rmse: 0.0121 - learning_rate: 0.0050\n",
            "Epoch 8/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.3174e-05 - mae: 0.0052 - mda: 0.5072 - rmse: 0.0769\n",
            "Epoch 8: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.3153e-05 - mae: 0.0052 - mda: 0.5072 - rmse: 0.0769 - val_loss: 8.6154e-04 - val_mae: 0.0167 - val_mda: 0.5256 - val_rmse: 0.0185 - learning_rate: 0.0050\n",
            "Epoch 9/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3728e-05 - mae: 0.0048 - mda: 0.5086 - rmse: 0.0771\n",
            "Epoch 9: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 4.3747e-05 - mae: 0.0048 - mda: 0.5086 - rmse: 0.0771 - val_loss: 4.9883e-04 - val_mae: 0.0122 - val_mda: 0.5259 - val_rmse: 0.0142 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.6484e-05 - mae: 0.0055 - mda: 0.5078 - rmse: 0.0771\n",
            "Epoch 10: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.6439e-05 - mae: 0.0055 - mda: 0.5077 - rmse: 0.0771 - val_loss: 6.6175e-04 - val_mae: 0.0166 - val_mda: 0.5249 - val_rmse: 0.0181 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7996e-05 - mae: 0.0051 - mda: 0.5097 - rmse: 0.0762\n",
            "Epoch 11: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 4.8022e-05 - mae: 0.0051 - mda: 0.5096 - rmse: 0.0762 - val_loss: 5.1853e-04 - val_mae: 0.0124 - val_mda: 0.5248 - val_rmse: 0.0145 - learning_rate: 0.0050\n",
            "Epoch 11: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "Validation Loss: 0.00004, RMSE: 0.00670, MDA: 0.52593, MAE: 0.00476\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0022 - mae: 0.0167 - mda: 0.5077 - rmse: 0.0805\n",
            "Epoch 1: val_loss improved from inf to 0.00013, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0022 - mae: 0.0165 - mda: 0.5077 - rmse: 0.0805 - val_loss: 1.3428e-04 - val_mae: 0.0079 - val_mda: 0.5232 - val_rmse: 0.0095 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3235e-05 - mae: 0.0053 - mda: 0.5075 - rmse: 0.0767\n",
            "Epoch 2: val_loss did not improve from 0.00013\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.3215e-05 - mae: 0.0053 - mda: 0.5075 - rmse: 0.0767 - val_loss: 2.1175e-04 - val_mae: 0.0107 - val_mda: 0.5243 - val_rmse: 0.0122 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.5112e-05 - mae: 0.0049 - mda: 0.5088 - rmse: 0.0764\n",
            "Epoch 3: val_loss improved from 0.00013 to 0.00005, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.5102e-05 - mae: 0.0049 - mda: 0.5088 - rmse: 0.0764 - val_loss: 5.0030e-05 - val_mae: 0.0058 - val_mda: 0.5251 - val_rmse: 0.0078 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6765e-05 - mae: 0.0050 - mda: 0.5081 - rmse: 0.0769\n",
            "Epoch 4: val_loss did not improve from 0.00005\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.6763e-05 - mae: 0.0050 - mda: 0.5081 - rmse: 0.0768 - val_loss: 1.4045e-04 - val_mae: 0.0077 - val_mda: 0.5261 - val_rmse: 0.0096 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1214e-05 - mae: 0.0048 - mda: 0.5089 - rmse: 0.0773\n",
            "Epoch 5: val_loss improved from 0.00005 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.1220e-05 - mae: 0.0048 - mda: 0.5089 - rmse: 0.0773 - val_loss: 3.8415e-05 - val_mae: 0.0044 - val_mda: 0.5255 - val_rmse: 0.0067 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6059e-05 - mae: 0.0050 - mda: 0.5092 - rmse: 0.0768\n",
            "Epoch 6: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.6024e-05 - mae: 0.0050 - mda: 0.5092 - rmse: 0.0768 - val_loss: 4.3900e-04 - val_mae: 0.0147 - val_mda: 0.5260 - val_rmse: 0.0163 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2230e-05 - mae: 0.0041 - mda: 0.5088 - rmse: 0.0768\n",
            "Epoch 7: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.2226e-05 - mae: 0.0041 - mda: 0.5088 - rmse: 0.0768 - val_loss: 9.0318e-05 - val_mae: 0.0060 - val_mda: 0.5258 - val_rmse: 0.0083 - learning_rate: 0.0050\n",
            "Epoch 8/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0568e-05 - mae: 0.0040 - mda: 0.5104 - rmse: 0.0776\n",
            "Epoch 8: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.0578e-05 - mae: 0.0040 - mda: 0.5104 - rmse: 0.0776 - val_loss: 1.3069e-04 - val_mae: 0.0073 - val_mda: 0.5230 - val_rmse: 0.0095 - learning_rate: 0.0050\n",
            "Epoch 9/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5135e-05 - mae: 0.0043 - mda: 0.5088 - rmse: 0.0766\n",
            "Epoch 9: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.5129e-05 - mae: 0.0043 - mda: 0.5088 - rmse: 0.0766 - val_loss: 1.0779e-04 - val_mae: 0.0064 - val_mda: 0.5233 - val_rmse: 0.0088 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3870e-05 - mae: 0.0042 - mda: 0.5089 - rmse: 0.0764\n",
            "Epoch 10: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.3880e-05 - mae: 0.0042 - mda: 0.5089 - rmse: 0.0764 - val_loss: 4.3419e-05 - val_mae: 0.0056 - val_mda: 0.5222 - val_rmse: 0.0080 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1263e-05 - mae: 0.0047 - mda: 0.5084 - rmse: 0.0774\n",
            "Epoch 11: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.1247e-05 - mae: 0.0047 - mda: 0.5084 - rmse: 0.0774 - val_loss: 6.9934e-05 - val_mae: 0.0056 - val_mda: 0.5212 - val_rmse: 0.0082 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0651e-05 - mae: 0.0040 - mda: 0.5093 - rmse: 0.0765\n",
            "Epoch 12: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.0643e-05 - mae: 0.0040 - mda: 0.5093 - rmse: 0.0765 - val_loss: 1.1331e-04 - val_mae: 0.0097 - val_mda: 0.5226 - val_rmse: 0.0116 - learning_rate: 0.0025\n",
            "Epoch 13/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1869e-05 - mae: 0.0041 - mda: 0.5071 - rmse: 0.0768\n",
            "Epoch 13: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.1860e-05 - mae: 0.0041 - mda: 0.5071 - rmse: 0.0768 - val_loss: 3.5813e-04 - val_mae: 0.0142 - val_mda: 0.5206 - val_rmse: 0.0158 - learning_rate: 0.0025\n",
            "Epoch 14/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7092e-05 - mae: 0.0038 - mda: 0.5070 - rmse: 0.0768\n",
            "Epoch 14: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.7101e-05 - mae: 0.0038 - mda: 0.5070 - rmse: 0.0768 - val_loss: 1.4741e-04 - val_mae: 0.0072 - val_mda: 0.5201 - val_rmse: 0.0097 - learning_rate: 0.0025\n",
            "Epoch 15/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9611e-05 - mae: 0.0040 - mda: 0.5092 - rmse: 0.0779\n",
            "Epoch 15: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.9590e-05 - mae: 0.0040 - mda: 0.5092 - rmse: 0.0778 - val_loss: 1.1688e-04 - val_mae: 0.0077 - val_mda: 0.5197 - val_rmse: 0.0099 - learning_rate: 0.0025\n",
            "Epoch 15: early stopping\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "Validation Loss: 0.00004, RMSE: 0.00675, MDA: 0.52615, MAE: 0.00442\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0033 - mae: 0.0174 - mda: 0.5077 - rmse: 0.0820\n",
            "Epoch 1: val_loss improved from inf to 0.00012, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 0.0033 - mae: 0.0173 - mda: 0.5077 - rmse: 0.0820 - val_loss: 1.2091e-04 - val_mae: 0.0073 - val_mda: 0.5236 - val_rmse: 0.0091 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.9308e-05 - mae: 0.0045 - mda: 0.5081 - rmse: 0.0768\n",
            "Epoch 2: val_loss improved from 0.00012 to 0.00009, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 3.9293e-05 - mae: 0.0045 - mda: 0.5081 - rmse: 0.0768 - val_loss: 9.0480e-05 - val_mae: 0.0073 - val_mda: 0.5242 - val_rmse: 0.0092 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.1732e-05 - mae: 0.0041 - mda: 0.5073 - rmse: 0.0774\n",
            "Epoch 3: val_loss improved from 0.00009 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 3.1732e-05 - mae: 0.0041 - mda: 0.5073 - rmse: 0.0774 - val_loss: 3.9190e-05 - val_mae: 0.0047 - val_mda: 0.5249 - val_rmse: 0.0070 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.9960e-05 - mae: 0.0040 - mda: 0.5098 - rmse: 0.0766\n",
            "Epoch 4: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.9964e-05 - mae: 0.0040 - mda: 0.5098 - rmse: 0.0766 - val_loss: 4.6613e-05 - val_mae: 0.0048 - val_mda: 0.5250 - val_rmse: 0.0072 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.4651e-05 - mae: 0.0043 - mda: 0.5083 - rmse: 0.0771\n",
            "Epoch 5: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 3.4653e-05 - mae: 0.0043 - mda: 0.5083 - rmse: 0.0771 - val_loss: 2.8720e-04 - val_mae: 0.0147 - val_mda: 0.5248 - val_rmse: 0.0160 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.3402e-05 - mae: 0.0043 - mda: 0.5115 - rmse: 0.0766\n",
            "Epoch 6: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 3.3405e-05 - mae: 0.0043 - mda: 0.5115 - rmse: 0.0766 - val_loss: 8.8304e-05 - val_mae: 0.0065 - val_mda: 0.5226 - val_rmse: 0.0088 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.1436e-05 - mae: 0.0034 - mda: 0.5092 - rmse: 0.0766\n",
            "Epoch 7: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.1434e-05 - mae: 0.0034 - mda: 0.5092 - rmse: 0.0766 - val_loss: 8.3726e-05 - val_mae: 0.0060 - val_mda: 0.5231 - val_rmse: 0.0084 - learning_rate: 0.0050\n",
            "Epoch 8/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.3482e-05 - mae: 0.0035 - mda: 0.5084 - rmse: 0.0763\n",
            "Epoch 8: val_loss improved from 0.00004 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.3482e-05 - mae: 0.0035 - mda: 0.5084 - rmse: 0.0763 - val_loss: 3.7281e-05 - val_mae: 0.0042 - val_mda: 0.5238 - val_rmse: 0.0069 - learning_rate: 0.0050\n",
            "Epoch 9/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.3905e-05 - mae: 0.0035 - mda: 0.5102 - rmse: 0.0765\n",
            "Epoch 9: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.3909e-05 - mae: 0.0035 - mda: 0.5102 - rmse: 0.0765 - val_loss: 3.1145e-04 - val_mae: 0.0135 - val_mda: 0.5245 - val_rmse: 0.0151 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.7146e-05 - mae: 0.0038 - mda: 0.5077 - rmse: 0.0768\n",
            "Epoch 10: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.7150e-05 - mae: 0.0038 - mda: 0.5077 - rmse: 0.0768 - val_loss: 4.8405e-05 - val_mae: 0.0060 - val_mda: 0.5217 - val_rmse: 0.0086 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.7078e-05 - mae: 0.0038 - mda: 0.5074 - rmse: 0.0762\n",
            "Epoch 11: val_loss improved from 0.00004 to 0.00003, saving model to best_model.keras\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.7081e-05 - mae: 0.0038 - mda: 0.5074 - rmse: 0.0762 - val_loss: 3.1333e-05 - val_mae: 0.0044 - val_mda: 0.5231 - val_rmse: 0.0072 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.0479e-05 - mae: 0.0033 - mda: 0.5082 - rmse: 0.0770\n",
            "Epoch 12: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.0479e-05 - mae: 0.0033 - mda: 0.5082 - rmse: 0.0770 - val_loss: 1.5923e-04 - val_mae: 0.0089 - val_mda: 0.5217 - val_rmse: 0.0110 - learning_rate: 0.0025\n",
            "Epoch 13/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.1265e-05 - mae: 0.0033 - mda: 0.5098 - rmse: 0.0763\n",
            "Epoch 13: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.1262e-05 - mae: 0.0033 - mda: 0.5098 - rmse: 0.0763 - val_loss: 1.3227e-04 - val_mae: 0.0081 - val_mda: 0.5220 - val_rmse: 0.0103 - learning_rate: 0.0025\n",
            "Epoch 14/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.1680e-05 - mae: 0.0033 - mda: 0.5090 - rmse: 0.0761\n",
            "Epoch 14: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.1678e-05 - mae: 0.0033 - mda: 0.5090 - rmse: 0.0761 - val_loss: 9.8482e-05 - val_mae: 0.0061 - val_mda: 0.5209 - val_rmse: 0.0087 - learning_rate: 0.0025\n",
            "Epoch 15/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.0461e-05 - mae: 0.0032 - mda: 0.5095 - rmse: 0.0763\n",
            "Epoch 15: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.0463e-05 - mae: 0.0032 - mda: 0.5095 - rmse: 0.0763 - val_loss: 1.1525e-04 - val_mae: 0.0064 - val_mda: 0.5229 - val_rmse: 0.0090 - learning_rate: 0.0025\n",
            "Epoch 16/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.3610e-05 - mae: 0.0035 - mda: 0.5089 - rmse: 0.0763\n",
            "Epoch 16: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.3609e-05 - mae: 0.0035 - mda: 0.5089 - rmse: 0.0763 - val_loss: 2.5021e-04 - val_mae: 0.0105 - val_mda: 0.5210 - val_rmse: 0.0126 - learning_rate: 0.0025\n",
            "Epoch 17/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.8483e-05 - mae: 0.0031 - mda: 0.5098 - rmse: 0.0768\n",
            "Epoch 17: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.8478e-05 - mae: 0.0031 - mda: 0.5098 - rmse: 0.0768 - val_loss: 3.4311e-05 - val_mae: 0.0040 - val_mda: 0.5199 - val_rmse: 0.0070 - learning_rate: 0.0012\n",
            "Epoch 18/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6898e-05 - mae: 0.0029 - mda: 0.5084 - rmse: 0.0766\n",
            "Epoch 18: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.6898e-05 - mae: 0.0029 - mda: 0.5084 - rmse: 0.0766 - val_loss: 6.5050e-05 - val_mae: 0.0052 - val_mda: 0.5192 - val_rmse: 0.0080 - learning_rate: 0.0012\n",
            "Epoch 19/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7068e-05 - mae: 0.0030 - mda: 0.5092 - rmse: 0.0767\n",
            "Epoch 19: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.7074e-05 - mae: 0.0030 - mda: 0.5092 - rmse: 0.0767 - val_loss: 1.6711e-04 - val_mae: 0.0084 - val_mda: 0.5187 - val_rmse: 0.0108 - learning_rate: 0.0012\n",
            "Epoch 20/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7025e-05 - mae: 0.0030 - mda: 0.5089 - rmse: 0.0756\n",
            "Epoch 20: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.7027e-05 - mae: 0.0030 - mda: 0.5089 - rmse: 0.0756 - val_loss: 2.5866e-04 - val_mae: 0.0096 - val_mda: 0.5184 - val_rmse: 0.0119 - learning_rate: 0.0012\n",
            "Epoch 21/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.8645e-05 - mae: 0.0030 - mda: 0.5070 - rmse: 0.0765\n",
            "Epoch 21: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 1.8644e-05 - mae: 0.0030 - mda: 0.5070 - rmse: 0.0765 - val_loss: 5.6892e-05 - val_mae: 0.0047 - val_mda: 0.5199 - val_rmse: 0.0076 - learning_rate: 0.0012\n",
            "Epoch 21: early stopping\n",
            "Restoring model weights from the end of the best epoch: 11.\n",
            "Validation Loss: 0.00003, RMSE: 0.00694, MDA: 0.52503, MAE: 0.00397\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.4088e-04 - mae: 0.0148 - mda: 0.5054 - rmse: 0.0756\n",
            "Epoch 1: val_loss improved from inf to 0.00014, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 7.3249e-04 - mae: 0.0147 - mda: 0.5054 - rmse: 0.0756 - val_loss: 1.3973e-04 - val_mae: 0.0084 - val_mda: 0.5209 - val_rmse: 0.0098 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m572/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.9214e-05 - mae: 0.0065 - mda: 0.5075 - rmse: 0.0771\n",
            "Epoch 2: val_loss improved from 0.00014 to 0.00013, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 8.9115e-05 - mae: 0.0065 - mda: 0.5075 - rmse: 0.0771 - val_loss: 1.2504e-04 - val_mae: 0.0097 - val_mda: 0.5228 - val_rmse: 0.0111 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0661e-05 - mae: 0.0065 - mda: 0.5080 - rmse: 0.0768\n",
            "Epoch 3: val_loss did not improve from 0.00013\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 8.0655e-05 - mae: 0.0065 - mda: 0.5080 - rmse: 0.0768 - val_loss: 4.1324e-04 - val_mae: 0.0145 - val_mda: 0.5220 - val_rmse: 0.0158 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m573/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.6067e-05 - mae: 0.0063 - mda: 0.5073 - rmse: 0.0761\n",
            "Epoch 4: val_loss did not improve from 0.00013\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 7.6100e-05 - mae: 0.0063 - mda: 0.5073 - rmse: 0.0761 - val_loss: 2.7505e-04 - val_mae: 0.0115 - val_mda: 0.5237 - val_rmse: 0.0129 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m572/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8939e-05 - mae: 0.0060 - mda: 0.5063 - rmse: 0.0754\n",
            "Epoch 5: val_loss improved from 0.00013 to 0.00011, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.9002e-05 - mae: 0.0060 - mda: 0.5063 - rmse: 0.0755 - val_loss: 1.0800e-04 - val_mae: 0.0073 - val_mda: 0.5237 - val_rmse: 0.0089 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m572/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2410e-05 - mae: 0.0061 - mda: 0.5069 - rmse: 0.0760\n",
            "Epoch 6: val_loss did not improve from 0.00011\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 7.2412e-05 - mae: 0.0061 - mda: 0.5069 - rmse: 0.0760 - val_loss: 2.1120e-04 - val_mae: 0.0096 - val_mda: 0.5235 - val_rmse: 0.0111 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m568/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4421e-05 - mae: 0.0058 - mda: 0.5081 - rmse: 0.0763\n",
            "Epoch 7: val_loss did not improve from 0.00011\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.4459e-05 - mae: 0.0058 - mda: 0.5080 - rmse: 0.0763 - val_loss: 3.9466e-04 - val_mae: 0.0151 - val_mda: 0.5241 - val_rmse: 0.0164 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5941e-05 - mae: 0.0058 - mda: 0.5072 - rmse: 0.0765\n",
            "Epoch 8: val_loss improved from 0.00011 to 0.00006, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.5923e-05 - mae: 0.0058 - mda: 0.5072 - rmse: 0.0765 - val_loss: 6.3179e-05 - val_mae: 0.0058 - val_mda: 0.5237 - val_rmse: 0.0077 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m568/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5062e-05 - mae: 0.0057 - mda: 0.5077 - rmse: 0.0754\n",
            "Epoch 9: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.5085e-05 - mae: 0.0057 - mda: 0.5077 - rmse: 0.0754 - val_loss: 7.6069e-05 - val_mae: 0.0060 - val_mda: 0.5238 - val_rmse: 0.0078 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m572/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0127e-05 - mae: 0.0056 - mda: 0.5082 - rmse: 0.0759\n",
            "Epoch 10: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.0181e-05 - mae: 0.0056 - mda: 0.5082 - rmse: 0.0759 - val_loss: 2.1143e-04 - val_mae: 0.0100 - val_mda: 0.5242 - val_rmse: 0.0116 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m568/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4411e-05 - mae: 0.0058 - mda: 0.5069 - rmse: 0.0769\n",
            "Epoch 11: val_loss did not improve from 0.00006\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.4357e-05 - mae: 0.0058 - mda: 0.5069 - rmse: 0.0769 - val_loss: 2.7564e-04 - val_mae: 0.0131 - val_mda: 0.5241 - val_rmse: 0.0145 - learning_rate: 5.0000e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.1572e-05 - mae: 0.0056 - mda: 0.5073 - rmse: 0.0765\n",
            "Epoch 12: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.1561e-05 - mae: 0.0056 - mda: 0.5073 - rmse: 0.0765 - val_loss: 3.1866e-04 - val_mae: 0.0134 - val_mda: 0.5247 - val_rmse: 0.0148 - learning_rate: 2.5000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.1022e-05 - mae: 0.0055 - mda: 0.5063 - rmse: 0.0767\n",
            "Epoch 13: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.1017e-05 - mae: 0.0055 - mda: 0.5063 - rmse: 0.0767 - val_loss: 3.7714e-04 - val_mae: 0.0142 - val_mda: 0.5242 - val_rmse: 0.0155 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.8563e-05 - mae: 0.0058 - mda: 0.5064 - rmse: 0.0772\n",
            "Epoch 14: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.8507e-05 - mae: 0.0058 - mda: 0.5064 - rmse: 0.0772 - val_loss: 3.3948e-04 - val_mae: 0.0138 - val_mda: 0.5247 - val_rmse: 0.0152 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.9694e-05 - mae: 0.0055 - mda: 0.5080 - rmse: 0.0765\n",
            "Epoch 15: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.9703e-05 - mae: 0.0055 - mda: 0.5080 - rmse: 0.0765 - val_loss: 1.9085e-04 - val_mae: 0.0101 - val_mda: 0.5250 - val_rmse: 0.0116 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m573/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.9397e-05 - mae: 0.0055 - mda: 0.5064 - rmse: 0.0769\n",
            "Epoch 16: val_loss did not improve from 0.00006\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.9403e-05 - mae: 0.0055 - mda: 0.5064 - rmse: 0.0769 - val_loss: 1.8043e-04 - val_mae: 0.0097 - val_mda: 0.5247 - val_rmse: 0.0112 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m571/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2280e-05 - mae: 0.0054 - mda: 0.5071 - rmse: 0.0759\n",
            "Epoch 17: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.2192e-05 - mae: 0.0054 - mda: 0.5071 - rmse: 0.0759 - val_loss: 9.7360e-05 - val_mae: 0.0067 - val_mda: 0.5254 - val_rmse: 0.0084 - learning_rate: 1.2500e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.7624e-05 - mae: 0.0053 - mda: 0.5075 - rmse: 0.0758\n",
            "Epoch 18: val_loss did not improve from 0.00006\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.7643e-05 - mae: 0.0053 - mda: 0.5075 - rmse: 0.0758 - val_loss: 1.2642e-04 - val_mae: 0.0077 - val_mda: 0.5256 - val_rmse: 0.0094 - learning_rate: 1.2500e-04\n",
            "Epoch 18: early stopping\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "Validation Loss: 0.00006, RMSE: 0.00769, MDA: 0.52565, MAE: 0.00583\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7317e-04 - mae: 0.0081 - mda: 0.5072 - rmse: 0.0763\n",
            "Epoch 1: val_loss improved from inf to 0.00007, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 1.7264e-04 - mae: 0.0080 - mda: 0.5072 - rmse: 0.0763 - val_loss: 6.6184e-05 - val_mae: 0.0064 - val_mda: 0.5221 - val_rmse: 0.0081 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.3886e-05 - mae: 0.0056 - mda: 0.5064 - rmse: 0.0771\n",
            "Epoch 2: val_loss did not improve from 0.00007\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 6.3829e-05 - mae: 0.0056 - mda: 0.5064 - rmse: 0.0771 - val_loss: 1.6715e-04 - val_mae: 0.0101 - val_mda: 0.5235 - val_rmse: 0.0116 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3095e-05 - mae: 0.0052 - mda: 0.5071 - rmse: 0.0767\n",
            "Epoch 3: val_loss did not improve from 0.00007\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.3108e-05 - mae: 0.0052 - mda: 0.5071 - rmse: 0.0767 - val_loss: 6.9385e-05 - val_mae: 0.0059 - val_mda: 0.5239 - val_rmse: 0.0078 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6186e-05 - mae: 0.0049 - mda: 0.5080 - rmse: 0.0758\n",
            "Epoch 4: val_loss did not improve from 0.00007\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.6205e-05 - mae: 0.0049 - mda: 0.5080 - rmse: 0.0758 - val_loss: 1.2932e-04 - val_mae: 0.0085 - val_mda: 0.5251 - val_rmse: 0.0102 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6866e-05 - mae: 0.0050 - mda: 0.5071 - rmse: 0.0774\n",
            "Epoch 5: val_loss improved from 0.00007 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.6864e-05 - mae: 0.0050 - mda: 0.5071 - rmse: 0.0774 - val_loss: 4.0926e-05 - val_mae: 0.0048 - val_mda: 0.5249 - val_rmse: 0.0068 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4884e-05 - mae: 0.0048 - mda: 0.5089 - rmse: 0.0765\n",
            "Epoch 6: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.4909e-05 - mae: 0.0048 - mda: 0.5089 - rmse: 0.0765 - val_loss: 2.6079e-04 - val_mae: 0.0129 - val_mda: 0.5241 - val_rmse: 0.0144 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1625e-05 - mae: 0.0046 - mda: 0.5075 - rmse: 0.0775\n",
            "Epoch 7: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.1595e-05 - mae: 0.0046 - mda: 0.5075 - rmse: 0.0775 - val_loss: 4.6733e-05 - val_mae: 0.0056 - val_mda: 0.5247 - val_rmse: 0.0078 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9391e-05 - mae: 0.0045 - mda: 0.5073 - rmse: 0.0759\n",
            "Epoch 8: val_loss improved from 0.00004 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.9395e-05 - mae: 0.0045 - mda: 0.5074 - rmse: 0.0759 - val_loss: 3.3464e-05 - val_mae: 0.0045 - val_mda: 0.5257 - val_rmse: 0.0067 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0117e-05 - mae: 0.0046 - mda: 0.5087 - rmse: 0.0769\n",
            "Epoch 9: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.0123e-05 - mae: 0.0046 - mda: 0.5087 - rmse: 0.0769 - val_loss: 5.0523e-05 - val_mae: 0.0050 - val_mda: 0.5261 - val_rmse: 0.0072 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7781e-05 - mae: 0.0044 - mda: 0.5067 - rmse: 0.0768\n",
            "Epoch 10: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.7781e-05 - mae: 0.0044 - mda: 0.5067 - rmse: 0.0768 - val_loss: 1.8554e-04 - val_mae: 0.0107 - val_mda: 0.5254 - val_rmse: 0.0124 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7096e-05 - mae: 0.0044 - mda: 0.5073 - rmse: 0.0760\n",
            "Epoch 11: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.7103e-05 - mae: 0.0044 - mda: 0.5073 - rmse: 0.0760 - val_loss: 4.2130e-05 - val_mae: 0.0053 - val_mda: 0.5251 - val_rmse: 0.0075 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m578/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.6705e-05 - mae: 0.0043 - mda: 0.5060 - rmse: 0.0770\n",
            "Epoch 12: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.6695e-05 - mae: 0.0043 - mda: 0.5061 - rmse: 0.0770 - val_loss: 6.7374e-05 - val_mae: 0.0058 - val_mda: 0.5245 - val_rmse: 0.0079 - learning_rate: 2.5000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3615e-05 - mae: 0.0041 - mda: 0.5079 - rmse: 0.0762\n",
            "Epoch 13: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.3621e-05 - mae: 0.0041 - mda: 0.5079 - rmse: 0.0762 - val_loss: 1.0841e-04 - val_mae: 0.0075 - val_mda: 0.5250 - val_rmse: 0.0094 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1428e-05 - mae: 0.0040 - mda: 0.5061 - rmse: 0.0755\n",
            "Epoch 14: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.1450e-05 - mae: 0.0040 - mda: 0.5061 - rmse: 0.0755 - val_loss: 1.3915e-04 - val_mae: 0.0088 - val_mda: 0.5255 - val_rmse: 0.0106 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5995e-05 - mae: 0.0043 - mda: 0.5077 - rmse: 0.0766\n",
            "Epoch 15: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.5994e-05 - mae: 0.0043 - mda: 0.5077 - rmse: 0.0766 - val_loss: 8.6372e-05 - val_mae: 0.0065 - val_mda: 0.5247 - val_rmse: 0.0085 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4943e-05 - mae: 0.0042 - mda: 0.5083 - rmse: 0.0774\n",
            "Epoch 16: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.4949e-05 - mae: 0.0042 - mda: 0.5083 - rmse: 0.0774 - val_loss: 6.0742e-05 - val_mae: 0.0056 - val_mda: 0.5251 - val_rmse: 0.0078 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4035e-05 - mae: 0.0040 - mda: 0.5077 - rmse: 0.0761\n",
            "Epoch 17: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.4042e-05 - mae: 0.0040 - mda: 0.5077 - rmse: 0.0761 - val_loss: 7.7150e-05 - val_mae: 0.0065 - val_mda: 0.5248 - val_rmse: 0.0086 - learning_rate: 1.2500e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4003e-05 - mae: 0.0041 - mda: 0.5070 - rmse: 0.0769\n",
            "Epoch 18: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.4000e-05 - mae: 0.0041 - mda: 0.5070 - rmse: 0.0769 - val_loss: 1.1028e-04 - val_mae: 0.0077 - val_mda: 0.5250 - val_rmse: 0.0096 - learning_rate: 1.2500e-04\n",
            "Epoch 18: early stopping\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "Validation Loss: 0.00003, RMSE: 0.00673, MDA: 0.52607, MAE: 0.00451\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7964e-04 - mae: 0.0075 - mda: 0.5074 - rmse: 0.0764\n",
            "Epoch 1: val_loss improved from inf to 0.00014, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 1.7930e-04 - mae: 0.0075 - mda: 0.5074 - rmse: 0.0764 - val_loss: 1.4156e-04 - val_mae: 0.0088 - val_mda: 0.5246 - val_rmse: 0.0103 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.7238e-05 - mae: 0.0048 - mda: 0.5072 - rmse: 0.0767\n",
            "Epoch 2: val_loss improved from 0.00014 to 0.00008, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 4.7237e-05 - mae: 0.0048 - mda: 0.5072 - rmse: 0.0767 - val_loss: 7.9028e-05 - val_mae: 0.0068 - val_mda: 0.5255 - val_rmse: 0.0086 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.1768e-05 - mae: 0.0046 - mda: 0.5076 - rmse: 0.0771\n",
            "Epoch 3: val_loss did not improve from 0.00008\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 4.1764e-05 - mae: 0.0046 - mda: 0.5076 - rmse: 0.0771 - val_loss: 3.0471e-04 - val_mae: 0.0144 - val_mda: 0.5256 - val_rmse: 0.0157 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.6232e-05 - mae: 0.0043 - mda: 0.5071 - rmse: 0.0773\n",
            "Epoch 4: val_loss improved from 0.00008 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 3.6232e-05 - mae: 0.0043 - mda: 0.5071 - rmse: 0.0773 - val_loss: 3.2339e-05 - val_mae: 0.0044 - val_mda: 0.5264 - val_rmse: 0.0067 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.6380e-05 - mae: 0.0042 - mda: 0.5091 - rmse: 0.0770\n",
            "Epoch 5: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 3.6374e-05 - mae: 0.0042 - mda: 0.5091 - rmse: 0.0770 - val_loss: 5.6821e-05 - val_mae: 0.0052 - val_mda: 0.5248 - val_rmse: 0.0074 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.5750e-05 - mae: 0.0044 - mda: 0.5083 - rmse: 0.0770\n",
            "Epoch 6: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 3.5731e-05 - mae: 0.0044 - mda: 0.5083 - rmse: 0.0770 - val_loss: 3.0070e-04 - val_mae: 0.0138 - val_mda: 0.5237 - val_rmse: 0.0153 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.1691e-05 - mae: 0.0041 - mda: 0.5069 - rmse: 0.0764\n",
            "Epoch 7: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 3.1697e-05 - mae: 0.0041 - mda: 0.5069 - rmse: 0.0764 - val_loss: 1.8216e-04 - val_mae: 0.0105 - val_mda: 0.5244 - val_rmse: 0.0123 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.0380e-05 - mae: 0.0040 - mda: 0.5087 - rmse: 0.0770\n",
            "Epoch 8: val_loss improved from 0.00003 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 3.0383e-05 - mae: 0.0040 - mda: 0.5087 - rmse: 0.0770 - val_loss: 2.7967e-05 - val_mae: 0.0042 - val_mda: 0.5241 - val_rmse: 0.0067 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.2812e-05 - mae: 0.0041 - mda: 0.5068 - rmse: 0.0765\n",
            "Epoch 9: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 3.2813e-05 - mae: 0.0041 - mda: 0.5068 - rmse: 0.0765 - val_loss: 1.7019e-04 - val_mae: 0.0120 - val_mda: 0.5241 - val_rmse: 0.0136 - learning_rate: 0.0010\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.6011e-05 - mae: 0.0037 - mda: 0.5082 - rmse: 0.0767\n",
            "Epoch 10: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.6005e-05 - mae: 0.0037 - mda: 0.5082 - rmse: 0.0767 - val_loss: 4.3047e-05 - val_mae: 0.0055 - val_mda: 0.5246 - val_rmse: 0.0080 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.5062e-05 - mae: 0.0036 - mda: 0.5086 - rmse: 0.0769\n",
            "Epoch 11: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.5061e-05 - mae: 0.0036 - mda: 0.5086 - rmse: 0.0769 - val_loss: 1.5825e-04 - val_mae: 0.0094 - val_mda: 0.5244 - val_rmse: 0.0113 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.4403e-05 - mae: 0.0036 - mda: 0.5080 - rmse: 0.0767\n",
            "Epoch 12: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.4404e-05 - mae: 0.0036 - mda: 0.5080 - rmse: 0.0767 - val_loss: 3.1867e-05 - val_mae: 0.0044 - val_mda: 0.5246 - val_rmse: 0.0069 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.6061e-05 - mae: 0.0036 - mda: 0.5085 - rmse: 0.0765\n",
            "Epoch 13: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.6061e-05 - mae: 0.0036 - mda: 0.5085 - rmse: 0.0765 - val_loss: 5.9583e-05 - val_mae: 0.0053 - val_mda: 0.5254 - val_rmse: 0.0077 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.5112e-05 - mae: 0.0036 - mda: 0.5088 - rmse: 0.0766\n",
            "Epoch 14: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.5112e-05 - mae: 0.0036 - mda: 0.5088 - rmse: 0.0766 - val_loss: 9.6461e-05 - val_mae: 0.0071 - val_mda: 0.5249 - val_rmse: 0.0093 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.3883e-05 - mae: 0.0035 - mda: 0.5085 - rmse: 0.0764\n",
            "Epoch 15: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.3879e-05 - mae: 0.0035 - mda: 0.5085 - rmse: 0.0764 - val_loss: 1.8146e-04 - val_mae: 0.0103 - val_mda: 0.5250 - val_rmse: 0.0121 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.1051e-05 - mae: 0.0033 - mda: 0.5080 - rmse: 0.0770\n",
            "Epoch 16: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.1056e-05 - mae: 0.0033 - mda: 0.5080 - rmse: 0.0770 - val_loss: 6.6134e-05 - val_mae: 0.0058 - val_mda: 0.5245 - val_rmse: 0.0082 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.3607e-05 - mae: 0.0034 - mda: 0.5093 - rmse: 0.0762\n",
            "Epoch 17: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.3605e-05 - mae: 0.0034 - mda: 0.5093 - rmse: 0.0762 - val_loss: 1.1945e-04 - val_mae: 0.0079 - val_mda: 0.5248 - val_rmse: 0.0100 - learning_rate: 2.5000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.2248e-05 - mae: 0.0033 - mda: 0.5090 - rmse: 0.0764\n",
            "Epoch 18: val_loss did not improve from 0.00003\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 2.2246e-05 - mae: 0.0033 - mda: 0.5090 - rmse: 0.0764 - val_loss: 3.2246e-05 - val_mae: 0.0041 - val_mda: 0.5252 - val_rmse: 0.0067 - learning_rate: 2.5000e-04\n",
            "Epoch 18: early stopping\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "Validation Loss: 0.00003, RMSE: 0.00668, MDA: 0.52639, MAE: 0.00407\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m569/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.2484e-04 - mae: 0.0156 - mda: 0.5058 - rmse: 0.0778\n",
            "Epoch 1: val_loss improved from inf to 0.00024, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 9.0829e-04 - mae: 0.0155 - mda: 0.5058 - rmse: 0.0778 - val_loss: 2.3599e-04 - val_mae: 0.0118 - val_mda: 0.5227 - val_rmse: 0.0132 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m572/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.9939e-05 - mae: 0.0074 - mda: 0.5059 - rmse: 0.0760\n",
            "Epoch 2: val_loss improved from 0.00024 to 0.00022, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 9.9871e-05 - mae: 0.0073 - mda: 0.5059 - rmse: 0.0760 - val_loss: 2.1834e-04 - val_mae: 0.0091 - val_mda: 0.5240 - val_rmse: 0.0107 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m572/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.8224e-05 - mae: 0.0068 - mda: 0.5072 - rmse: 0.0765\n",
            "Epoch 3: val_loss improved from 0.00022 to 0.00011, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 8.8237e-05 - mae: 0.0068 - mda: 0.5072 - rmse: 0.0765 - val_loss: 1.1158e-04 - val_mae: 0.0070 - val_mda: 0.5248 - val_rmse: 0.0089 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m574/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.6689e-05 - mae: 0.0072 - mda: 0.5070 - rmse: 0.0763\n",
            "Epoch 4: val_loss did not improve from 0.00011\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 9.6588e-05 - mae: 0.0071 - mda: 0.5070 - rmse: 0.0763 - val_loss: 6.2510e-04 - val_mae: 0.0147 - val_mda: 0.5243 - val_rmse: 0.0163 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m572/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.9719e-05 - mae: 0.0074 - mda: 0.5075 - rmse: 0.0770\n",
            "Epoch 5: val_loss did not improve from 0.00011\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 9.9695e-05 - mae: 0.0074 - mda: 0.5075 - rmse: 0.0769 - val_loss: 5.0228e-04 - val_mae: 0.0146 - val_mda: 0.5241 - val_rmse: 0.0161 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m572/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.3982e-05 - mae: 0.0071 - mda: 0.5061 - rmse: 0.0766\n",
            "Epoch 6: val_loss did not improve from 0.00011\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 9.3975e-05 - mae: 0.0071 - mda: 0.5061 - rmse: 0.0766 - val_loss: 0.0028 - val_mae: 0.0344 - val_mda: 0.5255 - val_rmse: 0.0354 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7441e-05 - mae: 0.0068 - mda: 0.5065 - rmse: 0.0766\n",
            "Epoch 7: val_loss did not improve from 0.00011\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 8.7454e-05 - mae: 0.0068 - mda: 0.5065 - rmse: 0.0766 - val_loss: 0.0011 - val_mae: 0.0180 - val_mda: 0.5254 - val_rmse: 0.0196 - learning_rate: 0.0100\n",
            "Epoch 8/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.0146e-05 - mae: 0.0070 - mda: 0.5064 - rmse: 0.0769\n",
            "Epoch 8: val_loss did not improve from 0.00011\n",
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 9.0147e-05 - mae: 0.0070 - mda: 0.5064 - rmse: 0.0769 - val_loss: 0.0022 - val_mae: 0.0256 - val_mda: 0.5251 - val_rmse: 0.0271 - learning_rate: 0.0100\n",
            "Epoch 9/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2380e-05 - mae: 0.0061 - mda: 0.5087 - rmse: 0.0764\n",
            "Epoch 9: val_loss did not improve from 0.00011\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 7.2397e-05 - mae: 0.0061 - mda: 0.5087 - rmse: 0.0764 - val_loss: 0.0017 - val_mae: 0.0220 - val_mda: 0.5257 - val_rmse: 0.0237 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0848e-05 - mae: 0.0060 - mda: 0.5087 - rmse: 0.0769\n",
            "Epoch 10: val_loss did not improve from 0.00011\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 7.0862e-05 - mae: 0.0060 - mda: 0.5087 - rmse: 0.0769 - val_loss: 9.5894e-04 - val_mae: 0.0163 - val_mda: 0.5248 - val_rmse: 0.0181 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1896e-05 - mae: 0.0061 - mda: 0.5079 - rmse: 0.0768\n",
            "Epoch 11: val_loss did not improve from 0.00011\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 7.1900e-05 - mae: 0.0061 - mda: 0.5079 - rmse: 0.0768 - val_loss: 0.0013 - val_mae: 0.0192 - val_mda: 0.5244 - val_rmse: 0.0210 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2544e-05 - mae: 0.0061 - mda: 0.5075 - rmse: 0.0763\n",
            "Epoch 12: val_loss did not improve from 0.00011\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 7.2547e-05 - mae: 0.0061 - mda: 0.5075 - rmse: 0.0763 - val_loss: 0.0023 - val_mae: 0.0291 - val_mda: 0.5250 - val_rmse: 0.0303 - learning_rate: 0.0050\n",
            "Epoch 13/50\n",
            "\u001b[1m573/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.9286e-05 - mae: 0.0061 - mda: 0.5071 - rmse: 0.0762\n",
            "Epoch 13: val_loss did not improve from 0.00011\n",
            "\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.9387e-05 - mae: 0.0061 - mda: 0.5071 - rmse: 0.0762 - val_loss: 0.0022 - val_mae: 0.0267 - val_mda: 0.5257 - val_rmse: 0.0282 - learning_rate: 0.0050\n",
            "Epoch 13: early stopping\n",
            "Restoring model weights from the end of the best epoch: 3.\n",
            "Validation Loss: 0.00011, RMSE: 0.00887, MDA: 0.52573, MAE: 0.00697\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0017 - mae: 0.0164 - mda: 0.5062 - rmse: 0.0791\n",
            "Epoch 1: val_loss improved from inf to 0.00009, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0016 - mae: 0.0163 - mda: 0.5062 - rmse: 0.0791 - val_loss: 8.6159e-05 - val_mae: 0.0080 - val_mda: 0.5227 - val_rmse: 0.0096 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.2068e-05 - mae: 0.0063 - mda: 0.5079 - rmse: 0.0759\n",
            "Epoch 2: val_loss did not improve from 0.00009\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 7.2071e-05 - mae: 0.0063 - mda: 0.5079 - rmse: 0.0759 - val_loss: 3.9494e-04 - val_mae: 0.0137 - val_mda: 0.5233 - val_rmse: 0.0152 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.1931e-05 - mae: 0.0063 - mda: 0.5071 - rmse: 0.0768\n",
            "Epoch 3: val_loss improved from 0.00009 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 7.1815e-05 - mae: 0.0063 - mda: 0.5071 - rmse: 0.0768 - val_loss: 4.2112e-05 - val_mae: 0.0047 - val_mda: 0.5252 - val_rmse: 0.0068 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.7972e-05 - mae: 0.0058 - mda: 0.5069 - rmse: 0.0762\n",
            "Epoch 4: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.7995e-05 - mae: 0.0058 - mda: 0.5069 - rmse: 0.0762 - val_loss: 1.1449e-04 - val_mae: 0.0068 - val_mda: 0.5255 - val_rmse: 0.0088 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m579/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.2748e-05 - mae: 0.0059 - mda: 0.5062 - rmse: 0.0765\n",
            "Epoch 5: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 6.2745e-05 - mae: 0.0059 - mda: 0.5062 - rmse: 0.0765 - val_loss: 4.0194e-04 - val_mae: 0.0138 - val_mda: 0.5260 - val_rmse: 0.0154 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.7629e-05 - mae: 0.0056 - mda: 0.5076 - rmse: 0.0764\n",
            "Epoch 6: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.7635e-05 - mae: 0.0056 - mda: 0.5076 - rmse: 0.0764 - val_loss: 7.6787e-05 - val_mae: 0.0069 - val_mda: 0.5253 - val_rmse: 0.0090 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m577/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7558e-05 - mae: 0.0051 - mda: 0.5071 - rmse: 0.0760\n",
            "Epoch 7: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.7556e-05 - mae: 0.0051 - mda: 0.5071 - rmse: 0.0760 - val_loss: 1.8209e-04 - val_mae: 0.0080 - val_mda: 0.5256 - val_rmse: 0.0101 - learning_rate: 0.0050\n",
            "Epoch 8/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8685e-05 - mae: 0.0051 - mda: 0.5096 - rmse: 0.0774\n",
            "Epoch 8: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.8710e-05 - mae: 0.0051 - mda: 0.5096 - rmse: 0.0774 - val_loss: 3.0800e-04 - val_mae: 0.0101 - val_mda: 0.5263 - val_rmse: 0.0121 - learning_rate: 0.0050\n",
            "Epoch 9/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.9660e-05 - mae: 0.0051 - mda: 0.5077 - rmse: 0.0761\n",
            "Epoch 9: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.9655e-05 - mae: 0.0051 - mda: 0.5077 - rmse: 0.0761 - val_loss: 5.3451e-04 - val_mae: 0.0156 - val_mda: 0.5259 - val_rmse: 0.0172 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m575/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0445e-05 - mae: 0.0052 - mda: 0.5073 - rmse: 0.0769\n",
            "Epoch 10: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.0468e-05 - mae: 0.0052 - mda: 0.5073 - rmse: 0.0769 - val_loss: 5.3637e-04 - val_mae: 0.0140 - val_mda: 0.5251 - val_rmse: 0.0158 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8761e-05 - mae: 0.0051 - mda: 0.5090 - rmse: 0.0763\n",
            "Epoch 11: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.8765e-05 - mae: 0.0051 - mda: 0.5090 - rmse: 0.0763 - val_loss: 0.0014 - val_mae: 0.0231 - val_mda: 0.5263 - val_rmse: 0.0245 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m576/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8926e-05 - mae: 0.0045 - mda: 0.5079 - rmse: 0.0764\n",
            "Epoch 12: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.8950e-05 - mae: 0.0045 - mda: 0.5079 - rmse: 0.0764 - val_loss: 6.5066e-04 - val_mae: 0.0156 - val_mda: 0.5251 - val_rmse: 0.0174 - learning_rate: 0.0025\n",
            "Epoch 13/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1336e-05 - mae: 0.0046 - mda: 0.5079 - rmse: 0.0768\n",
            "Epoch 13: val_loss did not improve from 0.00004\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.1334e-05 - mae: 0.0046 - mda: 0.5079 - rmse: 0.0768 - val_loss: 4.0153e-04 - val_mae: 0.0106 - val_mda: 0.5250 - val_rmse: 0.0128 - learning_rate: 0.0025\n",
            "Epoch 13: early stopping\n",
            "Restoring model weights from the end of the best epoch: 3.\n",
            "Validation Loss: 0.00004, RMSE: 0.00678, MDA: 0.52628, MAE: 0.00470\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 16, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0030 - mae: 0.0179 - mda: 0.5057 - rmse: 0.0817\n",
            "Epoch 1: val_loss improved from inf to 0.00010, saving model to best_model.keras\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 0.0030 - mae: 0.0179 - mda: 0.5057 - rmse: 0.0816 - val_loss: 1.0500e-04 - val_mae: 0.0068 - val_mda: 0.5222 - val_rmse: 0.0086 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.5093e-05 - mae: 0.0054 - mda: 0.5072 - rmse: 0.0760\n",
            "Epoch 2: val_loss did not improve from 0.00010\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 5.5090e-05 - mae: 0.0054 - mda: 0.5072 - rmse: 0.0760 - val_loss: 1.5935e-04 - val_mae: 0.0086 - val_mda: 0.5233 - val_rmse: 0.0102 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.6854e-05 - mae: 0.0051 - mda: 0.5078 - rmse: 0.0762\n",
            "Epoch 3: val_loss did not improve from 0.00010\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 4.6848e-05 - mae: 0.0051 - mda: 0.5078 - rmse: 0.0762 - val_loss: 1.4264e-04 - val_mae: 0.0087 - val_mda: 0.5251 - val_rmse: 0.0105 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.8596e-05 - mae: 0.0052 - mda: 0.5087 - rmse: 0.0766\n",
            "Epoch 4: val_loss did not improve from 0.00010\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 4.8589e-05 - mae: 0.0052 - mda: 0.5087 - rmse: 0.0766 - val_loss: 8.1396e-04 - val_mae: 0.0228 - val_mda: 0.5252 - val_rmse: 0.0238 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.0760e-05 - mae: 0.0052 - mda: 0.5067 - rmse: 0.0773\n",
            "Epoch 5: val_loss did not improve from 0.00010\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 5.0748e-05 - mae: 0.0052 - mda: 0.5067 - rmse: 0.0773 - val_loss: 6.1777e-04 - val_mae: 0.0200 - val_mda: 0.5252 - val_rmse: 0.0212 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.3607e-05 - mae: 0.0049 - mda: 0.5071 - rmse: 0.0757\n",
            "Epoch 6: val_loss did not improve from 0.00010\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 4.3630e-05 - mae: 0.0049 - mda: 0.5071 - rmse: 0.0757 - val_loss: 2.9385e-04 - val_mae: 0.0115 - val_mda: 0.5245 - val_rmse: 0.0134 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m582/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.3511e-05 - mae: 0.0042 - mda: 0.5072 - rmse: 0.0768\n",
            "Epoch 7: val_loss did not improve from 0.00010\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 3.3509e-05 - mae: 0.0042 - mda: 0.5072 - rmse: 0.0768 - val_loss: 1.7143e-04 - val_mae: 0.0091 - val_mda: 0.5250 - val_rmse: 0.0111 - learning_rate: 0.0050\n",
            "Epoch 8/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.1713e-05 - mae: 0.0041 - mda: 0.5085 - rmse: 0.0755\n",
            "Epoch 8: val_loss did not improve from 0.00010\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 3.1710e-05 - mae: 0.0041 - mda: 0.5085 - rmse: 0.0755 - val_loss: 4.3900e-04 - val_mae: 0.0130 - val_mda: 0.5255 - val_rmse: 0.0147 - learning_rate: 0.0050\n",
            "Epoch 9/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.2507e-05 - mae: 0.0042 - mda: 0.5087 - rmse: 0.0763\n",
            "Epoch 9: val_loss did not improve from 0.00010\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 3.2513e-05 - mae: 0.0042 - mda: 0.5087 - rmse: 0.0763 - val_loss: 3.3648e-04 - val_mae: 0.0126 - val_mda: 0.5243 - val_rmse: 0.0143 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m581/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.5988e-05 - mae: 0.0044 - mda: 0.5073 - rmse: 0.0766\n",
            "Epoch 10: val_loss did not improve from 0.00010\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 3.6000e-05 - mae: 0.0044 - mda: 0.5073 - rmse: 0.0766 - val_loss: 1.6864e-04 - val_mae: 0.0078 - val_mda: 0.5250 - val_rmse: 0.0100 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m580/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.1379e-05 - mae: 0.0047 - mda: 0.5072 - rmse: 0.0762\n",
            "Epoch 11: val_loss did not improve from 0.00010\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m583/583\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - loss: 4.1372e-05 - mae: 0.0047 - mda: 0.5072 - rmse: 0.0762 - val_loss: 1.4405e-04 - val_mae: 0.0071 - val_mda: 0.5238 - val_rmse: 0.0095 - learning_rate: 0.0050\n",
            "Epoch 11: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "Validation Loss: 0.00010, RMSE: 0.00855, MDA: 0.52549, MAE: 0.00679\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.9672e-04 - mae: 0.0127 - mda: 0.5021 - rmse: 0.0775\n",
            "Epoch 1: val_loss improved from inf to 0.00012, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 4.9357e-04 - mae: 0.0127 - mda: 0.5021 - rmse: 0.0775 - val_loss: 1.1724e-04 - val_mae: 0.0083 - val_mda: 0.5193 - val_rmse: 0.0114 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.9641e-05 - mae: 0.0054 - mda: 0.5039 - rmse: 0.0783\n",
            "Epoch 2: val_loss did not improve from 0.00012\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.9614e-05 - mae: 0.0054 - mda: 0.5039 - rmse: 0.0783 - val_loss: 2.0018e-04 - val_mae: 0.0106 - val_mda: 0.5182 - val_rmse: 0.0135 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m282/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.6979e-05 - mae: 0.0051 - mda: 0.5036 - rmse: 0.0776\n",
            "Epoch 3: val_loss did not improve from 0.00012\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 5.6942e-05 - mae: 0.0051 - mda: 0.5036 - rmse: 0.0776 - val_loss: 1.9633e-04 - val_mae: 0.0105 - val_mda: 0.5181 - val_rmse: 0.0135 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m282/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.2483e-05 - mae: 0.0050 - mda: 0.5030 - rmse: 0.0780\n",
            "Epoch 4: val_loss improved from 0.00012 to 0.00008, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 5.2472e-05 - mae: 0.0050 - mda: 0.5030 - rmse: 0.0780 - val_loss: 7.7168e-05 - val_mae: 0.0061 - val_mda: 0.5182 - val_rmse: 0.0098 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m282/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8080e-05 - mae: 0.0049 - mda: 0.5036 - rmse: 0.0782\n",
            "Epoch 5: val_loss did not improve from 0.00008\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 4.8167e-05 - mae: 0.0049 - mda: 0.5036 - rmse: 0.0782 - val_loss: 1.0603e-04 - val_mae: 0.0069 - val_mda: 0.5167 - val_rmse: 0.0105 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6768e-05 - mae: 0.0048 - mda: 0.5032 - rmse: 0.0780\n",
            "Epoch 6: val_loss did not improve from 0.00008\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 4.6763e-05 - mae: 0.0048 - mda: 0.5032 - rmse: 0.0780 - val_loss: 2.8889e-04 - val_mae: 0.0122 - val_mda: 0.5163 - val_rmse: 0.0152 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6278e-05 - mae: 0.0048 - mda: 0.5025 - rmse: 0.0787\n",
            "Epoch 7: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 4.6265e-05 - mae: 0.0048 - mda: 0.5025 - rmse: 0.0787 - val_loss: 1.2880e-04 - val_mae: 0.0079 - val_mda: 0.5159 - val_rmse: 0.0113 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1877e-05 - mae: 0.0046 - mda: 0.5041 - rmse: 0.0792\n",
            "Epoch 8: val_loss improved from 0.00008 to 0.00008, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 4.1877e-05 - mae: 0.0046 - mda: 0.5041 - rmse: 0.0792 - val_loss: 7.6241e-05 - val_mae: 0.0060 - val_mda: 0.5161 - val_rmse: 0.0098 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m282/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0678e-05 - mae: 0.0045 - mda: 0.5024 - rmse: 0.0785\n",
            "Epoch 9: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 4.0719e-05 - mae: 0.0045 - mda: 0.5024 - rmse: 0.0785 - val_loss: 2.2358e-04 - val_mae: 0.0108 - val_mda: 0.5156 - val_rmse: 0.0139 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4035e-05 - mae: 0.0047 - mda: 0.5028 - rmse: 0.0781\n",
            "Epoch 10: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 4.4005e-05 - mae: 0.0047 - mda: 0.5028 - rmse: 0.0781 - val_loss: 1.6184e-04 - val_mae: 0.0098 - val_mda: 0.5154 - val_rmse: 0.0130 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2928e-05 - mae: 0.0046 - mda: 0.5039 - rmse: 0.0788\n",
            "Epoch 11: val_loss did not improve from 0.00008\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 4.2891e-05 - mae: 0.0046 - mda: 0.5039 - rmse: 0.0788 - val_loss: 8.2643e-05 - val_mae: 0.0066 - val_mda: 0.5156 - val_rmse: 0.0103 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1507e-05 - mae: 0.0044 - mda: 0.5030 - rmse: 0.0780\n",
            "Epoch 12: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 4.1483e-05 - mae: 0.0044 - mda: 0.5030 - rmse: 0.0780 - val_loss: 1.2852e-04 - val_mae: 0.0082 - val_mda: 0.5157 - val_rmse: 0.0116 - learning_rate: 2.5000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.3448e-05 - mae: 0.0046 - mda: 0.5030 - rmse: 0.0782\n",
            "Epoch 13: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 4.3412e-05 - mae: 0.0046 - mda: 0.5030 - rmse: 0.0782 - val_loss: 8.3800e-05 - val_mae: 0.0064 - val_mda: 0.5155 - val_rmse: 0.0102 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8793e-05 - mae: 0.0044 - mda: 0.5026 - rmse: 0.0772\n",
            "Epoch 14: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.8811e-05 - mae: 0.0044 - mda: 0.5027 - rmse: 0.0772 - val_loss: 1.6268e-04 - val_mae: 0.0091 - val_mda: 0.5154 - val_rmse: 0.0125 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8126e-05 - mae: 0.0044 - mda: 0.5036 - rmse: 0.0782\n",
            "Epoch 15: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.8126e-05 - mae: 0.0044 - mda: 0.5036 - rmse: 0.0782 - val_loss: 1.1104e-04 - val_mae: 0.0074 - val_mda: 0.5157 - val_rmse: 0.0110 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8471e-05 - mae: 0.0044 - mda: 0.5029 - rmse: 0.0787\n",
            "Epoch 16: val_loss did not improve from 0.00008\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.8483e-05 - mae: 0.0044 - mda: 0.5029 - rmse: 0.0787 - val_loss: 9.7323e-05 - val_mae: 0.0067 - val_mda: 0.5153 - val_rmse: 0.0105 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7840e-05 - mae: 0.0044 - mda: 0.5026 - rmse: 0.0775\n",
            "Epoch 17: val_loss improved from 0.00008 to 0.00007, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.7890e-05 - mae: 0.0044 - mda: 0.5026 - rmse: 0.0775 - val_loss: 6.7122e-05 - val_mae: 0.0057 - val_mda: 0.5152 - val_rmse: 0.0097 - learning_rate: 1.2500e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8674e-05 - mae: 0.0043 - mda: 0.5033 - rmse: 0.0780\n",
            "Epoch 18: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.8673e-05 - mae: 0.0043 - mda: 0.5033 - rmse: 0.0780 - val_loss: 7.2280e-05 - val_mae: 0.0058 - val_mda: 0.5152 - val_rmse: 0.0098 - learning_rate: 1.2500e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0068e-05 - mae: 0.0044 - mda: 0.5028 - rmse: 0.0781\n",
            "Epoch 19: val_loss improved from 0.00007 to 0.00005, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 4.0049e-05 - mae: 0.0044 - mda: 0.5028 - rmse: 0.0781 - val_loss: 4.9661e-05 - val_mae: 0.0050 - val_mda: 0.5152 - val_rmse: 0.0092 - learning_rate: 1.2500e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8613e-05 - mae: 0.0044 - mda: 0.5037 - rmse: 0.0788\n",
            "Epoch 20: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.8609e-05 - mae: 0.0044 - mda: 0.5037 - rmse: 0.0788 - val_loss: 1.2032e-04 - val_mae: 0.0079 - val_mda: 0.5153 - val_rmse: 0.0114 - learning_rate: 1.2500e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7663e-05 - mae: 0.0043 - mda: 0.5041 - rmse: 0.0783\n",
            "Epoch 21: val_loss did not improve from 0.00005\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.7690e-05 - mae: 0.0043 - mda: 0.5041 - rmse: 0.0783 - val_loss: 1.3764e-04 - val_mae: 0.0082 - val_mda: 0.5149 - val_rmse: 0.0117 - learning_rate: 1.2500e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7850e-05 - mae: 0.0043 - mda: 0.5027 - rmse: 0.0782\n",
            "Epoch 22: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.7847e-05 - mae: 0.0043 - mda: 0.5027 - rmse: 0.0782 - val_loss: 9.6858e-05 - val_mae: 0.0068 - val_mda: 0.5152 - val_rmse: 0.0106 - learning_rate: 6.2500e-05\n",
            "Epoch 23/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7653e-05 - mae: 0.0043 - mda: 0.5033 - rmse: 0.0784\n",
            "Epoch 23: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.7638e-05 - mae: 0.0043 - mda: 0.5033 - rmse: 0.0784 - val_loss: 1.1219e-04 - val_mae: 0.0075 - val_mda: 0.5153 - val_rmse: 0.0112 - learning_rate: 6.2500e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9236e-05 - mae: 0.0042 - mda: 0.5041 - rmse: 0.0767\n",
            "Epoch 24: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.9206e-05 - mae: 0.0042 - mda: 0.5040 - rmse: 0.0767 - val_loss: 7.1676e-05 - val_mae: 0.0059 - val_mda: 0.5152 - val_rmse: 0.0099 - learning_rate: 6.2500e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8708e-05 - mae: 0.0043 - mda: 0.5044 - rmse: 0.0785\n",
            "Epoch 25: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.8677e-05 - mae: 0.0043 - mda: 0.5044 - rmse: 0.0785 - val_loss: 8.5488e-05 - val_mae: 0.0065 - val_mda: 0.5150 - val_rmse: 0.0103 - learning_rate: 6.2500e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5707e-05 - mae: 0.0042 - mda: 0.5042 - rmse: 0.0788\n",
            "Epoch 26: val_loss did not improve from 0.00005\n",
            "\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.5724e-05 - mae: 0.0042 - mda: 0.5042 - rmse: 0.0788 - val_loss: 9.3680e-05 - val_mae: 0.0066 - val_mda: 0.5149 - val_rmse: 0.0105 - learning_rate: 6.2500e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.6513e-05 - mae: 0.0043 - mda: 0.5028 - rmse: 0.0775\n",
            "Epoch 27: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.6510e-05 - mae: 0.0043 - mda: 0.5028 - rmse: 0.0775 - val_loss: 9.6041e-05 - val_mae: 0.0067 - val_mda: 0.5148 - val_rmse: 0.0105 - learning_rate: 3.1250e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5983e-05 - mae: 0.0042 - mda: 0.5031 - rmse: 0.0788\n",
            "Epoch 28: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.5984e-05 - mae: 0.0042 - mda: 0.5031 - rmse: 0.0788 - val_loss: 1.3096e-04 - val_mae: 0.0082 - val_mda: 0.5148 - val_rmse: 0.0117 - learning_rate: 3.1250e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4960e-05 - mae: 0.0042 - mda: 0.5038 - rmse: 0.0776\n",
            "Epoch 29: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.4972e-05 - mae: 0.0042 - mda: 0.5038 - rmse: 0.0776 - val_loss: 1.2051e-04 - val_mae: 0.0077 - val_mda: 0.5147 - val_rmse: 0.0114 - learning_rate: 3.1250e-05\n",
            "Epoch 29: early stopping\n",
            "Restoring model weights from the end of the best epoch: 19.\n",
            "Validation Loss: 0.00005, RMSE: 0.00921, MDA: 0.51929, MAE: 0.00499\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.2256e-04 - mae: 0.0108 - mda: 0.5039 - rmse: 0.0774\n",
            "Epoch 1: val_loss improved from inf to 0.00013, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 4.2062e-04 - mae: 0.0107 - mda: 0.5039 - rmse: 0.0774 - val_loss: 1.2603e-04 - val_mae: 0.0078 - val_mda: 0.5184 - val_rmse: 0.0110 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.8232e-05 - mae: 0.0044 - mda: 0.5037 - rmse: 0.0797\n",
            "Epoch 2: val_loss improved from 0.00013 to 0.00010, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.8227e-05 - mae: 0.0044 - mda: 0.5037 - rmse: 0.0797 - val_loss: 9.8580e-05 - val_mae: 0.0085 - val_mda: 0.5176 - val_rmse: 0.0118 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.5281e-05 - mae: 0.0045 - mda: 0.5035 - rmse: 0.0772\n",
            "Epoch 3: val_loss improved from 0.00010 to 0.00007, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.5248e-05 - mae: 0.0045 - mda: 0.5035 - rmse: 0.0772 - val_loss: 6.8340e-05 - val_mae: 0.0058 - val_mda: 0.5161 - val_rmse: 0.0096 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.3912e-05 - mae: 0.0039 - mda: 0.5037 - rmse: 0.0788\n",
            "Epoch 4: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.3916e-05 - mae: 0.0039 - mda: 0.5037 - rmse: 0.0787 - val_loss: 6.9887e-05 - val_mae: 0.0058 - val_mda: 0.5164 - val_rmse: 0.0097 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.6041e-05 - mae: 0.0041 - mda: 0.5035 - rmse: 0.0789\n",
            "Epoch 5: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.6023e-05 - mae: 0.0041 - mda: 0.5036 - rmse: 0.0789 - val_loss: 3.8825e-04 - val_mae: 0.0156 - val_mda: 0.5157 - val_rmse: 0.0183 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.1864e-05 - mae: 0.0039 - mda: 0.5032 - rmse: 0.0775\n",
            "Epoch 6: val_loss did not improve from 0.00007\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.1852e-05 - mae: 0.0039 - mda: 0.5032 - rmse: 0.0775 - val_loss: 9.1047e-05 - val_mae: 0.0068 - val_mda: 0.5153 - val_rmse: 0.0106 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.7992e-05 - mae: 0.0037 - mda: 0.5037 - rmse: 0.0781\n",
            "Epoch 7: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.7985e-05 - mae: 0.0037 - mda: 0.5037 - rmse: 0.0781 - val_loss: 1.0563e-04 - val_mae: 0.0072 - val_mda: 0.5146 - val_rmse: 0.0110 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.6440e-05 - mae: 0.0035 - mda: 0.5034 - rmse: 0.0789\n",
            "Epoch 8: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.6455e-05 - mae: 0.0035 - mda: 0.5034 - rmse: 0.0789 - val_loss: 9.1476e-05 - val_mae: 0.0067 - val_mda: 0.5147 - val_rmse: 0.0106 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.6283e-05 - mae: 0.0035 - mda: 0.5035 - rmse: 0.0782\n",
            "Epoch 9: val_loss did not improve from 0.00007\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.6317e-05 - mae: 0.0035 - mda: 0.5035 - rmse: 0.0782 - val_loss: 3.7003e-04 - val_mae: 0.0157 - val_mda: 0.5141 - val_rmse: 0.0183 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.8830e-05 - mae: 0.0038 - mda: 0.5049 - rmse: 0.0784\n",
            "Epoch 10: val_loss improved from 0.00007 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.8809e-05 - mae: 0.0038 - mda: 0.5049 - rmse: 0.0784 - val_loss: 4.4971e-05 - val_mae: 0.0048 - val_mda: 0.5148 - val_rmse: 0.0092 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.5965e-05 - mae: 0.0036 - mda: 0.5041 - rmse: 0.0784\n",
            "Epoch 11: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.5967e-05 - mae: 0.0036 - mda: 0.5041 - rmse: 0.0784 - val_loss: 4.8400e-05 - val_mae: 0.0049 - val_mda: 0.5140 - val_rmse: 0.0093 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.5006e-05 - mae: 0.0035 - mda: 0.5042 - rmse: 0.0785\n",
            "Epoch 12: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.5020e-05 - mae: 0.0035 - mda: 0.5042 - rmse: 0.0785 - val_loss: 4.7288e-05 - val_mae: 0.0048 - val_mda: 0.5139 - val_rmse: 0.0093 - learning_rate: 2.5000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.4653e-05 - mae: 0.0035 - mda: 0.5033 - rmse: 0.0779\n",
            "Epoch 13: val_loss improved from 0.00004 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.4652e-05 - mae: 0.0035 - mda: 0.5033 - rmse: 0.0779 - val_loss: 3.0122e-05 - val_mae: 0.0041 - val_mda: 0.5141 - val_rmse: 0.0087 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.4710e-05 - mae: 0.0034 - mda: 0.5034 - rmse: 0.0786\n",
            "Epoch 14: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.4718e-05 - mae: 0.0034 - mda: 0.5034 - rmse: 0.0786 - val_loss: 3.1845e-05 - val_mae: 0.0043 - val_mda: 0.5142 - val_rmse: 0.0090 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.5709e-05 - mae: 0.0034 - mda: 0.5034 - rmse: 0.0781\n",
            "Epoch 15: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.5698e-05 - mae: 0.0034 - mda: 0.5034 - rmse: 0.0781 - val_loss: 7.2338e-05 - val_mae: 0.0063 - val_mda: 0.5138 - val_rmse: 0.0104 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.3845e-05 - mae: 0.0034 - mda: 0.5039 - rmse: 0.0781\n",
            "Epoch 16: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.3848e-05 - mae: 0.0034 - mda: 0.5039 - rmse: 0.0781 - val_loss: 9.7163e-05 - val_mae: 0.0071 - val_mda: 0.5133 - val_rmse: 0.0111 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.2653e-05 - mae: 0.0033 - mda: 0.5041 - rmse: 0.0791\n",
            "Epoch 17: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.2643e-05 - mae: 0.0033 - mda: 0.5041 - rmse: 0.0791 - val_loss: 4.9685e-05 - val_mae: 0.0051 - val_mda: 0.5131 - val_rmse: 0.0095 - learning_rate: 1.2500e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.2480e-05 - mae: 0.0033 - mda: 0.5025 - rmse: 0.0784\n",
            "Epoch 18: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.2492e-05 - mae: 0.0033 - mda: 0.5025 - rmse: 0.0784 - val_loss: 7.2186e-05 - val_mae: 0.0061 - val_mda: 0.5129 - val_rmse: 0.0103 - learning_rate: 1.2500e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.3719e-05 - mae: 0.0034 - mda: 0.5033 - rmse: 0.0787\n",
            "Epoch 19: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.3716e-05 - mae: 0.0034 - mda: 0.5033 - rmse: 0.0787 - val_loss: 3.4211e-05 - val_mae: 0.0042 - val_mda: 0.5131 - val_rmse: 0.0090 - learning_rate: 1.2500e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.3082e-05 - mae: 0.0033 - mda: 0.5041 - rmse: 0.0780\n",
            "Epoch 20: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.3062e-05 - mae: 0.0033 - mda: 0.5041 - rmse: 0.0780 - val_loss: 3.2958e-05 - val_mae: 0.0042 - val_mda: 0.5132 - val_rmse: 0.0089 - learning_rate: 1.2500e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.3386e-05 - mae: 0.0034 - mda: 0.5035 - rmse: 0.0783\n",
            "Epoch 21: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.3381e-05 - mae: 0.0034 - mda: 0.5035 - rmse: 0.0783 - val_loss: 1.1619e-04 - val_mae: 0.0079 - val_mda: 0.5128 - val_rmse: 0.0117 - learning_rate: 1.2500e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.4149e-05 - mae: 0.0034 - mda: 0.5039 - rmse: 0.0782\n",
            "Epoch 22: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.4140e-05 - mae: 0.0034 - mda: 0.5039 - rmse: 0.0782 - val_loss: 4.9722e-05 - val_mae: 0.0049 - val_mda: 0.5129 - val_rmse: 0.0094 - learning_rate: 6.2500e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.2430e-05 - mae: 0.0033 - mda: 0.5044 - rmse: 0.0782\n",
            "Epoch 23: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.2437e-05 - mae: 0.0033 - mda: 0.5044 - rmse: 0.0782 - val_loss: 5.6222e-05 - val_mae: 0.0052 - val_mda: 0.5131 - val_rmse: 0.0097 - learning_rate: 6.2500e-05\n",
            "Epoch 23: early stopping\n",
            "Restoring model weights from the end of the best epoch: 13.\n",
            "Validation Loss: 0.00003, RMSE: 0.00875, MDA: 0.51843, MAE: 0.00408\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.3003e-04 - mae: 0.0094 - mda: 0.5041 - rmse: 0.0778\n",
            "Epoch 1: val_loss improved from inf to 0.00006, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 4.2895e-04 - mae: 0.0094 - mda: 0.5041 - rmse: 0.0778 - val_loss: 5.8480e-05 - val_mae: 0.0055 - val_mda: 0.5161 - val_rmse: 0.0093 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.0548e-05 - mae: 0.0042 - mda: 0.5044 - rmse: 0.0783\n",
            "Epoch 2: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 4.0538e-05 - mae: 0.0042 - mda: 0.5044 - rmse: 0.0783 - val_loss: 6.9894e-05 - val_mae: 0.0061 - val_mda: 0.5153 - val_rmse: 0.0099 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.7984e-05 - mae: 0.0035 - mda: 0.5032 - rmse: 0.0782\n",
            "Epoch 3: val_loss improved from 0.00006 to 0.00005, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 2.7988e-05 - mae: 0.0035 - mda: 0.5032 - rmse: 0.0782 - val_loss: 4.8988e-05 - val_mae: 0.0050 - val_mda: 0.5139 - val_rmse: 0.0093 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.7439e-05 - mae: 0.0034 - mda: 0.5033 - rmse: 0.0784\n",
            "Epoch 4: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 2.7436e-05 - mae: 0.0034 - mda: 0.5033 - rmse: 0.0784 - val_loss: 5.9609e-05 - val_mae: 0.0063 - val_mda: 0.5140 - val_rmse: 0.0104 - learning_rate: 0.0010\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.5216e-05 - mae: 0.0033 - mda: 0.5033 - rmse: 0.0781\n",
            "Epoch 5: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 2.5216e-05 - mae: 0.0033 - mda: 0.5033 - rmse: 0.0781 - val_loss: 6.3881e-05 - val_mae: 0.0059 - val_mda: 0.5139 - val_rmse: 0.0100 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.3099e-05 - mae: 0.0033 - mda: 0.5033 - rmse: 0.0770\n",
            "Epoch 6: val_loss did not improve from 0.00005\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 2.3106e-05 - mae: 0.0033 - mda: 0.5033 - rmse: 0.0770 - val_loss: 6.9542e-05 - val_mae: 0.0061 - val_mda: 0.5133 - val_rmse: 0.0103 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.7945e-05 - mae: 0.0028 - mda: 0.5042 - rmse: 0.0786\n",
            "Epoch 7: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.7948e-05 - mae: 0.0028 - mda: 0.5041 - rmse: 0.0786 - val_loss: 1.1832e-04 - val_mae: 0.0084 - val_mda: 0.5131 - val_rmse: 0.0122 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.8403e-05 - mae: 0.0029 - mda: 0.5036 - rmse: 0.0781\n",
            "Epoch 8: val_loss improved from 0.00005 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.8399e-05 - mae: 0.0029 - mda: 0.5036 - rmse: 0.0781 - val_loss: 3.8012e-05 - val_mae: 0.0049 - val_mda: 0.5129 - val_rmse: 0.0096 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.7696e-05 - mae: 0.0029 - mda: 0.5040 - rmse: 0.0777\n",
            "Epoch 9: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.7702e-05 - mae: 0.0029 - mda: 0.5040 - rmse: 0.0777 - val_loss: 1.0006e-04 - val_mae: 0.0076 - val_mda: 0.5128 - val_rmse: 0.0115 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.8662e-05 - mae: 0.0030 - mda: 0.5039 - rmse: 0.0782\n",
            "Epoch 10: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.8659e-05 - mae: 0.0030 - mda: 0.5039 - rmse: 0.0782 - val_loss: 7.9887e-05 - val_mae: 0.0066 - val_mda: 0.5129 - val_rmse: 0.0107 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.8149e-05 - mae: 0.0030 - mda: 0.5050 - rmse: 0.0785\n",
            "Epoch 11: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 1.8150e-05 - mae: 0.0030 - mda: 0.5050 - rmse: 0.0785 - val_loss: 9.2088e-05 - val_mae: 0.0073 - val_mda: 0.5127 - val_rmse: 0.0113 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.6919e-05 - mae: 0.0028 - mda: 0.5029 - rmse: 0.0778\n",
            "Epoch 12: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 27ms/step - loss: 1.6917e-05 - mae: 0.0028 - mda: 0.5029 - rmse: 0.0778 - val_loss: 3.8018e-05 - val_mae: 0.0045 - val_mda: 0.5129 - val_rmse: 0.0093 - learning_rate: 2.5000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.5432e-05 - mae: 0.0027 - mda: 0.5039 - rmse: 0.0781\n",
            "Epoch 13: val_loss improved from 0.00004 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.5435e-05 - mae: 0.0027 - mda: 0.5039 - rmse: 0.0781 - val_loss: 2.9565e-05 - val_mae: 0.0043 - val_mda: 0.5126 - val_rmse: 0.0092 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.7343e-05 - mae: 0.0028 - mda: 0.5028 - rmse: 0.0789\n",
            "Epoch 14: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 1.7341e-05 - mae: 0.0028 - mda: 0.5028 - rmse: 0.0789 - val_loss: 4.9853e-05 - val_mae: 0.0051 - val_mda: 0.5116 - val_rmse: 0.0097 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.6371e-05 - mae: 0.0028 - mda: 0.5041 - rmse: 0.0784\n",
            "Epoch 15: val_loss improved from 0.00003 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.6370e-05 - mae: 0.0028 - mda: 0.5041 - rmse: 0.0784 - val_loss: 2.9368e-05 - val_mae: 0.0039 - val_mda: 0.5121 - val_rmse: 0.0089 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.6201e-05 - mae: 0.0028 - mda: 0.5031 - rmse: 0.0785\n",
            "Epoch 16: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 1.6197e-05 - mae: 0.0028 - mda: 0.5031 - rmse: 0.0785 - val_loss: 4.0126e-05 - val_mae: 0.0045 - val_mda: 0.5118 - val_rmse: 0.0093 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.4540e-05 - mae: 0.0026 - mda: 0.5034 - rmse: 0.0783\n",
            "Epoch 17: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 1.4540e-05 - mae: 0.0026 - mda: 0.5034 - rmse: 0.0783 - val_loss: 5.2240e-05 - val_mae: 0.0053 - val_mda: 0.5121 - val_rmse: 0.0098 - learning_rate: 1.2500e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.5957e-05 - mae: 0.0027 - mda: 0.5035 - rmse: 0.0777\n",
            "Epoch 18: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 1.5954e-05 - mae: 0.0027 - mda: 0.5035 - rmse: 0.0777 - val_loss: 4.4368e-05 - val_mae: 0.0048 - val_mda: 0.5121 - val_rmse: 0.0095 - learning_rate: 1.2500e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.5421e-05 - mae: 0.0027 - mda: 0.5043 - rmse: 0.0779\n",
            "Epoch 19: val_loss improved from 0.00003 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 1.5419e-05 - mae: 0.0027 - mda: 0.5043 - rmse: 0.0779 - val_loss: 2.4768e-05 - val_mae: 0.0037 - val_mda: 0.5115 - val_rmse: 0.0088 - learning_rate: 1.2500e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.4852e-05 - mae: 0.0026 - mda: 0.5030 - rmse: 0.0786\n",
            "Epoch 20: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 1.4852e-05 - mae: 0.0026 - mda: 0.5030 - rmse: 0.0786 - val_loss: 2.5850e-05 - val_mae: 0.0039 - val_mda: 0.5124 - val_rmse: 0.0091 - learning_rate: 1.2500e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.4609e-05 - mae: 0.0026 - mda: 0.5035 - rmse: 0.0777\n",
            "Epoch 21: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 1.4612e-05 - mae: 0.0026 - mda: 0.5035 - rmse: 0.0777 - val_loss: 7.3187e-05 - val_mae: 0.0064 - val_mda: 0.5124 - val_rmse: 0.0107 - learning_rate: 1.2500e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.5252e-05 - mae: 0.0027 - mda: 0.5045 - rmse: 0.0786\n",
            "Epoch 22: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 1.5241e-05 - mae: 0.0027 - mda: 0.5045 - rmse: 0.0786 - val_loss: 4.4775e-05 - val_mae: 0.0048 - val_mda: 0.5124 - val_rmse: 0.0096 - learning_rate: 6.2500e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.4426e-05 - mae: 0.0026 - mda: 0.5031 - rmse: 0.0791\n",
            "Epoch 23: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 1.4425e-05 - mae: 0.0026 - mda: 0.5031 - rmse: 0.0791 - val_loss: 3.8100e-05 - val_mae: 0.0044 - val_mda: 0.5124 - val_rmse: 0.0093 - learning_rate: 6.2500e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.3307e-05 - mae: 0.0025 - mda: 0.5038 - rmse: 0.0785\n",
            "Epoch 24: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 1.3310e-05 - mae: 0.0025 - mda: 0.5038 - rmse: 0.0785 - val_loss: 3.8697e-05 - val_mae: 0.0044 - val_mda: 0.5125 - val_rmse: 0.0093 - learning_rate: 6.2500e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.4782e-05 - mae: 0.0026 - mda: 0.5042 - rmse: 0.0790\n",
            "Epoch 25: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.4780e-05 - mae: 0.0026 - mda: 0.5042 - rmse: 0.0790 - val_loss: 4.1413e-05 - val_mae: 0.0046 - val_mda: 0.5124 - val_rmse: 0.0094 - learning_rate: 6.2500e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.5024e-05 - mae: 0.0026 - mda: 0.5036 - rmse: 0.0780\n",
            "Epoch 26: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.5022e-05 - mae: 0.0026 - mda: 0.5036 - rmse: 0.0781 - val_loss: 2.7993e-05 - val_mae: 0.0038 - val_mda: 0.5123 - val_rmse: 0.0089 - learning_rate: 6.2500e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.3422e-05 - mae: 0.0025 - mda: 0.5037 - rmse: 0.0782\n",
            "Epoch 27: val_loss improved from 0.00002 to 0.00002, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.3424e-05 - mae: 0.0025 - mda: 0.5037 - rmse: 0.0782 - val_loss: 2.4347e-05 - val_mae: 0.0036 - val_mda: 0.5122 - val_rmse: 0.0088 - learning_rate: 3.1250e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.2860e-05 - mae: 0.0024 - mda: 0.5038 - rmse: 0.0781\n",
            "Epoch 28: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.2862e-05 - mae: 0.0024 - mda: 0.5038 - rmse: 0.0781 - val_loss: 6.6451e-05 - val_mae: 0.0060 - val_mda: 0.5117 - val_rmse: 0.0105 - learning_rate: 3.1250e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.4039e-05 - mae: 0.0025 - mda: 0.5031 - rmse: 0.0782\n",
            "Epoch 29: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.4037e-05 - mae: 0.0025 - mda: 0.5031 - rmse: 0.0782 - val_loss: 3.5998e-05 - val_mae: 0.0043 - val_mda: 0.5118 - val_rmse: 0.0093 - learning_rate: 3.1250e-05\n",
            "Epoch 30/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.3214e-05 - mae: 0.0025 - mda: 0.5041 - rmse: 0.0786\n",
            "Epoch 30: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.3215e-05 - mae: 0.0025 - mda: 0.5041 - rmse: 0.0786 - val_loss: 3.0312e-05 - val_mae: 0.0040 - val_mda: 0.5118 - val_rmse: 0.0091 - learning_rate: 3.1250e-05\n",
            "Epoch 31/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.3243e-05 - mae: 0.0025 - mda: 0.5032 - rmse: 0.0787\n",
            "Epoch 31: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.3243e-05 - mae: 0.0025 - mda: 0.5032 - rmse: 0.0787 - val_loss: 2.9124e-05 - val_mae: 0.0039 - val_mda: 0.5118 - val_rmse: 0.0090 - learning_rate: 3.1250e-05\n",
            "Epoch 32/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.3205e-05 - mae: 0.0025 - mda: 0.5039 - rmse: 0.0790\n",
            "Epoch 32: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 1.3204e-05 - mae: 0.0025 - mda: 0.5039 - rmse: 0.0790 - val_loss: 4.3926e-05 - val_mae: 0.0048 - val_mda: 0.5115 - val_rmse: 0.0096 - learning_rate: 1.5625e-05\n",
            "Epoch 33/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.3258e-05 - mae: 0.0025 - mda: 0.5035 - rmse: 0.0783\n",
            "Epoch 33: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.3258e-05 - mae: 0.0025 - mda: 0.5035 - rmse: 0.0783 - val_loss: 3.1335e-05 - val_mae: 0.0040 - val_mda: 0.5115 - val_rmse: 0.0091 - learning_rate: 1.5625e-05\n",
            "Epoch 34/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.3480e-05 - mae: 0.0025 - mda: 0.5034 - rmse: 0.0787\n",
            "Epoch 34: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.3479e-05 - mae: 0.0025 - mda: 0.5034 - rmse: 0.0787 - val_loss: 2.9352e-05 - val_mae: 0.0039 - val_mda: 0.5116 - val_rmse: 0.0090 - learning_rate: 1.5625e-05\n",
            "Epoch 35/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.3803e-05 - mae: 0.0025 - mda: 0.5046 - rmse: 0.0783\n",
            "Epoch 35: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.3803e-05 - mae: 0.0025 - mda: 0.5046 - rmse: 0.0783 - val_loss: 3.3924e-05 - val_mae: 0.0042 - val_mda: 0.5113 - val_rmse: 0.0092 - learning_rate: 1.5625e-05\n",
            "Epoch 36/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.3353e-05 - mae: 0.0025 - mda: 0.5041 - rmse: 0.0784\n",
            "Epoch 36: val_loss did not improve from 0.00002\n",
            "\n",
            "Epoch 36: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.3353e-05 - mae: 0.0025 - mda: 0.5041 - rmse: 0.0784 - val_loss: 3.6390e-05 - val_mae: 0.0043 - val_mda: 0.5115 - val_rmse: 0.0093 - learning_rate: 1.5625e-05\n",
            "Epoch 37/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.3660e-05 - mae: 0.0025 - mda: 0.5039 - rmse: 0.0779\n",
            "Epoch 37: val_loss did not improve from 0.00002\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.3659e-05 - mae: 0.0025 - mda: 0.5039 - rmse: 0.0779 - val_loss: 3.8393e-05 - val_mae: 0.0045 - val_mda: 0.5115 - val_rmse: 0.0094 - learning_rate: 7.8125e-06\n",
            "Epoch 37: early stopping\n",
            "Restoring model weights from the end of the best epoch: 27.\n",
            "Validation Loss: 0.00002, RMSE: 0.00880, MDA: 0.51605, MAE: 0.00362\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.5202e-04 - mae: 0.0128 - mda: 0.5024 - rmse: 0.0774\n",
            "Epoch 1: val_loss improved from inf to 0.00023, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.4815e-04 - mae: 0.0127 - mda: 0.5024 - rmse: 0.0774 - val_loss: 2.2639e-04 - val_mae: 0.0096 - val_mda: 0.5130 - val_rmse: 0.0132 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.3487e-05 - mae: 0.0059 - mda: 0.5030 - rmse: 0.0787\n",
            "Epoch 2: val_loss improved from 0.00023 to 0.00008, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.3515e-05 - mae: 0.0059 - mda: 0.5030 - rmse: 0.0787 - val_loss: 8.3120e-05 - val_mae: 0.0065 - val_mda: 0.5137 - val_rmse: 0.0106 - learning_rate: 0.0100\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3785e-05 - mae: 0.0054 - mda: 0.5034 - rmse: 0.0784\n",
            "Epoch 3: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 5.3845e-05 - mae: 0.0054 - mda: 0.5034 - rmse: 0.0784 - val_loss: 1.4378e-04 - val_mae: 0.0085 - val_mda: 0.5132 - val_rmse: 0.0123 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.3698e-05 - mae: 0.0059 - mda: 0.5027 - rmse: 0.0781\n",
            "Epoch 4: val_loss improved from 0.00008 to 0.00006, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.3643e-05 - mae: 0.0059 - mda: 0.5027 - rmse: 0.0781 - val_loss: 5.8396e-05 - val_mae: 0.0064 - val_mda: 0.5124 - val_rmse: 0.0107 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.2697e-05 - mae: 0.0053 - mda: 0.5031 - rmse: 0.0782\n",
            "Epoch 5: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 5.2645e-05 - mae: 0.0053 - mda: 0.5031 - rmse: 0.0782 - val_loss: 1.4086e-04 - val_mae: 0.0070 - val_mda: 0.5129 - val_rmse: 0.0114 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.5966e-05 - mae: 0.0055 - mda: 0.5035 - rmse: 0.0780\n",
            "Epoch 6: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 5.5993e-05 - mae: 0.0055 - mda: 0.5035 - rmse: 0.0780 - val_loss: 2.6202e-04 - val_mae: 0.0106 - val_mda: 0.5115 - val_rmse: 0.0143 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.4570e-05 - mae: 0.0054 - mda: 0.5031 - rmse: 0.0781\n",
            "Epoch 7: val_loss did not improve from 0.00006\n",
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.4561e-05 - mae: 0.0054 - mda: 0.5031 - rmse: 0.0781 - val_loss: 3.5216e-04 - val_mae: 0.0107 - val_mda: 0.5120 - val_rmse: 0.0147 - learning_rate: 0.0100\n",
            "Epoch 8/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1134e-05 - mae: 0.0047 - mda: 0.5038 - rmse: 0.0779\n",
            "Epoch 8: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.1149e-05 - mae: 0.0047 - mda: 0.5038 - rmse: 0.0779 - val_loss: 2.5939e-04 - val_mae: 0.0087 - val_mda: 0.5122 - val_rmse: 0.0131 - learning_rate: 0.0050\n",
            "Epoch 9/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.3986e-05 - mae: 0.0047 - mda: 0.5024 - rmse: 0.0781\n",
            "Epoch 9: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 4.3980e-05 - mae: 0.0047 - mda: 0.5024 - rmse: 0.0781 - val_loss: 4.4927e-04 - val_mae: 0.0113 - val_mda: 0.5114 - val_rmse: 0.0154 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.5741e-05 - mae: 0.0048 - mda: 0.5028 - rmse: 0.0790\n",
            "Epoch 10: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 4.5662e-05 - mae: 0.0048 - mda: 0.5028 - rmse: 0.0789 - val_loss: 4.0467e-04 - val_mae: 0.0119 - val_mda: 0.5123 - val_rmse: 0.0157 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4211e-05 - mae: 0.0048 - mda: 0.5038 - rmse: 0.0782\n",
            "Epoch 11: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 4.4186e-05 - mae: 0.0048 - mda: 0.5038 - rmse: 0.0782 - val_loss: 4.6708e-04 - val_mae: 0.0112 - val_mda: 0.5121 - val_rmse: 0.0155 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6834e-05 - mae: 0.0049 - mda: 0.5035 - rmse: 0.0790\n",
            "Epoch 12: val_loss did not improve from 0.00006\n",
            "\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 4.6800e-05 - mae: 0.0049 - mda: 0.5035 - rmse: 0.0789 - val_loss: 4.0144e-04 - val_mae: 0.0107 - val_mda: 0.5113 - val_rmse: 0.0149 - learning_rate: 0.0050\n",
            "Epoch 13/50\n",
            "\u001b[1m282/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1125e-05 - mae: 0.0046 - mda: 0.5031 - rmse: 0.0785\n",
            "Epoch 13: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 4.1021e-05 - mae: 0.0046 - mda: 0.5031 - rmse: 0.0785 - val_loss: 4.8233e-04 - val_mae: 0.0111 - val_mda: 0.5118 - val_rmse: 0.0155 - learning_rate: 0.0025\n",
            "Epoch 14/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.7314e-05 - mae: 0.0044 - mda: 0.5029 - rmse: 0.0779\n",
            "Epoch 14: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.7351e-05 - mae: 0.0044 - mda: 0.5029 - rmse: 0.0779 - val_loss: 3.6645e-04 - val_mae: 0.0098 - val_mda: 0.5117 - val_rmse: 0.0143 - learning_rate: 0.0025\n",
            "Epoch 14: early stopping\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "Validation Loss: 0.00006, RMSE: 0.01059, MDA: 0.51373, MAE: 0.00642\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0030 - mae: 0.0230 - mda: 0.5027 - rmse: 0.0837\n",
            "Epoch 1: val_loss improved from inf to 0.00022, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 0.0030 - mae: 0.0229 - mda: 0.5027 - rmse: 0.0837 - val_loss: 2.2011e-04 - val_mae: 0.0101 - val_mda: 0.5162 - val_rmse: 0.0134 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.9564e-05 - mae: 0.0050 - mda: 0.5034 - rmse: 0.0779\n",
            "Epoch 2: val_loss did not improve from 0.00022\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.9570e-05 - mae: 0.0050 - mda: 0.5034 - rmse: 0.0779 - val_loss: 3.4863e-04 - val_mae: 0.0140 - val_mda: 0.5150 - val_rmse: 0.0169 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.1842e-05 - mae: 0.0047 - mda: 0.5038 - rmse: 0.0783\n",
            "Epoch 3: val_loss improved from 0.00022 to 0.00010, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.1837e-05 - mae: 0.0047 - mda: 0.5038 - rmse: 0.0783 - val_loss: 9.8751e-05 - val_mae: 0.0081 - val_mda: 0.5150 - val_rmse: 0.0118 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.0449e-05 - mae: 0.0046 - mda: 0.5037 - rmse: 0.0778\n",
            "Epoch 4: val_loss improved from 0.00010 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.0448e-05 - mae: 0.0046 - mda: 0.5037 - rmse: 0.0778 - val_loss: 3.4934e-05 - val_mae: 0.0043 - val_mda: 0.5147 - val_rmse: 0.0088 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.5661e-05 - mae: 0.0044 - mda: 0.5034 - rmse: 0.0792\n",
            "Epoch 5: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 3.5667e-05 - mae: 0.0044 - mda: 0.5034 - rmse: 0.0792 - val_loss: 3.8373e-04 - val_mae: 0.0135 - val_mda: 0.5133 - val_rmse: 0.0166 - learning_rate: 0.0100\n",
            "Epoch 6/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.5061e-05 - mae: 0.0043 - mda: 0.5028 - rmse: 0.0789\n",
            "Epoch 6: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 3.5052e-05 - mae: 0.0043 - mda: 0.5028 - rmse: 0.0789 - val_loss: 9.0105e-05 - val_mae: 0.0067 - val_mda: 0.5132 - val_rmse: 0.0107 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.4371e-05 - mae: 0.0043 - mda: 0.5040 - rmse: 0.0781\n",
            "Epoch 7: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 3.4428e-05 - mae: 0.0043 - mda: 0.5040 - rmse: 0.0781 - val_loss: 3.1358e-04 - val_mae: 0.0128 - val_mda: 0.5128 - val_rmse: 0.0159 - learning_rate: 0.0100\n",
            "Epoch 8/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.2883e-05 - mae: 0.0042 - mda: 0.5037 - rmse: 0.0792\n",
            "Epoch 8: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 3.2884e-05 - mae: 0.0042 - mda: 0.5037 - rmse: 0.0792 - val_loss: 8.0848e-05 - val_mae: 0.0058 - val_mda: 0.5136 - val_rmse: 0.0102 - learning_rate: 0.0100\n",
            "Epoch 9/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.8823e-05 - mae: 0.0038 - mda: 0.5039 - rmse: 0.0789\n",
            "Epoch 9: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 2.8817e-05 - mae: 0.0038 - mda: 0.5039 - rmse: 0.0789 - val_loss: 1.8537e-04 - val_mae: 0.0098 - val_mda: 0.5132 - val_rmse: 0.0133 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.8185e-05 - mae: 0.0038 - mda: 0.5040 - rmse: 0.0784\n",
            "Epoch 10: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.8203e-05 - mae: 0.0038 - mda: 0.5040 - rmse: 0.0784 - val_loss: 4.5897e-05 - val_mae: 0.0046 - val_mda: 0.5130 - val_rmse: 0.0093 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.9704e-05 - mae: 0.0040 - mda: 0.5036 - rmse: 0.0780\n",
            "Epoch 11: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.9691e-05 - mae: 0.0040 - mda: 0.5036 - rmse: 0.0780 - val_loss: 1.1484e-04 - val_mae: 0.0070 - val_mda: 0.5129 - val_rmse: 0.0111 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.7338e-05 - mae: 0.0037 - mda: 0.5025 - rmse: 0.0778\n",
            "Epoch 12: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 2.7331e-05 - mae: 0.0037 - mda: 0.5025 - rmse: 0.0778 - val_loss: 1.0277e-04 - val_mae: 0.0063 - val_mda: 0.5123 - val_rmse: 0.0107 - learning_rate: 0.0050\n",
            "Epoch 13/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.9142e-05 - mae: 0.0039 - mda: 0.5030 - rmse: 0.0783\n",
            "Epoch 13: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.9143e-05 - mae: 0.0039 - mda: 0.5030 - rmse: 0.0783 - val_loss: 4.6461e-04 - val_mae: 0.0155 - val_mda: 0.5113 - val_rmse: 0.0185 - learning_rate: 0.0050\n",
            "Epoch 14/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.4900e-05 - mae: 0.0036 - mda: 0.5038 - rmse: 0.0779\n",
            "Epoch 14: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.4903e-05 - mae: 0.0036 - mda: 0.5038 - rmse: 0.0779 - val_loss: 1.3221e-04 - val_mae: 0.0076 - val_mda: 0.5108 - val_rmse: 0.0117 - learning_rate: 0.0025\n",
            "Epoch 14: early stopping\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "Validation Loss: 0.00003, RMSE: 0.00878, MDA: 0.51618, MAE: 0.00432\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0026 - mae: 0.0230 - mda: 0.5019 - rmse: 0.0839\n",
            "Epoch 1: val_loss improved from inf to 0.00026, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 0.0026 - mae: 0.0230 - mda: 0.5019 - rmse: 0.0839 - val_loss: 2.6192e-04 - val_mae: 0.0103 - val_mda: 0.5137 - val_rmse: 0.0135 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.4364e-05 - mae: 0.0041 - mda: 0.5041 - rmse: 0.0786\n",
            "Epoch 2: val_loss improved from 0.00026 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 3.4366e-05 - mae: 0.0041 - mda: 0.5040 - rmse: 0.0786 - val_loss: 3.5653e-05 - val_mae: 0.0045 - val_mda: 0.5153 - val_rmse: 0.0087 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.2739e-05 - mae: 0.0041 - mda: 0.5035 - rmse: 0.0782\n",
            "Epoch 3: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 3.2740e-05 - mae: 0.0041 - mda: 0.5035 - rmse: 0.0782 - val_loss: 4.8583e-05 - val_mae: 0.0054 - val_mda: 0.5151 - val_rmse: 0.0096 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.4456e-05 - mae: 0.0043 - mda: 0.5039 - rmse: 0.0786\n",
            "Epoch 4: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 3.4433e-05 - mae: 0.0043 - mda: 0.5039 - rmse: 0.0786 - val_loss: 6.7217e-05 - val_mae: 0.0060 - val_mda: 0.5140 - val_rmse: 0.0101 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.6220e-05 - mae: 0.0037 - mda: 0.5036 - rmse: 0.0788\n",
            "Epoch 5: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.6244e-05 - mae: 0.0037 - mda: 0.5036 - rmse: 0.0788 - val_loss: 3.6263e-04 - val_mae: 0.0141 - val_mda: 0.5133 - val_rmse: 0.0170 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.6960e-05 - mae: 0.0037 - mda: 0.5034 - rmse: 0.0787\n",
            "Epoch 6: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 2.6950e-05 - mae: 0.0037 - mda: 0.5034 - rmse: 0.0787 - val_loss: 2.3409e-04 - val_mae: 0.0116 - val_mda: 0.5127 - val_rmse: 0.0148 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.8635e-05 - mae: 0.0038 - mda: 0.5035 - rmse: 0.0789\n",
            "Epoch 7: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 2.8619e-05 - mae: 0.0038 - mda: 0.5035 - rmse: 0.0789 - val_loss: 6.4569e-05 - val_mae: 0.0053 - val_mda: 0.5133 - val_rmse: 0.0098 - learning_rate: 0.0100\n",
            "Epoch 8/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.0210e-05 - mae: 0.0032 - mda: 0.5037 - rmse: 0.0787\n",
            "Epoch 8: val_loss improved from 0.00004 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 2.0214e-05 - mae: 0.0032 - mda: 0.5037 - rmse: 0.0787 - val_loss: 3.0301e-05 - val_mae: 0.0043 - val_mda: 0.5129 - val_rmse: 0.0091 - learning_rate: 0.0050\n",
            "Epoch 9/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.1650e-05 - mae: 0.0033 - mda: 0.5042 - rmse: 0.0794\n",
            "Epoch 9: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 2.1645e-05 - mae: 0.0033 - mda: 0.5042 - rmse: 0.0794 - val_loss: 1.6759e-04 - val_mae: 0.0098 - val_mda: 0.5125 - val_rmse: 0.0133 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.2422e-05 - mae: 0.0034 - mda: 0.5041 - rmse: 0.0786\n",
            "Epoch 10: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.2420e-05 - mae: 0.0034 - mda: 0.5041 - rmse: 0.0786 - val_loss: 3.6341e-05 - val_mae: 0.0049 - val_mda: 0.5128 - val_rmse: 0.0096 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.2321e-05 - mae: 0.0033 - mda: 0.5033 - rmse: 0.0787\n",
            "Epoch 11: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.2327e-05 - mae: 0.0033 - mda: 0.5033 - rmse: 0.0787 - val_loss: 1.1132e-04 - val_mae: 0.0070 - val_mda: 0.5118 - val_rmse: 0.0112 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.1697e-05 - mae: 0.0033 - mda: 0.5038 - rmse: 0.0787\n",
            "Epoch 12: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.1701e-05 - mae: 0.0033 - mda: 0.5038 - rmse: 0.0787 - val_loss: 6.3123e-05 - val_mae: 0.0052 - val_mda: 0.5116 - val_rmse: 0.0099 - learning_rate: 0.0050\n",
            "Epoch 13/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.7793e-05 - mae: 0.0030 - mda: 0.5047 - rmse: 0.0779\n",
            "Epoch 13: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 1.7794e-05 - mae: 0.0030 - mda: 0.5047 - rmse: 0.0780 - val_loss: 5.1873e-05 - val_mae: 0.0061 - val_mda: 0.5114 - val_rmse: 0.0106 - learning_rate: 0.0025\n",
            "Epoch 14/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.7679e-05 - mae: 0.0030 - mda: 0.5030 - rmse: 0.0782\n",
            "Epoch 14: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 1.7678e-05 - mae: 0.0030 - mda: 0.5030 - rmse: 0.0782 - val_loss: 1.0534e-04 - val_mae: 0.0068 - val_mda: 0.5105 - val_rmse: 0.0111 - learning_rate: 0.0025\n",
            "Epoch 15/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.6351e-05 - mae: 0.0029 - mda: 0.5039 - rmse: 0.0786\n",
            "Epoch 15: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 1.6353e-05 - mae: 0.0029 - mda: 0.5039 - rmse: 0.0786 - val_loss: 1.0366e-04 - val_mae: 0.0072 - val_mda: 0.5106 - val_rmse: 0.0114 - learning_rate: 0.0025\n",
            "Epoch 16/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.7190e-05 - mae: 0.0029 - mda: 0.5043 - rmse: 0.0778\n",
            "Epoch 16: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 1.7192e-05 - mae: 0.0029 - mda: 0.5043 - rmse: 0.0778 - val_loss: 1.0756e-04 - val_mae: 0.0073 - val_mda: 0.5098 - val_rmse: 0.0115 - learning_rate: 0.0025\n",
            "Epoch 17/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.7563e-05 - mae: 0.0030 - mda: 0.5035 - rmse: 0.0784\n",
            "Epoch 17: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 1.7564e-05 - mae: 0.0030 - mda: 0.5035 - rmse: 0.0784 - val_loss: 6.7544e-05 - val_mae: 0.0054 - val_mda: 0.5099 - val_rmse: 0.0101 - learning_rate: 0.0025\n",
            "Epoch 18/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.6256e-05 - mae: 0.0028 - mda: 0.5042 - rmse: 0.0790\n",
            "Epoch 18: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 1.6256e-05 - mae: 0.0028 - mda: 0.5042 - rmse: 0.0790 - val_loss: 2.0826e-04 - val_mae: 0.0107 - val_mda: 0.5096 - val_rmse: 0.0142 - learning_rate: 0.0012\n",
            "Epoch 18: early stopping\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "Validation Loss: 0.00003, RMSE: 0.00873, MDA: 0.51526, MAE: 0.00429\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.1475e-04 - mae: 0.0157 - mda: 0.5025 - rmse: 0.0768\n",
            "Epoch 1: val_loss improved from inf to 0.00009, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 7.1174e-04 - mae: 0.0156 - mda: 0.5025 - rmse: 0.0768 - val_loss: 8.6502e-05 - val_mae: 0.0069 - val_mda: 0.5203 - val_rmse: 0.0099 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.7334e-05 - mae: 0.0066 - mda: 0.5033 - rmse: 0.0778\n",
            "Epoch 2: val_loss did not improve from 0.00009\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 9.7193e-05 - mae: 0.0066 - mda: 0.5033 - rmse: 0.0778 - val_loss: 1.0029e-04 - val_mae: 0.0069 - val_mda: 0.5191 - val_rmse: 0.0102 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.8333e-05 - mae: 0.0061 - mda: 0.5033 - rmse: 0.0778\n",
            "Epoch 3: val_loss improved from 0.00009 to 0.00008, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 7.8427e-05 - mae: 0.0061 - mda: 0.5033 - rmse: 0.0778 - val_loss: 8.2951e-05 - val_mae: 0.0063 - val_mda: 0.5180 - val_rmse: 0.0099 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.8144e-05 - mae: 0.0063 - mda: 0.5031 - rmse: 0.0784\n",
            "Epoch 4: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 7.8076e-05 - mae: 0.0062 - mda: 0.5031 - rmse: 0.0784 - val_loss: 4.1841e-04 - val_mae: 0.0149 - val_mda: 0.5171 - val_rmse: 0.0175 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.1293e-05 - mae: 0.0060 - mda: 0.5035 - rmse: 0.0781\n",
            "Epoch 5: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 7.1291e-05 - mae: 0.0060 - mda: 0.5035 - rmse: 0.0781 - val_loss: 4.1237e-04 - val_mae: 0.0150 - val_mda: 0.5171 - val_rmse: 0.0176 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.9321e-05 - mae: 0.0059 - mda: 0.5029 - rmse: 0.0777\n",
            "Epoch 6: val_loss did not improve from 0.00008\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.9385e-05 - mae: 0.0060 - mda: 0.5029 - rmse: 0.0777 - val_loss: 1.1023e-04 - val_mae: 0.0074 - val_mda: 0.5180 - val_rmse: 0.0108 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.2596e-05 - mae: 0.0057 - mda: 0.5029 - rmse: 0.0783\n",
            "Epoch 7: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.2613e-05 - mae: 0.0057 - mda: 0.5029 - rmse: 0.0783 - val_loss: 3.6213e-04 - val_mae: 0.0138 - val_mda: 0.5175 - val_rmse: 0.0165 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.4489e-05 - mae: 0.0057 - mda: 0.5038 - rmse: 0.0787\n",
            "Epoch 8: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.4491e-05 - mae: 0.0057 - mda: 0.5038 - rmse: 0.0787 - val_loss: 2.0492e-04 - val_mae: 0.0105 - val_mda: 0.5171 - val_rmse: 0.0135 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.6984e-05 - mae: 0.0058 - mda: 0.5030 - rmse: 0.0777\n",
            "Epoch 9: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.6990e-05 - mae: 0.0058 - mda: 0.5030 - rmse: 0.0777 - val_loss: 2.8829e-04 - val_mae: 0.0123 - val_mda: 0.5172 - val_rmse: 0.0152 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.6502e-05 - mae: 0.0059 - mda: 0.5030 - rmse: 0.0784\n",
            "Epoch 10: val_loss did not improve from 0.00008\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.6474e-05 - mae: 0.0059 - mda: 0.5030 - rmse: 0.0784 - val_loss: 2.2747e-04 - val_mae: 0.0115 - val_mda: 0.5170 - val_rmse: 0.0144 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.5168e-05 - mae: 0.0058 - mda: 0.5026 - rmse: 0.0781\n",
            "Epoch 11: val_loss improved from 0.00008 to 0.00005, saving model to best_model.keras\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.5187e-05 - mae: 0.0058 - mda: 0.5026 - rmse: 0.0781 - val_loss: 4.7289e-05 - val_mae: 0.0052 - val_mda: 0.5171 - val_rmse: 0.0092 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.8581e-05 - mae: 0.0055 - mda: 0.5025 - rmse: 0.0781\n",
            "Epoch 12: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 5.8632e-05 - mae: 0.0055 - mda: 0.5026 - rmse: 0.0781 - val_loss: 9.7711e-05 - val_mae: 0.0068 - val_mda: 0.5169 - val_rmse: 0.0104 - learning_rate: 2.5000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.0877e-05 - mae: 0.0055 - mda: 0.5031 - rmse: 0.0781\n",
            "Epoch 13: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.0855e-05 - mae: 0.0055 - mda: 0.5031 - rmse: 0.0781 - val_loss: 1.0151e-04 - val_mae: 0.0069 - val_mda: 0.5160 - val_rmse: 0.0104 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m283/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.2010e-05 - mae: 0.0056 - mda: 0.5027 - rmse: 0.0778\n",
            "Epoch 14: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.1972e-05 - mae: 0.0056 - mda: 0.5027 - rmse: 0.0778 - val_loss: 2.2821e-04 - val_mae: 0.0111 - val_mda: 0.5158 - val_rmse: 0.0141 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.0385e-05 - mae: 0.0055 - mda: 0.5023 - rmse: 0.0781\n",
            "Epoch 15: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.0399e-05 - mae: 0.0055 - mda: 0.5023 - rmse: 0.0781 - val_loss: 2.0138e-04 - val_mae: 0.0106 - val_mda: 0.5161 - val_rmse: 0.0137 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.6620e-05 - mae: 0.0057 - mda: 0.5032 - rmse: 0.0784\n",
            "Epoch 16: val_loss did not improve from 0.00005\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.6485e-05 - mae: 0.0057 - mda: 0.5032 - rmse: 0.0784 - val_loss: 8.4128e-05 - val_mae: 0.0062 - val_mda: 0.5162 - val_rmse: 0.0100 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.8369e-05 - mae: 0.0053 - mda: 0.5034 - rmse: 0.0786\n",
            "Epoch 17: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 5.8381e-05 - mae: 0.0053 - mda: 0.5034 - rmse: 0.0786 - val_loss: 1.4475e-04 - val_mae: 0.0083 - val_mda: 0.5161 - val_rmse: 0.0117 - learning_rate: 1.2500e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.7516e-05 - mae: 0.0054 - mda: 0.5024 - rmse: 0.0772\n",
            "Epoch 18: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 5.7550e-05 - mae: 0.0054 - mda: 0.5024 - rmse: 0.0772 - val_loss: 1.8920e-04 - val_mae: 0.0097 - val_mda: 0.5159 - val_rmse: 0.0129 - learning_rate: 1.2500e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.1276e-05 - mae: 0.0055 - mda: 0.5028 - rmse: 0.0791\n",
            "Epoch 19: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.1244e-05 - mae: 0.0055 - mda: 0.5028 - rmse: 0.0791 - val_loss: 6.7929e-05 - val_mae: 0.0057 - val_mda: 0.5161 - val_rmse: 0.0096 - learning_rate: 1.2500e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.8463e-05 - mae: 0.0055 - mda: 0.5035 - rmse: 0.0788\n",
            "Epoch 20: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 5.8490e-05 - mae: 0.0055 - mda: 0.5035 - rmse: 0.0788 - val_loss: 8.3339e-05 - val_mae: 0.0062 - val_mda: 0.5159 - val_rmse: 0.0100 - learning_rate: 1.2500e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.8540e-05 - mae: 0.0054 - mda: 0.5033 - rmse: 0.0770\n",
            "Epoch 21: val_loss did not improve from 0.00005\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 5.8549e-05 - mae: 0.0054 - mda: 0.5033 - rmse: 0.0770 - val_loss: 1.2444e-04 - val_mae: 0.0076 - val_mda: 0.5159 - val_rmse: 0.0111 - learning_rate: 1.2500e-04\n",
            "Epoch 21: early stopping\n",
            "Restoring model weights from the end of the best epoch: 11.\n",
            "Validation Loss: 0.00005, RMSE: 0.00923, MDA: 0.52031, MAE: 0.00525\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.2089e-04 - mae: 0.0135 - mda: 0.5028 - rmse: 0.0780\n",
            "Epoch 1: val_loss improved from inf to 0.00006, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 6.1375e-04 - mae: 0.0134 - mda: 0.5028 - rmse: 0.0781 - val_loss: 6.2639e-05 - val_mae: 0.0058 - val_mda: 0.5175 - val_rmse: 0.0092 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.0286e-05 - mae: 0.0057 - mda: 0.5043 - rmse: 0.0792\n",
            "Epoch 2: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 8.0218e-05 - mae: 0.0057 - mda: 0.5043 - rmse: 0.0791 - val_loss: 1.8052e-04 - val_mae: 0.0094 - val_mda: 0.5169 - val_rmse: 0.0125 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.3993e-05 - mae: 0.0048 - mda: 0.5036 - rmse: 0.0784\n",
            "Epoch 3: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 5.3975e-05 - mae: 0.0048 - mda: 0.5036 - rmse: 0.0784 - val_loss: 1.7261e-04 - val_mae: 0.0088 - val_mda: 0.5173 - val_rmse: 0.0120 - learning_rate: 0.0010\n",
            "Epoch 4/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.2220e-05 - mae: 0.0050 - mda: 0.5035 - rmse: 0.0786\n",
            "Epoch 4: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 5.2177e-05 - mae: 0.0050 - mda: 0.5035 - rmse: 0.0786 - val_loss: 1.3346e-04 - val_mae: 0.0077 - val_mda: 0.5154 - val_rmse: 0.0112 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.6049e-05 - mae: 0.0047 - mda: 0.5025 - rmse: 0.0782\n",
            "Epoch 5: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.6031e-05 - mae: 0.0047 - mda: 0.5025 - rmse: 0.0782 - val_loss: 1.9722e-04 - val_mae: 0.0105 - val_mda: 0.5162 - val_rmse: 0.0137 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.3612e-05 - mae: 0.0047 - mda: 0.5029 - rmse: 0.0784\n",
            "Epoch 6: val_loss improved from 0.00006 to 0.00006, saving model to best_model.keras\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.3616e-05 - mae: 0.0047 - mda: 0.5029 - rmse: 0.0784 - val_loss: 6.1903e-05 - val_mae: 0.0056 - val_mda: 0.5159 - val_rmse: 0.0096 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.5347e-05 - mae: 0.0042 - mda: 0.5018 - rmse: 0.0781\n",
            "Epoch 7: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.5372e-05 - mae: 0.0042 - mda: 0.5018 - rmse: 0.0781 - val_loss: 2.3418e-04 - val_mae: 0.0109 - val_mda: 0.5145 - val_rmse: 0.0140 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.0073e-05 - mae: 0.0044 - mda: 0.5022 - rmse: 0.0780\n",
            "Epoch 8: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.0084e-05 - mae: 0.0044 - mda: 0.5022 - rmse: 0.0780 - val_loss: 1.2413e-04 - val_mae: 0.0075 - val_mda: 0.5144 - val_rmse: 0.0112 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.9974e-05 - mae: 0.0045 - mda: 0.5030 - rmse: 0.0789\n",
            "Epoch 9: val_loss did not improve from 0.00006\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.9982e-05 - mae: 0.0045 - mda: 0.5030 - rmse: 0.0789 - val_loss: 1.1096e-04 - val_mae: 0.0074 - val_mda: 0.5148 - val_rmse: 0.0111 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.8367e-05 - mae: 0.0044 - mda: 0.5024 - rmse: 0.0785\n",
            "Epoch 10: val_loss improved from 0.00006 to 0.00005, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.8384e-05 - mae: 0.0044 - mda: 0.5025 - rmse: 0.0785 - val_loss: 4.8561e-05 - val_mae: 0.0056 - val_mda: 0.5148 - val_rmse: 0.0097 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.9602e-05 - mae: 0.0045 - mda: 0.5029 - rmse: 0.0780\n",
            "Epoch 11: val_loss did not improve from 0.00005\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.9592e-05 - mae: 0.0045 - mda: 0.5030 - rmse: 0.0780 - val_loss: 8.1623e-05 - val_mae: 0.0062 - val_mda: 0.5146 - val_rmse: 0.0102 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.6868e-05 - mae: 0.0043 - mda: 0.5041 - rmse: 0.0784\n",
            "Epoch 12: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.6872e-05 - mae: 0.0043 - mda: 0.5041 - rmse: 0.0784 - val_loss: 2.3984e-04 - val_mae: 0.0115 - val_mda: 0.5150 - val_rmse: 0.0146 - learning_rate: 2.5000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.7375e-05 - mae: 0.0043 - mda: 0.5034 - rmse: 0.0788\n",
            "Epoch 13: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.7338e-05 - mae: 0.0043 - mda: 0.5034 - rmse: 0.0788 - val_loss: 2.0786e-04 - val_mae: 0.0108 - val_mda: 0.5148 - val_rmse: 0.0140 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.7823e-05 - mae: 0.0043 - mda: 0.5026 - rmse: 0.0783\n",
            "Epoch 14: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.7815e-05 - mae: 0.0043 - mda: 0.5026 - rmse: 0.0783 - val_loss: 8.2846e-05 - val_mae: 0.0062 - val_mda: 0.5152 - val_rmse: 0.0102 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.7647e-05 - mae: 0.0043 - mda: 0.5036 - rmse: 0.0785\n",
            "Epoch 15: val_loss improved from 0.00005 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.7633e-05 - mae: 0.0043 - mda: 0.5036 - rmse: 0.0784 - val_loss: 4.3715e-05 - val_mae: 0.0051 - val_mda: 0.5150 - val_rmse: 0.0094 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.7701e-05 - mae: 0.0043 - mda: 0.5036 - rmse: 0.0775\n",
            "Epoch 16: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.7696e-05 - mae: 0.0043 - mda: 0.5036 - rmse: 0.0775 - val_loss: 2.0700e-04 - val_mae: 0.0107 - val_mda: 0.5144 - val_rmse: 0.0140 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.4932e-05 - mae: 0.0041 - mda: 0.5030 - rmse: 0.0788\n",
            "Epoch 17: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.4911e-05 - mae: 0.0041 - mda: 0.5030 - rmse: 0.0788 - val_loss: 8.1925e-05 - val_mae: 0.0064 - val_mda: 0.5145 - val_rmse: 0.0103 - learning_rate: 1.2500e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.3557e-05 - mae: 0.0041 - mda: 0.5021 - rmse: 0.0782\n",
            "Epoch 18: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.3570e-05 - mae: 0.0041 - mda: 0.5021 - rmse: 0.0782 - val_loss: 4.9801e-05 - val_mae: 0.0049 - val_mda: 0.5146 - val_rmse: 0.0093 - learning_rate: 1.2500e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.4506e-05 - mae: 0.0041 - mda: 0.5034 - rmse: 0.0787\n",
            "Epoch 19: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.4509e-05 - mae: 0.0041 - mda: 0.5034 - rmse: 0.0787 - val_loss: 6.9315e-05 - val_mae: 0.0058 - val_mda: 0.5143 - val_rmse: 0.0099 - learning_rate: 1.2500e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.3023e-05 - mae: 0.0041 - mda: 0.5032 - rmse: 0.0785\n",
            "Epoch 20: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.3039e-05 - mae: 0.0041 - mda: 0.5032 - rmse: 0.0785 - val_loss: 8.3708e-05 - val_mae: 0.0065 - val_mda: 0.5142 - val_rmse: 0.0104 - learning_rate: 1.2500e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.3594e-05 - mae: 0.0041 - mda: 0.5029 - rmse: 0.0787\n",
            "Epoch 21: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.3598e-05 - mae: 0.0041 - mda: 0.5029 - rmse: 0.0787 - val_loss: 6.9596e-05 - val_mae: 0.0058 - val_mda: 0.5140 - val_rmse: 0.0099 - learning_rate: 1.2500e-04\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.2245e-05 - mae: 0.0040 - mda: 0.5026 - rmse: 0.0775\n",
            "Epoch 22: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.2243e-05 - mae: 0.0040 - mda: 0.5026 - rmse: 0.0775 - val_loss: 6.9602e-05 - val_mae: 0.0058 - val_mda: 0.5143 - val_rmse: 0.0099 - learning_rate: 6.2500e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.1049e-05 - mae: 0.0039 - mda: 0.5032 - rmse: 0.0800\n",
            "Epoch 23: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.1048e-05 - mae: 0.0039 - mda: 0.5032 - rmse: 0.0800 - val_loss: 5.0906e-05 - val_mae: 0.0050 - val_mda: 0.5141 - val_rmse: 0.0093 - learning_rate: 6.2500e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.1636e-05 - mae: 0.0040 - mda: 0.5017 - rmse: 0.0784\n",
            "Epoch 24: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.1660e-05 - mae: 0.0040 - mda: 0.5017 - rmse: 0.0784 - val_loss: 1.1182e-04 - val_mae: 0.0074 - val_mda: 0.5145 - val_rmse: 0.0112 - learning_rate: 6.2500e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.2800e-05 - mae: 0.0040 - mda: 0.5031 - rmse: 0.0787\n",
            "Epoch 25: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.2805e-05 - mae: 0.0040 - mda: 0.5031 - rmse: 0.0787 - val_loss: 7.0055e-05 - val_mae: 0.0058 - val_mda: 0.5139 - val_rmse: 0.0099 - learning_rate: 6.2500e-05\n",
            "Epoch 25: early stopping\n",
            "Restoring model weights from the end of the best epoch: 15.\n",
            "Validation Loss: 0.00004, RMSE: 0.00923, MDA: 0.51746, MAE: 0.00495\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.5009e-04 - mae: 0.0088 - mda: 0.5038 - rmse: 0.0769\n",
            "Epoch 1: val_loss improved from inf to 0.00011, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 2.4855e-04 - mae: 0.0087 - mda: 0.5038 - rmse: 0.0769 - val_loss: 1.1169e-04 - val_mae: 0.0078 - val_mda: 0.5160 - val_rmse: 0.0112 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.8318e-05 - mae: 0.0045 - mda: 0.5029 - rmse: 0.0780\n",
            "Epoch 2: val_loss did not improve from 0.00011\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 4.8321e-05 - mae: 0.0045 - mda: 0.5029 - rmse: 0.0780 - val_loss: 2.9545e-04 - val_mae: 0.0131 - val_mda: 0.5150 - val_rmse: 0.0160 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.9700e-05 - mae: 0.0043 - mda: 0.5041 - rmse: 0.0785\n",
            "Epoch 3: val_loss did not improve from 0.00011\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 3.9685e-05 - mae: 0.0043 - mda: 0.5041 - rmse: 0.0785 - val_loss: 1.8804e-04 - val_mae: 0.0105 - val_mda: 0.5142 - val_rmse: 0.0137 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.7255e-05 - mae: 0.0042 - mda: 0.5036 - rmse: 0.0792\n",
            "Epoch 4: val_loss improved from 0.00011 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 3.7218e-05 - mae: 0.0042 - mda: 0.5036 - rmse: 0.0792 - val_loss: 3.7088e-05 - val_mae: 0.0044 - val_mda: 0.5132 - val_rmse: 0.0089 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.5350e-05 - mae: 0.0042 - mda: 0.5032 - rmse: 0.0780\n",
            "Epoch 5: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 3.5346e-05 - mae: 0.0042 - mda: 0.5032 - rmse: 0.0780 - val_loss: 6.3569e-05 - val_mae: 0.0057 - val_mda: 0.5133 - val_rmse: 0.0099 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.3114e-05 - mae: 0.0040 - mda: 0.5027 - rmse: 0.0788\n",
            "Epoch 6: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 3.3112e-05 - mae: 0.0040 - mda: 0.5027 - rmse: 0.0788 - val_loss: 1.5017e-04 - val_mae: 0.0109 - val_mda: 0.5130 - val_rmse: 0.0142 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.5790e-05 - mae: 0.0036 - mda: 0.5037 - rmse: 0.0776\n",
            "Epoch 7: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.5790e-05 - mae: 0.0036 - mda: 0.5037 - rmse: 0.0776 - val_loss: 1.1926e-04 - val_mae: 0.0079 - val_mda: 0.5135 - val_rmse: 0.0117 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.7454e-05 - mae: 0.0036 - mda: 0.5034 - rmse: 0.0779\n",
            "Epoch 8: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 2.7436e-05 - mae: 0.0036 - mda: 0.5034 - rmse: 0.0779 - val_loss: 1.2488e-04 - val_mae: 0.0087 - val_mda: 0.5133 - val_rmse: 0.0123 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.5763e-05 - mae: 0.0036 - mda: 0.5050 - rmse: 0.0781\n",
            "Epoch 9: val_loss improved from 0.00004 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.5754e-05 - mae: 0.0036 - mda: 0.5050 - rmse: 0.0781 - val_loss: 3.6498e-05 - val_mae: 0.0048 - val_mda: 0.5134 - val_rmse: 0.0095 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.5620e-05 - mae: 0.0035 - mda: 0.5035 - rmse: 0.0782\n",
            "Epoch 10: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.5619e-05 - mae: 0.0035 - mda: 0.5035 - rmse: 0.0782 - val_loss: 9.5472e-05 - val_mae: 0.0071 - val_mda: 0.5126 - val_rmse: 0.0111 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.3716e-05 - mae: 0.0034 - mda: 0.5041 - rmse: 0.0779\n",
            "Epoch 11: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.3718e-05 - mae: 0.0034 - mda: 0.5041 - rmse: 0.0779 - val_loss: 6.9246e-05 - val_mae: 0.0059 - val_mda: 0.5123 - val_rmse: 0.0102 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.2306e-05 - mae: 0.0033 - mda: 0.5032 - rmse: 0.0788\n",
            "Epoch 12: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.2307e-05 - mae: 0.0033 - mda: 0.5032 - rmse: 0.0788 - val_loss: 5.8497e-05 - val_mae: 0.0054 - val_mda: 0.5126 - val_rmse: 0.0098 - learning_rate: 2.5000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.1989e-05 - mae: 0.0033 - mda: 0.5040 - rmse: 0.0787\n",
            "Epoch 13: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.1988e-05 - mae: 0.0033 - mda: 0.5040 - rmse: 0.0787 - val_loss: 6.0480e-05 - val_mae: 0.0054 - val_mda: 0.5128 - val_rmse: 0.0099 - learning_rate: 2.5000e-04\n",
            "Epoch 14/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.3516e-05 - mae: 0.0034 - mda: 0.5041 - rmse: 0.0778\n",
            "Epoch 14: val_loss improved from 0.00004 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.3514e-05 - mae: 0.0034 - mda: 0.5041 - rmse: 0.0778 - val_loss: 3.0130e-05 - val_mae: 0.0040 - val_mda: 0.5128 - val_rmse: 0.0090 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.2312e-05 - mae: 0.0033 - mda: 0.5035 - rmse: 0.0787\n",
            "Epoch 15: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.2314e-05 - mae: 0.0033 - mda: 0.5035 - rmse: 0.0787 - val_loss: 3.5900e-05 - val_mae: 0.0043 - val_mda: 0.5128 - val_rmse: 0.0091 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.3056e-05 - mae: 0.0034 - mda: 0.5041 - rmse: 0.0781\n",
            "Epoch 16: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.3054e-05 - mae: 0.0034 - mda: 0.5041 - rmse: 0.0781 - val_loss: 3.6193e-05 - val_mae: 0.0043 - val_mda: 0.5127 - val_rmse: 0.0091 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.1450e-05 - mae: 0.0032 - mda: 0.5024 - rmse: 0.0791\n",
            "Epoch 17: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.1447e-05 - mae: 0.0032 - mda: 0.5024 - rmse: 0.0791 - val_loss: 3.7903e-05 - val_mae: 0.0044 - val_mda: 0.5124 - val_rmse: 0.0092 - learning_rate: 1.2500e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.0679e-05 - mae: 0.0032 - mda: 0.5036 - rmse: 0.0792\n",
            "Epoch 18: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.0678e-05 - mae: 0.0032 - mda: 0.5036 - rmse: 0.0792 - val_loss: 8.0365e-05 - val_mae: 0.0064 - val_mda: 0.5124 - val_rmse: 0.0106 - learning_rate: 1.2500e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.0967e-05 - mae: 0.0032 - mda: 0.5029 - rmse: 0.0782\n",
            "Epoch 19: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.0967e-05 - mae: 0.0032 - mda: 0.5029 - rmse: 0.0782 - val_loss: 5.1202e-05 - val_mae: 0.0050 - val_mda: 0.5125 - val_rmse: 0.0097 - learning_rate: 1.2500e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.9216e-05 - mae: 0.0031 - mda: 0.5044 - rmse: 0.0783\n",
            "Epoch 20: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 1.9219e-05 - mae: 0.0031 - mda: 0.5044 - rmse: 0.0783 - val_loss: 4.5437e-05 - val_mae: 0.0047 - val_mda: 0.5123 - val_rmse: 0.0094 - learning_rate: 1.2500e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.0468e-05 - mae: 0.0032 - mda: 0.5042 - rmse: 0.0785\n",
            "Epoch 21: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.0470e-05 - mae: 0.0032 - mda: 0.5042 - rmse: 0.0785 - val_loss: 3.1529e-05 - val_mae: 0.0041 - val_mda: 0.5123 - val_rmse: 0.0091 - learning_rate: 1.2500e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.9858e-05 - mae: 0.0031 - mda: 0.5026 - rmse: 0.0789\n",
            "Epoch 22: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 1.9853e-05 - mae: 0.0031 - mda: 0.5027 - rmse: 0.0789 - val_loss: 4.3525e-05 - val_mae: 0.0046 - val_mda: 0.5120 - val_rmse: 0.0094 - learning_rate: 6.2500e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.9530e-05 - mae: 0.0031 - mda: 0.5039 - rmse: 0.0779\n",
            "Epoch 23: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 1.9531e-05 - mae: 0.0031 - mda: 0.5039 - rmse: 0.0779 - val_loss: 4.2972e-05 - val_mae: 0.0046 - val_mda: 0.5120 - val_rmse: 0.0094 - learning_rate: 6.2500e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.9212e-05 - mae: 0.0031 - mda: 0.5029 - rmse: 0.0784\n",
            "Epoch 24: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 1.9211e-05 - mae: 0.0031 - mda: 0.5029 - rmse: 0.0784 - val_loss: 6.2138e-05 - val_mae: 0.0056 - val_mda: 0.5120 - val_rmse: 0.0101 - learning_rate: 6.2500e-05\n",
            "Epoch 24: early stopping\n",
            "Restoring model weights from the end of the best epoch: 14.\n",
            "Validation Loss: 0.00003, RMSE: 0.00890, MDA: 0.51600, MAE: 0.00404\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'rnn_units': 32}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0015 - mae: 0.0206 - mda: 0.5022 - rmse: 0.0790\n",
            "Epoch 1: val_loss improved from inf to 0.00135, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0014 - mae: 0.0205 - mda: 0.5022 - rmse: 0.0790 - val_loss: 0.0014 - val_mae: 0.0270 - val_mda: 0.5169 - val_rmse: 0.0292 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.7986e-05 - mae: 0.0072 - mda: 0.5027 - rmse: 0.0773\n",
            "Epoch 2: val_loss improved from 0.00135 to 0.00080, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 9.7890e-05 - mae: 0.0072 - mda: 0.5027 - rmse: 0.0774 - val_loss: 7.9800e-04 - val_mae: 0.0194 - val_mda: 0.5155 - val_rmse: 0.0221 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.5096e-05 - mae: 0.0067 - mda: 0.5030 - rmse: 0.0777\n",
            "Epoch 3: val_loss did not improve from 0.00080\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 8.5197e-05 - mae: 0.0067 - mda: 0.5030 - rmse: 0.0777 - val_loss: 0.0010 - val_mae: 0.0248 - val_mda: 0.5143 - val_rmse: 0.0270 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.4496e-05 - mae: 0.0066 - mda: 0.5027 - rmse: 0.0779\n",
            "Epoch 4: val_loss improved from 0.00080 to 0.00042, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 8.4372e-05 - mae: 0.0066 - mda: 0.5027 - rmse: 0.0779 - val_loss: 4.2182e-04 - val_mae: 0.0116 - val_mda: 0.5135 - val_rmse: 0.0152 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m284/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.6124e-05 - mae: 0.0067 - mda: 0.5024 - rmse: 0.0778\n",
            "Epoch 5: val_loss improved from 0.00042 to 0.00040, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 8.5993e-05 - mae: 0.0067 - mda: 0.5024 - rmse: 0.0779 - val_loss: 4.0286e-04 - val_mae: 0.0115 - val_mda: 0.5138 - val_rmse: 0.0151 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.9071e-05 - mae: 0.0064 - mda: 0.5030 - rmse: 0.0775\n",
            "Epoch 6: val_loss did not improve from 0.00040\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 7.9071e-05 - mae: 0.0064 - mda: 0.5030 - rmse: 0.0776 - val_loss: 7.9545e-04 - val_mae: 0.0165 - val_mda: 0.5140 - val_rmse: 0.0198 - learning_rate: 0.0100\n",
            "Epoch 7/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m282/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.1851e-05 - mae: 0.0061 - mda: 0.5026 - rmse: 0.0791\n",
            "Epoch 7: val_loss did not improve from 0.00040\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 7.2205e-05 - mae: 0.0061 - mda: 0.5026 - rmse: 0.0790 - val_loss: 6.5695e-04 - val_mae: 0.0160 - val_mda: 0.5135 - val_rmse: 0.0193 - learning_rate: 0.0100\n",
            "Epoch 8/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.6453e-05 - mae: 0.0063 - mda: 0.5024 - rmse: 0.0777\n",
            "Epoch 8: val_loss did not improve from 0.00040\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 7.6553e-05 - mae: 0.0063 - mda: 0.5024 - rmse: 0.0777 - val_loss: 7.0763e-04 - val_mae: 0.0142 - val_mda: 0.5129 - val_rmse: 0.0180 - learning_rate: 0.0100\n",
            "Epoch 9/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.3028e-05 - mae: 0.0062 - mda: 0.5031 - rmse: 0.0780\n",
            "Epoch 9: val_loss did not improve from 0.00040\n",
            "\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 7.3144e-05 - mae: 0.0062 - mda: 0.5031 - rmse: 0.0780 - val_loss: 4.3395e-04 - val_mae: 0.0133 - val_mda: 0.5128 - val_rmse: 0.0168 - learning_rate: 0.0100\n",
            "Epoch 10/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.7948e-05 - mae: 0.0058 - mda: 0.5033 - rmse: 0.0786\n",
            "Epoch 10: val_loss did not improve from 0.00040\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.7899e-05 - mae: 0.0058 - mda: 0.5033 - rmse: 0.0786 - val_loss: 8.2658e-04 - val_mae: 0.0160 - val_mda: 0.5130 - val_rmse: 0.0196 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.2268e-05 - mae: 0.0057 - mda: 0.5033 - rmse: 0.0781\n",
            "Epoch 11: val_loss did not improve from 0.00040\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.2302e-05 - mae: 0.0057 - mda: 0.5033 - rmse: 0.0781 - val_loss: 0.0015 - val_mae: 0.0224 - val_mda: 0.5132 - val_rmse: 0.0257 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.5751e-05 - mae: 0.0058 - mda: 0.5031 - rmse: 0.0782\n",
            "Epoch 12: val_loss did not improve from 0.00040\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.5739e-05 - mae: 0.0058 - mda: 0.5031 - rmse: 0.0782 - val_loss: 5.8630e-04 - val_mae: 0.0132 - val_mda: 0.5129 - val_rmse: 0.0170 - learning_rate: 0.0050\n",
            "Epoch 13/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.6438e-05 - mae: 0.0060 - mda: 0.5031 - rmse: 0.0783\n",
            "Epoch 13: val_loss did not improve from 0.00040\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.6458e-05 - mae: 0.0060 - mda: 0.5031 - rmse: 0.0783 - val_loss: 0.0012 - val_mae: 0.0196 - val_mda: 0.5130 - val_rmse: 0.0231 - learning_rate: 0.0050\n",
            "Epoch 14/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.6272e-05 - mae: 0.0063 - mda: 0.5026 - rmse: 0.0786\n",
            "Epoch 14: val_loss did not improve from 0.00040\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 7.6197e-05 - mae: 0.0063 - mda: 0.5027 - rmse: 0.0786 - val_loss: 0.0013 - val_mae: 0.0210 - val_mda: 0.5127 - val_rmse: 0.0243 - learning_rate: 0.0050\n",
            "Epoch 15/50\n",
            "\u001b[1m285/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.3754e-05 - mae: 0.0057 - mda: 0.5036 - rmse: 0.0782\n",
            "Epoch 15: val_loss did not improve from 0.00040\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6.3737e-05 - mae: 0.0057 - mda: 0.5036 - rmse: 0.0782 - val_loss: 9.8293e-04 - val_mae: 0.0167 - val_mda: 0.5132 - val_rmse: 0.0205 - learning_rate: 0.0025\n",
            "Epoch 15: early stopping\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "Validation Loss: 0.00040, RMSE: 0.01515, MDA: 0.51688, MAE: 0.01155\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'rnn_units': 64}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0025 - mae: 0.0218 - mda: 0.5028 - rmse: 0.0828\n",
            "Epoch 1: val_loss improved from inf to 0.00030, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0024 - mae: 0.0216 - mda: 0.5028 - rmse: 0.0828 - val_loss: 2.9539e-04 - val_mae: 0.0135 - val_mda: 0.5163 - val_rmse: 0.0162 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m287/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7047e-05 - mae: 0.0064 - mda: 0.5029 - rmse: 0.0787\n",
            "Epoch 2: val_loss improved from 0.00030 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 7.6969e-05 - mae: 0.0063 - mda: 0.5029 - rmse: 0.0787 - val_loss: 4.3127e-05 - val_mae: 0.0048 - val_mda: 0.5145 - val_rmse: 0.0091 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.2654e-05 - mae: 0.0057 - mda: 0.5029 - rmse: 0.0788\n",
            "Epoch 3: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 6.2653e-05 - mae: 0.0057 - mda: 0.5029 - rmse: 0.0788 - val_loss: 4.9877e-04 - val_mae: 0.0162 - val_mda: 0.5147 - val_rmse: 0.0189 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.3981e-05 - mae: 0.0054 - mda: 0.5034 - rmse: 0.0782\n",
            "Epoch 4: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 5.3980e-05 - mae: 0.0054 - mda: 0.5034 - rmse: 0.0782 - val_loss: 4.5992e-05 - val_mae: 0.0051 - val_mda: 0.5147 - val_rmse: 0.0094 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.4554e-05 - mae: 0.0055 - mda: 0.5022 - rmse: 0.0787\n",
            "Epoch 5: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 5.4587e-05 - mae: 0.0055 - mda: 0.5022 - rmse: 0.0787 - val_loss: 2.7427e-04 - val_mae: 0.0100 - val_mda: 0.5139 - val_rmse: 0.0137 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m289/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.8119e-05 - mae: 0.0057 - mda: 0.5027 - rmse: 0.0786\n",
            "Epoch 6: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 5.8111e-05 - mae: 0.0057 - mda: 0.5027 - rmse: 0.0786 - val_loss: 1.6253e-04 - val_mae: 0.0082 - val_mda: 0.5132 - val_rmse: 0.0121 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.0657e-05 - mae: 0.0052 - mda: 0.5034 - rmse: 0.0784\n",
            "Epoch 7: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 5.0663e-05 - mae: 0.0052 - mda: 0.5034 - rmse: 0.0784 - val_loss: 3.3971e-04 - val_mae: 0.0121 - val_mda: 0.5132 - val_rmse: 0.0155 - learning_rate: 0.0100\n",
            "Epoch 8/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.3578e-05 - mae: 0.0048 - mda: 0.5025 - rmse: 0.0782\n",
            "Epoch 8: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.3546e-05 - mae: 0.0048 - mda: 0.5025 - rmse: 0.0782 - val_loss: 1.0716e-04 - val_mae: 0.0073 - val_mda: 0.5129 - val_rmse: 0.0113 - learning_rate: 0.0050\n",
            "Epoch 9/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.4391e-05 - mae: 0.0047 - mda: 0.5036 - rmse: 0.0784\n",
            "Epoch 9: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.4378e-05 - mae: 0.0047 - mda: 0.5036 - rmse: 0.0784 - val_loss: 2.3248e-04 - val_mae: 0.0111 - val_mda: 0.5131 - val_rmse: 0.0145 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.2512e-05 - mae: 0.0047 - mda: 0.5027 - rmse: 0.0783\n",
            "Epoch 10: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.2506e-05 - mae: 0.0047 - mda: 0.5027 - rmse: 0.0783 - val_loss: 5.8898e-04 - val_mae: 0.0176 - val_mda: 0.5135 - val_rmse: 0.0204 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m286/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.6835e-05 - mae: 0.0050 - mda: 0.5038 - rmse: 0.0781\n",
            "Epoch 11: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.6813e-05 - mae: 0.0050 - mda: 0.5038 - rmse: 0.0781 - val_loss: 2.9190e-04 - val_mae: 0.0119 - val_mda: 0.5133 - val_rmse: 0.0152 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m288/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.7278e-05 - mae: 0.0049 - mda: 0.5034 - rmse: 0.0784\n",
            "Epoch 12: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.7287e-05 - mae: 0.0049 - mda: 0.5034 - rmse: 0.0784 - val_loss: 5.0663e-05 - val_mae: 0.0058 - val_mda: 0.5136 - val_rmse: 0.0102 - learning_rate: 0.0050\n",
            "Epoch 12: early stopping\n",
            "Restoring model weights from the end of the best epoch: 2.\n",
            "Validation Loss: 0.00004, RMSE: 0.00908, MDA: 0.51629, MAE: 0.00481\n",
            "--------------------------------------------------\n",
            "Training with parameters: {'batch_size': 32, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'rnn_units': 128}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch 1/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0088 - mae: 0.0287 - mda: 0.5022 - rmse: 0.0899\n",
            "Epoch 1: val_loss improved from inf to 0.00014, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 0.0087 - mae: 0.0286 - mda: 0.5022 - rmse: 0.0898 - val_loss: 1.3953e-04 - val_mae: 0.0080 - val_mda: 0.5133 - val_rmse: 0.0113 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 7.2949e-05 - mae: 0.0061 - mda: 0.5028 - rmse: 0.0780\n",
            "Epoch 2: val_loss improved from 0.00014 to 0.00006, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 7.2923e-05 - mae: 0.0061 - mda: 0.5028 - rmse: 0.0780 - val_loss: 5.7409e-05 - val_mae: 0.0054 - val_mda: 0.5149 - val_rmse: 0.0093 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 5.4013e-05 - mae: 0.0053 - mda: 0.5036 - rmse: 0.0774\n",
            "Epoch 3: val_loss improved from 0.00006 to 0.00005, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 5.4002e-05 - mae: 0.0053 - mda: 0.5036 - rmse: 0.0774 - val_loss: 5.0425e-05 - val_mae: 0.0056 - val_mda: 0.5153 - val_rmse: 0.0096 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.4554e-05 - mae: 0.0048 - mda: 0.5039 - rmse: 0.0778\n",
            "Epoch 4: val_loss did not improve from 0.00005\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 4.4537e-05 - mae: 0.0048 - mda: 0.5039 - rmse: 0.0778 - val_loss: 1.4716e-04 - val_mae: 0.0086 - val_mda: 0.5144 - val_rmse: 0.0120 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.9189e-05 - mae: 0.0045 - mda: 0.5033 - rmse: 0.0785\n",
            "Epoch 5: val_loss improved from 0.00005 to 0.00004, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 3.9171e-05 - mae: 0.0045 - mda: 0.5033 - rmse: 0.0785 - val_loss: 4.0749e-05 - val_mae: 0.0046 - val_mda: 0.5142 - val_rmse: 0.0089 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.4944e-05 - mae: 0.0043 - mda: 0.5033 - rmse: 0.0789\n",
            "Epoch 6: val_loss did not improve from 0.00004\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 3.4953e-05 - mae: 0.0043 - mda: 0.5033 - rmse: 0.0789 - val_loss: 3.8188e-04 - val_mae: 0.0158 - val_mda: 0.5138 - val_rmse: 0.0184 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.9585e-05 - mae: 0.0040 - mda: 0.5025 - rmse: 0.0783\n",
            "Epoch 7: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.9587e-05 - mae: 0.0040 - mda: 0.5025 - rmse: 0.0783 - val_loss: 1.0068e-04 - val_mae: 0.0071 - val_mda: 0.5138 - val_rmse: 0.0108 - learning_rate: 0.0050\n",
            "Epoch 8/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.9384e-05 - mae: 0.0039 - mda: 0.5033 - rmse: 0.0785\n",
            "Epoch 8: val_loss did not improve from 0.00004\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 2.9391e-05 - mae: 0.0039 - mda: 0.5033 - rmse: 0.0785 - val_loss: 4.4818e-05 - val_mae: 0.0048 - val_mda: 0.5142 - val_rmse: 0.0091 - learning_rate: 0.0050\n",
            "Epoch 9/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.2741e-05 - mae: 0.0041 - mda: 0.5047 - rmse: 0.0788\n",
            "Epoch 9: val_loss improved from 0.00004 to 0.00003, saving model to best_model.keras\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 3.2725e-05 - mae: 0.0041 - mda: 0.5047 - rmse: 0.0788 - val_loss: 3.1784e-05 - val_mae: 0.0043 - val_mda: 0.5138 - val_rmse: 0.0088 - learning_rate: 0.0050\n",
            "Epoch 10/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.8008e-05 - mae: 0.0038 - mda: 0.5035 - rmse: 0.0780\n",
            "Epoch 10: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.8010e-05 - mae: 0.0038 - mda: 0.5035 - rmse: 0.0780 - val_loss: 9.1399e-05 - val_mae: 0.0070 - val_mda: 0.5137 - val_rmse: 0.0109 - learning_rate: 0.0050\n",
            "Epoch 11/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.9292e-05 - mae: 0.0040 - mda: 0.5025 - rmse: 0.0777\n",
            "Epoch 11: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.9294e-05 - mae: 0.0040 - mda: 0.5025 - rmse: 0.0777 - val_loss: 9.0471e-05 - val_mae: 0.0068 - val_mda: 0.5142 - val_rmse: 0.0108 - learning_rate: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.1042e-05 - mae: 0.0040 - mda: 0.5032 - rmse: 0.0785\n",
            "Epoch 12: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 3.1040e-05 - mae: 0.0040 - mda: 0.5032 - rmse: 0.0785 - val_loss: 3.3381e-05 - val_mae: 0.0045 - val_mda: 0.5134 - val_rmse: 0.0091 - learning_rate: 0.0050\n",
            "Epoch 13/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.9382e-05 - mae: 0.0040 - mda: 0.5032 - rmse: 0.0782\n",
            "Epoch 13: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2.9389e-05 - mae: 0.0040 - mda: 0.5032 - rmse: 0.0782 - val_loss: 5.4331e-05 - val_mae: 0.0061 - val_mda: 0.5137 - val_rmse: 0.0104 - learning_rate: 0.0050\n",
            "Epoch 14/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.2830e-05 - mae: 0.0042 - mda: 0.5031 - rmse: 0.0789\n",
            "Epoch 14: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 3.2825e-05 - mae: 0.0042 - mda: 0.5031 - rmse: 0.0789 - val_loss: 8.8468e-05 - val_mae: 0.0064 - val_mda: 0.5125 - val_rmse: 0.0105 - learning_rate: 0.0050\n",
            "Epoch 15/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.7119e-05 - mae: 0.0038 - mda: 0.5056 - rmse: 0.0786\n",
            "Epoch 15: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 2.7121e-05 - mae: 0.0038 - mda: 0.5056 - rmse: 0.0786 - val_loss: 4.0067e-05 - val_mae: 0.0045 - val_mda: 0.5125 - val_rmse: 0.0091 - learning_rate: 0.0025\n",
            "Epoch 16/50\n",
            "\u001b[1m291/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.5852e-05 - mae: 0.0036 - mda: 0.5029 - rmse: 0.0788\n",
            "Epoch 16: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 2.5846e-05 - mae: 0.0036 - mda: 0.5029 - rmse: 0.0788 - val_loss: 1.7851e-04 - val_mae: 0.0103 - val_mda: 0.5127 - val_rmse: 0.0137 - learning_rate: 0.0025\n",
            "Epoch 17/50\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.7617e-05 - mae: 0.0038 - mda: 0.5029 - rmse: 0.0783\n",
            "Epoch 17: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 2.7616e-05 - mae: 0.0038 - mda: 0.5029 - rmse: 0.0783 - val_loss: 3.8109e-05 - val_mae: 0.0048 - val_mda: 0.5126 - val_rmse: 0.0095 - learning_rate: 0.0025\n",
            "Epoch 18/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.6231e-05 - mae: 0.0037 - mda: 0.5033 - rmse: 0.0787\n",
            "Epoch 18: val_loss did not improve from 0.00003\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 2.6233e-05 - mae: 0.0037 - mda: 0.5033 - rmse: 0.0787 - val_loss: 8.2725e-05 - val_mae: 0.0058 - val_mda: 0.5123 - val_rmse: 0.0103 - learning_rate: 0.0025\n",
            "Epoch 19/50\n",
            "\u001b[1m290/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.6031e-05 - mae: 0.0037 - mda: 0.5034 - rmse: 0.0777\n",
            "Epoch 19: val_loss did not improve from 0.00003\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "\u001b[1m292/292\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 2.6043e-05 - mae: 0.0037 - mda: 0.5034 - rmse: 0.0777 - val_loss: 5.8591e-05 - val_mae: 0.0053 - val_mda: 0.5128 - val_rmse: 0.0098 - learning_rate: 0.0025\n",
            "Epoch 19: early stopping\n",
            "Restoring model weights from the end of the best epoch: 9.\n",
            "Validation Loss: 0.00003, RMSE: 0.00880, MDA: 0.51527, MAE: 0.00427\n",
            "--------------------------------------------------\n",
            "Best LSTM Parameters: {'batch_size': 16, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'rnn_units': 128}, Validation Loss: 2.0388733901199885e-05\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Tuning LSTM...\")\n",
        "best_lstm_model, best_lstm_params, best_lstm_loss, lstm_results = tune_model(build_lstm, X_train, y_train, X_val, y_val, param_grid)\n",
        "print(f\"Best LSTM Parameters: {best_lstm_params}, Validation Loss: {best_lstm_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd9ef4fe",
      "metadata": {
        "id": "fd9ef4fe"
      },
      "outputs": [],
      "source": [
        "# Saving results to CSV\n",
        "rnn_results.to_csv('rnn_results.csv', index=False)\n",
        "gru_results.to_csv('gru_results.csv', index=False)\n",
        "lstm_results.to_csv('lstm_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7f96605",
      "metadata": {
        "id": "c7f96605"
      },
      "source": [
        "## Loss Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c8ed04b",
      "metadata": {
        "id": "0c8ed04b"
      },
      "outputs": [],
      "source": [
        "# Plot training/validation loss curves\n",
        "def plot_loss_curves(history, title=\"Training and Validation Loss\"):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93df318d",
      "metadata": {
        "id": "93df318d"
      },
      "source": [
        "### Vanilla RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f715382c",
      "metadata": {
        "id": "f715382c",
        "outputId": "01070247-1620-4b21-c451-74da921767b7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsUAAAIhCAYAAACmHseMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAB6PElEQVR4nO3deVxUVeM/8M+dnWFRQBRQUDQ3NDXBDMylTBTLtPTRyi9laf1MLZd6UirLVrW0yMfUFtKsnqQyqyfNwFJzwdJcWjSzQtGEFFDZZ4aZ8/tjmIFhBoQRGGbm8341L2bunHvvmePVPpw59xxJCCFAREREROTFZK6uABERERGRqzEUExEREZHXYygmIiIiIq/HUExEREREXo+hmIiIiIi8HkMxEREREXk9hmIiIiIi8noMxURERETk9RiKiYiIiMjrMRQTUYvx/fff47bbbkNkZCTUajXatWuHuLg4PPLIIzblhg0bhmHDhrmkjidPnoQkSVi3bp3T+1oeMpkMgYGBGD58ONLT0+3KL1q0CJIkoW3btigqKrJ7v1OnTrjllltstlmOvWTJErvy69atgyRJOHDgQJ313LFjByRJwieffNLAT+gaP/30E+69915ERUVBo9HAz88P/fv3x0svvYSCggJXV4+I3ARDMRG1CJs3b0Z8fDwKCwvx0ksvIT09Ha+99hoGDRqEtLQ0m7KrVq3CqlWrXFTTK/fQQw8hMzMTu3btwrJly3DixAmMHj0a3333ncPy58+fx0svvdSgcyxZssQrAuFbb72FmJgY7N+/H//+97+xdetWbNq0Cf/617+wZs0aTJ061dVVJCI3oXB1BYiIAOCll15CVFQUvv76aygUVf803XHHHXaBMDo6urmr16giIyNx3XXXAQAGDRqErl27YujQoUhNTcWQIUPsyo8aNQqvvvoqZs6cidDQ0Mse/6abbsKOHTvwwgsvYPny5Y1e/5YiMzMTDz74IEaMGIHPPvsMarXa+t6IESPwyCOPYOvWrY1yrrKyMmg0GkiS1CjHI6KWhz3FRNQi5Ofno02bNjaB2EIms/2nqubwCcuwhJdffhlLly5Fp06d4OPjg2HDhuH333+HwWDAggULEB4ejlatWuG2227DuXPnbI5pGYqwadMm9OnTBxqNBp07d8aKFSvqVf8TJ07grrvuQtu2baFWq9GzZ0+8/vrr9do3NjYWAPDPP/84fP/5559HRUUFFi1aVK/jde/eHVOnTsXrr7+OU6dO1WsfZ/zyyy8YO3YsAgMDodFo0K9fP7z77rs2ZUwmE55//nl0794dPj4+aN26Nfr06YPXXnvNWub8+fN44IEHEBERAbVajZCQEAwaNAjbtm2r8/wvvvgiJEnCm2++aROILVQqFW699Vbra0mSHLZhp06dMGXKFOtryzCT9PR03HfffQgJCYFWq0VaWhokScI333xjd4zVq1dDkiT89NNP1m0HDhzArbfeiqCgIGg0GlxzzTX46KOPbPYrLS3Fo48+ah36ERQUhNjYWHz44Yd1fnYianwMxUTUIsTFxeH777/Hww8/jO+//x4Gg6HBx3j99dexZ88evP7663j77bfx22+/YcyYMZg6dSrOnz+Pd955By+99BK2bduGadOm2e1/+PBhzJkzB3PnzsWmTZsQHx+P2bNnY9myZXWe9+jRoxgwYAB++eUXLF++HF9++SVuvvlmPPzww3jmmWcuW++srCwAQLdu3Ry+37FjR8yYMQOpqan4/fff69ES5vHIcrkcCxcurFf5hjp+/Dji4+Px66+/YsWKFfj0008RHR2NKVOm2PTsv/TSS1i0aBHuvPNObN68GWlpaZg6dSouXrxoLZOUlITPPvsMTz31FNLT0/H222/jpptuQn5+fq3nNxqN+PbbbxETE4OIiIgm+Yz33XcflEol3nvvPXzyySe47bbb0LZtW6xdu9au7Lp169C/f3/06dMHALB9+3YMGjQIFy9exJo1a/D555+jX79+mDRpks149Hnz5mH16tV4+OGHsXXrVrz33nv417/+VednJ6ImIoiIWoC8vDxx/fXXCwACgFAqlSI+Pl4sXrxYFBUV2ZQdOnSoGDp0qPV1VlaWACD69u0rjEajdXtKSooAIG699Vab/efMmSMAiEuXLlm3dezYUUiSJA4fPmxTdsSIESIgIECUlJTYnGvt2rXWMiNHjhQdOnSwOZ4QQsyaNUtoNBpRUFBgs+/SpUuFwWAQ5eXl4vDhwyIuLk6EhYWJrKwsm/2ffvppAUCcP39e5OXliVatWonx48fb1Pnmm2+22QeAmDlzphBCiCeeeELIZDJx5MgRIYQQa9euFQDE/v37RV22b98uAIiPP/641jJ33HGHUKvVIjs722Z7YmKi0Gq14uLFi0IIIW655RbRr1+/Os/n5+cn5syZU2eZmnJzcwUAcccdd9R7HwDi6aefttvesWNHcc8991hfW9rp7rvvtis7b9484ePjY/18Qghx9OhRAUD85z//sW7r0aOHuOaaa4TBYLDZ/5ZbbhFhYWHW67R3795i3Lhx9f4MRNR02FNMRC1CcHAwdu3ahf3792PJkiUYO3Ysfv/9dyQnJ+Pqq69GXl7eZY8xevRom6EWPXv2BADcfPPNNuUs27Ozs2229+rVC3379rXZdtddd6GwsBAHDx50eM7y8nJ88803uO2226DValFRUWF9jB49GuXl5di3b5/NPvPnz4dSqbQOOfjll1/wv//9D506dar1swUHB2P+/PnYuHEjvv/++7obotJjjz2GoKAgzJ8/v17lG+Lbb7/F8OHD7Xppp0yZgtLSUmRmZgIArr32Whw5cgQzZszA119/jcLCQrtjXXvttVi3bh2ef/557Nu3z6lvCZrC+PHj7bbdd999KCsrs7n5c+3atVCr1bjrrrsAAH/88Qd+++03TJ48GQDsromcnBwcP34cgPmzf/XVV1iwYAF27NiBsrKyZvhkROQIQzERtSixsbGYP38+Pv74Y5w9exZz587FyZMn6zX7QlBQkM1rlUpV5/by8nKb7Y5uYrNsq+3r7Pz8fFRUVOA///kPlEqlzWP06NEAYBfoZ8+ejf3792P37t1YtmwZDAYDxo4de9mvzOfMmYPw8HA89thjdZazCAgIwJNPPomtW7di+/bt9dqnvvLz8xEWFma3PTw83Po+ACQnJ2PZsmXYt28fEhMTERwcjOHDh9tMC5eWloZ77rkHb7/9NuLi4hAUFIS7774bubm5tZ6/TZs20Gq11qEnTcHR5+vVqxcGDBhgHUJhNBrx/vvvY+zYsdbrzDI2/NFHH7W7JmbMmAGg6ppYsWIF5s+fj88++ww33HADgoKCMG7cOJw4caLJPhcROcZQTEQtllKpxNNPPw3AfFNXU3MUwizbgoODHe4TGBgIuVyOKVOmYP/+/Q4flnBs0aFDB8TGxmLQoEF45JFH8Pbbb+Pvv/+2ftba+Pj4YNGiRfjuu++wefPmen2mBx98EFFRUZg/fz6EEPXapz6Cg4ORk5Njt/3s2bMAzKEVABQKBebNm4eDBw+ioKAAH374IU6fPo2RI0eitLTUWjYlJQUnT57EqVOnsHjxYnz66ac2N7/VJJfLMXz4cPz44484c+ZMveqsVquh0+nsttf2y0htM03ce++92LdvH44dO4atW7ciJycH9957r/V9y2dPTk6u9Zro168fAMDX1xfPPPMMfvvtN+Tm5mL16tXYt28fxowZU6/PRESNh6GYiFoERwELAI4dOwagqgeyKf366684cuSIzbb//ve/8Pf3R//+/R3uo9VqccMNN+DQoUPo06cPYmNj7R61BWqLyZMnY9iwYXjrrbcuO1vEfffdh549e2LBggUwmUyX/UwqlQrPP/889u/fj48//viy5etr+PDh+Pbbb60h2GL9+vXQarXWKeeqa926NSZMmICZM2eioKAAJ0+etCsTGRmJWbNmYcSIEbUOWbFITk6GEAL3338/9Hq93fsGgwH/+9//rK87depkMzsEYB4GUlxcXOd5arrzzjuh0Wiwbt06rFu3Du3bt0dCQoL1/e7du6Nr1644cuSIw+shNjYW/v7+dsdt164dpkyZgjvvvBPHjx+3/tJARM2D8xQTUYswcuRIdOjQAWPGjEGPHj1gMplw+PBhLF++HH5+fpg9e3aT1yE8PBy33norFi1ahLCwMLz//vvIyMjA0qVLodVqa93vtddew/XXX4/BgwfjwQcfRKdOnVBUVIQ//vgD//vf//Dtt99e9txLly7FwIED8dxzz+Htt9+utZxcLseLL76I2267DQCssx3U5c4778SyZcvw1VdfXbZsdTXHQlsMHToUTz/9NL788kvccMMNeOqppxAUFIQPPvgAmzdvxksvvYRWrVoBAMaMGYPevXsjNjYWISEhOHXqFFJSUtCxY0d07doVly5dwg033IC77roLPXr0gL+/P/bv34+tW7fi9ttvr7N+cXFxWL16NWbMmIGYmBg8+OCD6NWrFwwGAw4dOoQ333wTvXv3tva6JiUlYeHChXjqqacwdOhQHD16FCtXrrTWtb5at26N2267DevWrcPFixfx6KOP2k0b+MYbbyAxMREjR47ElClT0L59exQUFODYsWM4ePCg9ReUgQMH4pZbbkGfPn0QGBiIY8eO4b333kNcXFyd1xwRNQFX3+lHRCSEEGlpaeKuu+4SXbt2FX5+fkKpVIrIyEiRlJQkjh49alO2ttknXn75ZZtytc2i4GgWBstMDp988ono1auXUKlUolOnTuKVV16x2dfR7BOW7ffdd59o3769UCqVIiQkRMTHx4vnn3/+svW0+Ne//iUUCoX4448/hBC2s0/UFB8fLwDUOftEdenp6daZPeo7+0Rtj+3btwshhPj555/FmDFjRKtWrYRKpRJ9+/a1a5fly5eL+Ph40aZNG6FSqURkZKSYOnWqOHnypBBCiPLycjF9+nTRp08fERAQIHx8fET37t3F008/bZ3x43IOHz4s7rnnHhEZGSlUKpXw9fUV11xzjXjqqafEuXPnrOV0Op147LHHREREhPDx8RFDhw4Vhw8frnX2ibraqXp7/v777w7LHDlyREycOFG0bdtWKJVKERoaKm688UaxZs0aa5kFCxaI2NhYERgYKNRqtejcubOYO3euyMvLq9dnJ6LGIwnRiIPMiIjcVKdOndC7d298+eWXrq4KERG5AMcUExEREZHXYygmIiIiIq/H4RNERERE5PXYU0xEREREXo+hmIiIiIi8HkMxEREREXk9Lt7hJJPJhLNnz8Lf37/WpUCJiIiIyHWEECgqKkJ4eLjdIjs1MRQ76ezZs4iIiHB1NYiIiIjoMk6fPo0OHTrUWYah2EmWdetPnz6NgICAJj+fwWBAeno6EhISoFQqm/x8noRt5xy2m3PYbs5huzmPbecctptz3K3dCgsLERERYc1tdWEodpJlyERAQECzhWKtVouAgAC3uAhbEradc9huzmG7OYft5jy2nXPYbs5x13arz1BX3mhHRERERF6PoZiIiIiIvB5DMRERERF5PY4pJiIioiZnNBphMBhcXQ0rg8EAhUKB8vJyGI1GV1fHbbS0dpPL5VAoFI0yPS5DMRERETWp4uJinDlzBkIIV1fFSgiB0NBQnD59musNNEBLbDetVouwsDCoVKorOg5DMRERETUZo9GIM2fOQKvVIiQkpMUEKZPJhOLiYvj5+V12UQeq0pLaTQgBvV6P8+fPIysrC127dr2iOjEUExERUZMxGAwQQiAkJAQ+Pj6uro6VyWSCXq+HRqNxebhzJy2t3Xx8fKBUKnHq1ClrvZzl+k9DREREHq+l9BCT52mscM5QTERERERej6GYiIiIiLweQzERERFRMxg2bBjmzJlT7/InT56EJEk4fPhwk9WJqjAUExEREVUjSVKdjylTpjh13E8//RTPPfdcvctHREQgJycHvXv3dup89cXwbcbZJ4iIiIiqycnJsT5PS0vDU089hePHj1u31ZxFw2AwQKlUXva4QUFBDaqHXC5HaGhog/Yh57GnmIiIiJqNEAKl+gqXPOq7eEhoaKj10apVK0iSZH1dXl6O1q1b46OPPsKwYcOg0Wjw/vvvIz8/H3feeSc6dOgArVaLq6++Gh9++KHNcWsOn+jUqRNefPFF3HffffD390dkZCTefPNN6/s1e3B37NgBSZLwzTffIDY2FlqtFvHx8TaBHQCef/55tG3bFv7+/pg2bRoWLFiAfv36OfXnBQA6nQ4PP/ww2rZtC61Wi1GjRmH//v3W9y9cuIDJkydbp93r2rUr1q5dCwDQ6/WYNWsWwsLCoNFo0KlTJyxevNjpujQl9hQTERFRsykzGBH91NcuOffRZ0dCq2qc6DN//nwsX74ca9euhVqtRnl5OWJiYjB//nwEBARg8+bNSEpKQufOnTFw4MBaj7N8+XI899xzePzxx/HJJ5/gwQcfxJAhQ9CjR49a93niiSewfPlyhISEYPr06bjvvvuwZ88eAMAHH3yAF154AatWrcKgQYOwYcMGLF++HFFRUU5/1sceewwbN27Eu+++i4iICLz44otITEzEH3/8gaCgICxcuBBHjx7FV199hTZt2uCPP/5AWVkZAGDFihX44osv8NFHHyEyMhKnT5/G6dOnna5LU2IoJiIiImqgOXPm4Pbbb7fZ9uijj1qfP/TQQ9i6dSs+/vjjOkPx6NGjMWPGDADmoP3qq69ix44ddYbiF154AUOHDgUALFiwADfffDPKy8uh0Wjwn//8B1OnTsW9994LAHjqqaeQnp6O4uJipz5nSUkJVq9ejXXr1iExMREmkwmvvfYa+vXrh9TUVPz73/9GdnY2rrnmGsTGxgIw94BbZGdno2vXrrj++ushSRI6duzoVD2aA0Oxm/jpzCUczJPQ92IZOoVcftwSERFRS+SjlOPosyNddu7GYgmAFkajEUuWLEFaWhr+/vtv6HQ66HQ6+Pr61nmcPn36WJ9bhmmcO3eu3vuEhYUBAM6dO4fIyEgcP37cGrItrr32Wnz77bf1+lw1/fnnnzAYDBg0aJB1m1KpxIABA3Ds2DEAwIMPPojx48fj4MGDSEhIwLhx4xAfHw8AmDJlCkaMGIHu3btj1KhRuOWWW5CQkOBUXZoaQ7GbWJ5xAnv/kqP3yQvoFBLg6uoQERE5RZKkRhvC4Eo1w+7y5cvx6quvIiUlBVdffTV8fX0xZ84c6PX6Oo9T8wY9SZJgMpnqvY9lpcDq+9RcPbC+Y6kdsezr6JiWbYmJiTh16hQ2b96Mbdu2Yfjw4Zg5cyaWLVuG/v37IysrC1999RW2bduGiRMn4qabbsInn3zidJ2aCm+0cxOBvioAwIVSg4trQkRERDXt2rULY8eOxf/93/+hb9++6Ny5M06cONHs9ejevTt++OEHm20HDhxw+nhXXXUVVCoVdu/ebd1mMBjw448/omfPntZtISEhmDJlCt5//32kpKTY3DAYEBCASZMm4a233kJaWho2btyIgoICp+vUVNz/VzUvEVQZigtK6v6Nk4iIiJrfVVddhY0bN2Lv3r0IDAzEK6+8gtzcXJvg2Bweeugh3H///YiNjUV8fDzS0tLw008/oXPnzpfdt+YsFgAQHR2NBx98EP/+978RFBSEDh064MUXX0RpaSmmTp0KwDxuOSYmBr169YJOp8OXX35p/dyvvvoqwsLC0K9fP8hkMnz88ccIDQ1F69atG/VzNwaGYjcRpDV/VcJQTERE1PIsXLgQWVlZGDlyJLRaLR544AGMGzcOly5datZ6TJ48GX/99RceffRRlJeXY+LEiZgyZYpd77Ejd9xxh922rKwsLFmyBCaTCUlJSSgqKkK/fv3w1VdfITAwEACgUqmQnJyMkydPwsfHB4MHD8aGDRsAAH5+fli6dClOnDgBuVyOAQMGYMuWLZDJWt5gBUlcyUATL1ZYWIhWrVrh0qVLCAho+jG+6/b8hUX/O4abeoTg7SnXNvn5PInBYMCWLVswevToek2uTmZsN+ew3ZzDdnNeS2+78vJyZGVlISoqChqNxtXVsTKZTCgsLERAQECLDGiNacSIEQgNDcV77713xcdqie1W1zXWkLzGnmI3Ye0p5phiIiIiqkVpaSnWrFmDkSNHQi6X48MPP8S2bduQkZHh6qq1eAzFbsIypvgCh08QERFRLSRJwpYtW/D8889Dp9Ohe/fu2LhxI2666SZXV63FYyh2E9Yb7UoZiomIiMgxHx8fbNu2zdXVcEstYzAIXZYlFF8qq4DBWPf8hURERETUMAzFbqK1jxISzPdEXmBvMREREVGjYih2E3KZBG3lYBdOy0ZERETUuBiK3Yhf5Uw7DMVEREREjYuh2I34saeYiIiIqEkwFLsRX6V5TDFDMREREVHjYih2I5ae4vxihmIiIqKWbtiwYZgzZ471dadOnZCSklLnPpIk4bPPPrviczfWcbwJQ7Eb4ZhiIiKipjdmzJhaF7vIzMyEJEk4ePBgg4+7f/9+PPDAA1daPRuLFi1Cv3797Lbn5OQgMTGxUc9V07p169C6desmPUdzYih2I9bhE5ySjYiIqMlMnToV3377LU6dOmX33jvvvIN+/fqhf//+DT5uSEgItFptY1TxskJDQ6FWq5vlXJ7C5aF41apViIqKgkajQUxMDHbt2lVn+Z07dyImJgYajQadO3fGmjVr7Mps3LgR0dHRUKvViI6OxqZNm2zeX7RoESRJsnmEhoY26udqCtYb7Th8goiI3JUQgL7ENQ8h6lXFW265BW3btsW6detstpeWliItLQ1Tp05Ffn4+7rzzTnTo0AFarRZXX301PvzwwzqPW3P4xIkTJzBkyBBoNBpER0cjIyPDbp/58+ejW7du0Gq16Ny5MxYuXAiDwQDA3FP7zDPP4MiRI9Y8Y6lzzeETP//8M2688Ub4+PggODgYDzzwAIqLi63vT5kyBePGjcOyZcsQFhaG4OBgzJw503ouZ2RnZ2Ps2LHw8/NDQEAAJk6ciH/++cf6/pEjR3DDDTfA398fAQEBiImJwYEDBwAAp06dwpgxYxAYGAhfX1/06tULW7Zscbou9eHSZZ7T0tIwZ84crFq1CoMGDcIbb7yBxMREHD16FJGRkXbls7KyMHr0aNx///14//33sWfPHsyYMQMhISEYP348APPXGpMmTcJzzz2H2267DZs2bcLEiROxe/duDBw40HqsXr162SyDKJfLm/4DXyEOnyAiIrdnKAVeDHfNuR8/C6h8L1tMoVDg7rvvxrp16/DUU09BkiQAwMcffwy9Xo/JkyejtLQUMTExmD9/PgICArB582YkJSWhc+fONnmjNiaTCbfffjvatGmDffv2obCw0Gb8sYW/vz/WrVuH8PBw/Pzzz7j//vvh7++Pxx57DJMmTcIvv/yCrVu3WjNNq1at7I5RWlqKUaNG4brrrsP+/ftx7tw5TJs2DbNmzbIJ/tu3b0dYWBi2b9+OP/74A5MmTUK/fv1w//33X/bz1CSEwLhx4+Dr64udO3eioqICM2bMwKRJk7Bjxw4AwOTJk3HNNddg9erVkMvlOHz4MJRKc9iZOXMm9Ho9vvvuO/j6+uLo0aPw8/NrcD0awqWh+JVXXsHUqVMxbdo0AEBKSgq+/vprrF69GosXL7Yrv2bNGkRGRlp/y+rZsycOHDiAZcuWWUNxSkoKRowYgeTkZABAcnIydu7ciZSUFJvf4BQKhVv0DlfnVzl8Ip+hmIiIqEndd999ePnll7Fjxw7ccMMNAMxDJ26//XYEBgYiMDAQjz76qLX8Qw89hK1bt+Ljjz+uVyjetm0bjh07hpMnT6JDhw4AgBdffNFuHPCTTz5pfd6pUyc88sgjSEtLw2OPPQYfHx/4+fldNtN88MEHKCsrw/r16+Hra/6lYOXKlRgzZgyWLl2Kdu3aAQACAwOxcuVKyOVy9OjRAzfffDO++eYbp0Lxtm3b8NNPPyErKwsREREAgPfeew+9evXC/v37MWDAAGRnZ+Pf//43evToAQDo2rWrdf/s7GyMHz8eV199NQCgc+fODa5DQ7ksFOv1evz4449YsGCBzfaEhATs3bvX4T6ZmZlISEiw2TZy5EikpqbCYDBAqVQiMzMTc+fOtStT827PEydOIDw8HGq1GgMHDsSLL75YZ4PrdDrodDrr68LCQgCAwWC4oq8W6stgMFiHT1wo1UOv11t/c6W6Wf58muPPyZOw3ZzDdnMO2815Lb3tDAYDhBAwmUwwmUyAXAMsOOOaysg1gMkEwNyTaflpqtxWXbdu3RAfH4/U1FQMHToUf/75J3bt2oWtW7fCZDLBaDRi6dKl+Oijj/D3339bc4JWq7U5Xs3jW15bvhUPDw+3vm8J09a2AvDJJ59gxYoV+OOPP1BcXIyKigoEBARY37d8DkefwXKco0ePom/fvvDx8bGWi4uLg8lkwrFjxxASEgIhBKKjoyFJkrVMaGgofvnlF7v61/xcjs599OhRREREoH379tb3e/TogdatW+PXX39FTEwM5s6di2nTpuG9997D8OHDMWHCBHTp0gUAMGvWLMycORPp6ekYPnw4br/9dvTp08fhH6vJZIIQAgaDwe6b/4b8vXBZKM7Ly4PRaLT+dmLRrl075ObmOtwnNzfXYfmKigrk5eUhLCys1jLVjzlw4ECsX78e3bp1wz///IPnn38e8fHx+PXXXxEcHOzw3IsXL8Yzzzxjtz09Pb3ZBs1bhk8YTQIb//eVddlnqh9HY7Xo8thuzmG7OYft5ryW2naWXszi4mLo9S7+prO8yG5TUZH9Nos777wTjz32GF588UW88cYbiIiIwIABA1BYWIjXXnsNK1aswIsvvojo6Gj4+voiOTkZpaWl1o6ziooK6PV662uTyYTy8nIUFhairKwMJpPJ+l71upSVlaGwsBD79+/HXXfdhQULFuC5555DQEAAPv30U6xcudK6n06ng9FotDmOheU4jspYnlvqazAYIEmSTRmDwWBT/5rtVl5eDiFEred29F71Npg7dy7GjBmD9PR0ZGRkYNGiRUhNTcUtt9yCiRMnIj4+Hunp6di+fTuWLFmC559/3uHsHXq9HmVlZfjuu+9QUVFh815paald+dq4PFbV7O0UQtTZA+qofM3tlztm9a8mrr76asTFxaFLly549913MW/ePIfnTU5OtnmvsLAQERERSEhIQEBAQK31bSwGgwEZGRnwVclRojciJn4ootpcflwUVbXdiBEjrGOV6PLYbs5huzmH7ea8lt525eXlOH36NPz8/KDRaFxdHSshBIqKiuDv719r7rj77ruRnJyML7/8EmlpaZg2bZp1zO7+/fsxduxY69ACk8mEkydPokePHtZcoFAooFKprK9lMhk0Gg0CAgJwzTXX4MyZMyguLkZ4uHmMdWZmJgDAx8cHAQEBOHLkCDp27Ihnn33WWqdVq1ZBkiTrMf39/QHAYRaxHKdv377YsGED5HK5dfjE7t27IZPJcM011yAgIABKpRIKhcLmOCqVym5b9XbTaDQ2danO8vkuXbpkHT5x9OhRFBYWon///tZ9+vfvj/79+2PBggW46667kJaWhrvuugsAEB0djejoaMyZMwePP/443n//fZshKxbl5eXw8fGx3rRYnaPAXhuXheI2bdpALpfb9QqfO3fOrqfXIjQ01GF5hUJh7eGtrUxtxwQAX19fXH311Thx4kStZdRqtcOpTZRKZbP+IxTkq0KJvgyFOlOL/MevJWvuPytPwXZzDtvNOWw357XUtjMajZAkCTKZDDKZyye9srJ8pW+pmyMBAQGYNGkSnnzySVy6dAn33nuvtWzXrl2xceNG7Nu3D4GBgXjllVeQm5uLnj172hyv5vEtrxMSEtC9e3dMmTIFy5cvR2FhIRYuXAgA1rbq2rUrsrOz8dFHH2HAgAHYvHmzdUYJyzGjoqKQlZWFn376CR06dIC/v781r1iOk5SUhGeeeQb33nsvFi1ahPPnz2P27NlISkpCWFiYtV6O6lr9XI7azWg04qeffrJpN5VKhYSEBPTp0wdJSUlISUmx3mg3dOhQXHvttSgrK8O///1vTJgwAVFRUThz5gwOHDiA8ePHQyaTYc6cOUhMTES3bt1w4cIFbN++3a5tLWQyGSRJcvh3oCF/J1x2dapUKsTExNh93ZORkYH4+HiH+8TFxdmVT09PR2xsrPVD11amtmMC5q8ejh07Zr0wWrJAX/Pn5M12RERETW/q1Km4cOECbrrpJpuZsRYuXIj+/ftj5MiRGDZsGEJDQzFu3Lh6H1cmk2HTpk3Q6XS49tprMW3aNLzwwgs2ZcaOHYu5c+di1qxZ6NevH/bu3WsNzhbjx4/HqFGjcMMNNyAkJMThtHBarRZff/01CgoKMGDAAEyYMAHDhw/HypUrG9YYDhQXF+Oaa66xeYwePdo6JVxgYCCGDBmCm266CZ07d0ZaWhoA86xf+fn5uPvuu9GtWzdMnDgRiYmJ1qGqRqMRM2fORM+ePTFq1Ch0794dq1atuuL61km40IYNG4RSqRSpqani6NGjYs6cOcLX11ecPHlSCCHEggULRFJSkrX8X3/9JbRarZg7d644evSoSE1NFUqlUnzyySfWMnv27BFyuVwsWbJEHDt2TCxZskQoFAqxb98+a5lHHnlE7NixQ/z1119i37594pZbbhH+/v7W89bHpUuXBABx6dKlRmiJy9Pr9eKzzz4T96TuEx3nfyk+/P5Us5zXE1jaTq/Xu7oqboXt5hy2m3PYbs5r6W1XVlYmjh49KsrKylxdFRtGo1FcuHBBGI1GV1fFrbTEdqvrGmtIXnPpmOJJkyYhPz8fzz77LHJyctC7d29s2bIFHTt2BGBeojA7O9taPioqClu2bMHcuXPx+uuvIzw8HCtWrLBOxwYA8fHx2LBhA5588kksXLgQXbp0QVpams30KGfOnMGdd96JvLw8hISE4LrrrsO+ffus523JgnxVANhTTERERNSYXH6j3YwZMzBjxgyH79VcSQYAhg4detn1xidMmIAJEybU+v6GDRsaVMeWxBKKuYAHERERUeNpOSPeqV6CKscUMxQTERERNR6GYjcTqOXwCSIiIqLGxlDsZizDJy4wFBMRkRsRlesKEDW2xrq2GIrdTJCWwyeIiMh9WJbddflqduSxLKvWXek83S6/0Y4apmr2CZ2La0JERHR5CoUCWq0W58+fh1KpbDELeJhMJuj1epSXl7eYOrmDltRuQgiUlpbi3LlzaN26tfUXMGcxFLsZSyguN5hQqq+AVsU/QiIiarkkSUJYWBiysrJw6tQpV1fHSgiBsrIy+Pj41LrMM9lrie3WunVrhIaGXvFxmKjcjK9KDpVcBr3RhPxiPbRB/CMkIqKWTaVSoWvXri1qCIXBYMB3332HIUOGtMjlsVuqltZuSqXyinuILZio3IwkSQjyVSG3sBwXSvWICNK6ukpERESXJZPJoNFoXF0NK7lcjoqKCmg0mhYR7tyFJ7cbB9G4Ia5qR0RERNS4GIrdULBf5ap2xQzFRERERI2BodgNcalnIiIiosbFUOyGuKodERERUeNiKHZDwVzVjoiIiKhRMRS7oSA/9hQTERERNSaGYjcUbB1TzFXtiIiIiBoDQ7EbCvJVA+CNdkRERESNhaHYDQX5mifL5vAJIiIiosbBUOyGLD3FReUVMBhNLq4NERERkftjKHZDrX2UkEnm55yBgoiIiOjKMRS7IZlM4lzFRERERI2IodhNcVU7IiIiosbDUOymAn3ZU0xERETUWBiK3RRXtSMiIiJqPAzFbiqIPcVEREREjYah2E1xVTsiIiKixsNQ7KZ4ox0RERFR42EodlPWG+2KGYqJiIiIrhRDsZsKrlzV7kIpQzERERHRlWIodlMcPkFERETUeBiK3VSwX+WUbKUGmEzCxbUhIiIicm8MxW7Kssyz0SRwqczg4toQERERuTeGYjelUsjgr1YA4FzFRERERFeKodiNBVmHUDAUExEREV0JhmI3FsRp2YiIiIgaBUOxGwvmDBREREREjYKh2I1ZbrbjUs9EREREV4ah2I1ZxhTzRjsiIiKiK8NQ7MYswycuMBQTERERXRGGYjcWVLnUM3uKiYiIiK4MQ7Eb4412RERERI2DodiNBTIUExERETUKhmI3Vr2nWAjh4toQERERuS+GYjdmWbxDV2FCqd7o4toQERERuS+GYjemVcmhVpj/CDmEgoiIiMh5DMVuTJIk6xAKzkBBRERE5DyGYjdXdbMdV7UjIiIichZDsZsLsoZig4trQkREROS+GIrdXDB7iomIiIiuGEOxm+OqdkRERERXjqHYzQX7VfYUFzMUExERETmLodjNBWq5qh0RERHRlWIodnPWG+1KGYqJiIiInMVQ7OaswyfYU0xERETkNIZiN2ftKeaYYiIiIiKnMRS7OcuUbEW6CugqjC6uDREREZF7Yih2cwEaJeQyCQBwgQt4EBERETmFodjNyWQSArVKABxXTEREROQshmIPULXUM0MxERERkTMYij2AJRTnc6lnIiIiIqcwFHuA4MqlntlTTEREROQchmIPEOjLMcVEREREV4Kh2AMEsaeYiIiI6IowFHuAYN5oR0RERHRFGIo9QNWNdgzFRERERM5gKPYA7CkmIiIiujIMxR4gkKGYiIiI6IowFHsAS0/xxVI9jCbh4toQERERuR+GYg9g6Sk2CeBSmcHFtSEiIiJyPwzFHkAplyFAowAAFHBVOyIiIqIGc3koXrVqFaKioqDRaBATE4Ndu3bVWX7nzp2IiYmBRqNB586dsWbNGrsyGzduRHR0NNRqNaKjo7Fp06Zaj7d48WJIkoQ5c+Zc6UdxqWA/81zF+cUcV0xERETUUC4NxWlpaZgzZw6eeOIJHDp0CIMHD0ZiYiKys7Mdls/KysLo0aMxePBgHDp0CI8//jgefvhhbNy40VomMzMTkyZNQlJSEo4cOYKkpCRMnDgR33//vd3x9u/fjzfffBN9+vRpss/YXAK1XNWOiIiIyFkuDcWvvPIKpk6dimnTpqFnz55ISUlBREQEVq9e7bD8mjVrEBkZiZSUFPTs2RPTpk3Dfffdh2XLllnLpKSkYMSIEUhOTkaPHj2QnJyM4cOHIyUlxeZYxcXFmDx5Mt566y0EBgY25cdsFtZV7UoZiomIiIgaSuGqE+v1evz4449YsGCBzfaEhATs3bvX4T6ZmZlISEiw2TZy5EikpqbCYDBAqVQiMzMTc+fOtStTMxTPnDkTN998M2666SY8//zzl62vTqeDTlc1XrewsBAAYDAYYDA0/c1tlnPUdq5ArfmP8nxhebPUx51cru3IMbabc9huzmG7OY9t5xy2m3Pcrd0aUk+XheK8vDwYjUa0a9fOZnu7du2Qm5vrcJ/c3FyH5SsqKpCXl4ewsLBay1Q/5oYNG3Dw4EHs37+/3vVdvHgxnnnmGbvt6enp0Gq19T7OlcrIyHC4/UKODIAMB3/9HVtKf2u2+riT2tqO6sZ2cw7bzTlsN+ex7ZzDdnOOu7RbaWlpvcu6LBRbSJJk81oIYbftcuVrbq/rmKdPn8bs2bORnp4OjUZT73omJydj3rx51teFhYWIiIhAQkICAgIC6n0cZxkMBmRkZGDEiBFQKpV27+fuOYltZ39HQEg4Ro92/zHSjelybUeOsd2cw3ZzDtvNeWw757DdnONu7Wb5Zr8+XBaK27RpA7lcbtcrfO7cObueXovQ0FCH5RUKBYKDg+ssYznmjz/+iHPnziEmJsb6vtFoxHfffYeVK1dCp9NBLpfbnVutVkOtVtttVyqVzXpR1Ha+Nv4+AICLZRVucZG6QnP/WXkKtptz2G7OYbs5j23nHLabc9yl3RpSR5fdaKdSqRATE2PX/Z6RkYH4+HiH+8TFxdmVT09PR2xsrPVD11bGcszhw4fj559/xuHDh62P2NhYTJ48GYcPH3YYiN1BkB+XeiYiIiJylkuHT8ybNw9JSUmIjY1FXFwc3nzzTWRnZ2P69OkAzEMW/v77b6xfvx4AMH36dKxcuRLz5s3D/fffj8zMTKSmpuLDDz+0HnP27NkYMmQIli5dirFjx+Lzzz/Htm3bsHv3bgCAv78/evfubVMPX19fBAcH2213J5alnhmKiYiIiBrOpaF40qRJyM/Px7PPPoucnBz07t0bW7ZsQceOHQEAOTk5NnMWR0VFYcuWLZg7dy5ef/11hIeHY8WKFRg/fry1THx8PDZs2IAnn3wSCxcuRJcuXZCWloaBAwc2++drTkHVQvHlxmUTERERkS2X32g3Y8YMzJgxw+F769ats9s2dOhQHDx4sM5jTpgwARMmTKh3HXbs2FHvsi1VcOU8xXqjCcW6CvhrWv44HyIiIqKWwuXLPFPj8FHJoVGa/zg5hIKIiIioYRiKPYilt5ihmIiIiKhhGIo9SBBvtiMiIiJyCkOxB7GE4nyGYiIiIqIGYSj2IJyWjYiIiMg5DMUeJJChmIiIiMgpDMUehGOKiYiIiJzDUOxBOHyCiIiIyDkMxR6EN9oREREROYeh2IME+1l6inUurgkRERGRe2Eo9iCBWnMovlBicHFNiIiIiNwLQ7EHsaxoV6yrgK7C6OLaEBEREbkPhmIPEuCjgEImAeDNdkREREQNwVDsQSRJss5VnF/MUExERERUXwzFHiZIy2nZiIiIiBqKodjDWKZlu1DKUExERERUXwzFHibIj8MniIiIiBqKodjDcFU7IiIiooZjKPYwXNWOiIiIqOEYij1MkC9XtSMiIiJqKIZiD2O90Y6r2hERERHVG0Oxh6kaPsGeYiIiIqL6Yij2MJalnnmjHREREVH9MRR7GEtP8cUyA4wm4eLaEBEREbkHhmIP01qrBAAIwQU8iIiIiOqLodjDKOUytPIxB+MLHEJBREREVC8MxR4omHMVExERETUIQ7EHCuKqdkREREQNwlDsgbiqHREREVHDMBR7IGtPcTFDMREREVF9MBR7IOuqdpx9goiIiKheGIo9EIdPEBERETUMQ7EHCvaz3GjHpZ6JiIiI6oOh2AMFVS71nM8xxURERET1wlDsgYK0nJKNiIiIqCEYij1QkF/VjXZCCBfXhoiIiKjlYyj2QJYV7QxGgSJdhYtrQ0RERNTyMRR7II1SDq1KDoBzFRMRERHVB0Oxh+K0bERERET1x1Dsoayr2jEUExEREV0WQ7GHsq5qx1BMREREdFkMxR6KwyeIiIiI6o+h2EMF+3JVOyIiIqL6Yij2UNZV7dhTTERERHRZDMUeKshXCYA32hERERHVB0Oxh7L0FPNGOyIiIqLLYyj2ULzRjoiIiKj+GIo9VDDnKSYiIiKqN4ZiDxXkZw7FpXojyg1GF9eGiIiIqGVjKPZQ/moFlHIJAIdQEBEREV0OQ7GHkiQJgVquakdERERUHwzFHow32xERERHVD0OxBwv246p2RERERPXBUOzBrKvaFbOnmIiIiKguDMUeLEjLVe2IiIiI6oOh2INZV7UrZSgmIiIiqgtDsQezzFXM4RNEREREdWMo9mBc1Y6IiIiofhiKPVgQQzERERFRvTAUezBrKOaYYiIiIqI6MRR7MEsovlhqQIXR5OLaEBEREbVcDMUeLFCrgiSZn18oNbi2MkREREQtGEOxB5PLJLT24VzFRERERJfDUOzhLEMo8rnUMxEREVGtGIo9nCUUXyjh8AkiIiKi2jAUe7iqadnYU0xERERUG4ZiD2dZ6jmfY4qJiIiIasVQ7OG4qh0RERHR5TEUe7iqG+0YiomIiIhq4/JQvGrVKkRFRUGj0SAmJga7du2qs/zOnTsRExMDjUaDzp07Y82aNXZlNm7ciOjoaKjVakRHR2PTpk02769evRp9+vRBQEAAAgICEBcXh6+++qpRP1dLUXWjHUMxERERUW1cGorT0tIwZ84cPPHEEzh06BAGDx6MxMREZGdnOyyflZWF0aNHY/DgwTh06BAef/xxPPzww9i4caO1TGZmJiZNmoSkpCQcOXIESUlJmDhxIr7//ntrmQ4dOmDJkiU4cOAADhw4gBtvvBFjx47Fr7/+2uSfubkFcfgEERER0WW5NBS/8sormDp1KqZNm4aePXsiJSUFERERWL16tcPya9asQWRkJFJSUtCzZ09MmzYN9913H5YtW2Ytk5KSghEjRiA5ORk9evRAcnIyhg8fjpSUFGuZMWPGYPTo0ejWrRu6deuGF154AX5+fti3b19Tf+Rmx+ETRERERJencNWJ9Xo9fvzxRyxYsMBme0JCAvbu3etwn8zMTCQkJNhsGzlyJFJTU2EwGKBUKpGZmYm5c+falakeiqszGo34+OOPUVJSgri4uFrrq9PpoNNVTWtWWFgIADAYDDAYmn4OYMs5GnquALX5954LJXro9XpIlnWfvYizbeft2G7OYbs5h+3mPLadc9huznG3dmtIPV0WivPy8mA0GtGuXTub7e3atUNubq7DfXJzcx2Wr6ioQF5eHsLCwmotU/OYP//8M+Li4lBeXg4/Pz9s2rQJ0dHRtdZ38eLFeOaZZ+y2p6enQ6vV1vlZG1NGRkaDyhtMAKBAhUlg4/++gtZlf+Ku19C2IzO2m3PYbs5huzmPbecctptz3KXdSktL613W5RGpZs+lEKLO3kxH5Wtur88xu3fvjsOHD+PixYvYuHEj7rnnHuzcubPWYJycnIx58+ZZXxcWFiIiIgIJCQkICAio4xM2DoPBgIyMDIwYMQJKpbJB+z596BuU6I2IHTQUnYJ9m6iGLdeVtJ03Y7s5h+3mHLab89h2zmG7Ocfd2s3yzX59uCwUt2nTBnK53K4H99y5c3Y9vRahoaEOyysUCgQHB9dZpuYxVSoVrrrqKgBAbGws9u/fj9deew1vvPGGw3Or1Wqo1Wq77UqlslkvCmfOF+SnQklBGQp1Jre4gJtKc/9ZeQq2m3PYbs5huzmPbecctptz3KXdGlJHl91op1KpEBMTY9f9npGRgfj4eIf7xMXF2ZVPT09HbGys9UPXVqa2Y1oIIWzGDHsS66p2xbzZjoiIiMgRlw6fmDdvHpKSkhAbG4u4uDi8+eabyM7OxvTp0wGYhyz8/fffWL9+PQBg+vTpWLlyJebNm4f7778fmZmZSE1NxYcffmg95uzZszFkyBAsXboUY8eOxeeff45t27Zh9+7d1jKPP/44EhMTERERgaKiImzYsAE7duzA1q1bm7cBmglXtSMiIiKqm0tD8aRJk5Cfn49nn30WOTk56N27N7Zs2YKOHTsCAHJycmzmLI6KisKWLVswd+5cvP766wgPD8eKFSswfvx4a5n4+Hhs2LABTz75JBYuXIguXbogLS0NAwcOtJb5559/kJSUhJycHLRq1Qp9+vTB1q1bMWLEiOb78M0oUMtp2YiIiIjq4vIb7WbMmIEZM2Y4fG/dunV224YOHYqDBw/WecwJEyZgwoQJtb6fmpraoDq6u2A/rmpHREREVBeXL/NMTY+r2hERERHVjaHYC3BVOyIiIqK6MRR7Ad5oR0RERFQ3hmIvEMhQTERERFQnhmIvwJ5iIiIioroxFHsBy5jiMoMRZXqji2tDRERE1PIwFHsBP7UCKrn5jzq/xDNX7SMiIiK6EgzFXkCSJE7LRkRERFQHhmIvEchp2YiIiIhqxVDsJSw323FVOyIiIiJ7DMVegsMniIiIiGrnVCg+ffo0zpw5Y339ww8/YM6cOXjzzTcbrWLUuLiqHREREVHtnArFd911F7Zv3w4AyM3NxYgRI/DDDz/g8ccfx7PPPtuoFaTGYZ2ruJihmIiIiKgmp0LxL7/8gmuvvRYA8NFHH6F3797Yu3cv/vvf/2LdunWNWT9qJLzRjoiIiKh2ToVig8EAtVoNANi2bRtuvfVWAECPHj2Qk5PTeLWjRmO90a6UoZiIiIioJqdCca9evbBmzRrs2rULGRkZGDVqFADg7NmzCA4ObtQKUuPgjXZEREREtXMqFC9duhRvvPEGhg0bhjvvvBN9+/YFAHzxxRfWYRXUsgT7VQ6fKOaKdkREREQ1KZzZadiwYcjLy0NhYSECAwOt2x944AFotdpGqxw1niBf83CXwvIKGIwmKOWcjY+IiIjIwqlkVFZWBp1OZw3Ep06dQkpKCo4fP462bds2agWpcbTyUUKSzM+5gAcRERGRLadC8dixY7F+/XoAwMWLFzFw4EAsX74c48aNw+rVqxu1gtQ45DIJgdrKccW82Y6IiIjIhlOh+ODBgxg8eDAA4JNPPkG7du1w6tQprF+/HitWrGjUClLjCeJcxUREREQOORWKS0tL4e/vDwBIT0/H7bffDplMhuuuuw6nTp1q1ApS4+GqdkRERESOORWKr7rqKnz22Wc4ffo0vv76ayQkJAAAzp07h4CAgEatIDWeYE7LRkREROSQU6H4qaeewqOPPopOnTrh2muvRVxcHABzr/E111zTqBWkxsNV7YiIiIgcc2pKtgkTJuD6669HTk6OdY5iABg+fDhuu+22RqscNS7rqnYMxUREREQ2nArFABAaGorQ0FCcOXMGkiShffv2XLijheOqdkRERESOOTV8wmQy4dlnn0WrVq3QsWNHREZGonXr1njuuedgMpkau47USKputOOqdkRERETVOdVT/MQTTyA1NRVLlizBoEGDIITAnj17sGjRIpSXl+OFF15o7HpSIwiuXNWOPcVEREREtpwKxe+++y7efvtt3HrrrdZtffv2Rfv27TFjxgyG4hYq0FcJACgoMbi4JkREREQti1PDJwoKCtCjRw+77T169EBBQcEVV4qahqWn+EKpHiaTcHFtiIiIiFoOp0Jx3759sXLlSrvtK1euRJ8+fa64UtQ0LD3FRpNAYTl7i4mIiIgsnBo+8dJLL+Hmm2/Gtm3bEBcXB0mSsHfvXpw+fRpbtmxp7DpSI1Er5PBXK1Ckq0B+iR6ttSpXV4mIiIioRXCqp3jo0KH4/fffcdttt+HixYsoKCjA7bffjl9//RVr165t7DpSIwry47RsRERERDU5PU9xeHi43Q11R44cwbvvvot33nnniitGTSNQq8Kp/FKGYiIiIqJqnOopJvcVzAU8iIiIiOwwFHsZrmpHREREZI+h2MtYxhTnFzMUExEREVk0aEzx7bffXuf7Fy9evJK6UDOoGj7BpZ6JiIiILBoUilu1anXZ9+++++4rqhA1rcDKadgKSjlPMREREZFFg0Ixp1tzf8F+7CkmIiIiqoljir1MUOVSzwUcU0xERERkxVDsZSxjivNL9BBCuLg2RERERC0DQ7GXsUzJpqswoVRvdHFtiIiIiFoGhmIvo1XJoVKY/9g5VzERERGRGUOxl5EkiavaEREREdXAUOyFuKodERERkS2GYi8UVO1mOyIiIiJiKPZKXNWOiIiIyBZDsRcKtIZirmpHREREBDAUeyX2FBMRERHZYij2QtZV7TimmIiIiAgAQ7FX4o12RERERLYYir0Qp2QjIiIissVQ7IUYiomIiIhsMRR7IcuNdkXlFdBXmFxcGyIiIiLXYyj2Qq18lJDLJADAhVL2FhMRERExFHshmUxCoFYJAMgvZigmIiIiYij2UoFajismIiIismAo9lLWm+04fIKIiIiIodhbBftVhuJirmpHRERExFDspTgtGxEREVEVhmIvZVnqmavaERERETEUe62gytkn2FNMRERExFDstYL8zD3FDMVEREREDMVeK5hjiomIiIisGIq9FG+0IyIiIqrCUOylLD3FF0r1MJmEi2tDRERE5FoMxV6qdeWKdiYBXCwzuLg2RERERK7FUOylVAoZ/DUKABxCQURERMRQ7MV4sx0RERGRmctD8apVqxAVFQWNRoOYmBjs2rWrzvI7d+5ETEwMNBoNOnfujDVr1tiV2bhxI6Kjo6FWqxEdHY1NmzbZvL948WIMGDAA/v7+aNu2LcaNG4fjx4836udyB1U323GpZyIiIvJuLg3FaWlpmDNnDp544gkcOnQIgwcPRmJiIrKzsx2Wz8rKwujRozF48GAcOnQIjz/+OB5++GFs3LjRWiYzMxOTJk1CUlISjhw5gqSkJEycOBHff/+9tczOnTsxc+ZM7Nu3DxkZGaioqEBCQgJKSkqa/DO3JFzVjoiIiMhM4cqTv/LKK5g6dSqmTZsGAEhJScHXX3+N1atXY/HixXbl16xZg8jISKSkpAAAevbsiQMHDmDZsmUYP3689RgjRoxAcnIyACA5ORk7d+5ESkoKPvzwQwDA1q1bbY67du1atG3bFj/++COGDBnisK46nQ46XVWPamFhIQDAYDDAYGj6G9Us52jMc7X2Mf/xny8sb5bP4CpN0XbegO3mHLabc9huzmPbOYft5hx3a7eG1NNloViv1+PHH3/EggULbLYnJCRg7969DvfJzMxEQkKCzbaRI0ciNTUVBoMBSqUSmZmZmDt3rl0ZS5B25NKlSwCAoKCgWsssXrwYzzzzjN329PR0aLXaWvdrbBkZGY12rAu5MgAyHPr1d2wp/a3RjttSNWbbeRO2m3PYbs5huzmPbecctptz3KXdSktL613WZaE4Ly8PRqMR7dq1s9nerl075ObmOtwnNzfXYfmKigrk5eUhLCys1jK1HVMIgXnz5uH6669H7969a61vcnIy5s2bZ31dWFiIiIgIJCQkICAgoM7P2hgMBgMyMjIwYsQIKJXKRjlmzp6T+Obs7whoG47Ro/s0yjFboqZoO2/AdnMO2805bDfnse2cw3Zzjru1m+Wb/fpw6fAJAJAkyea1EMJu2+XK19zekGPOmjULP/30E3bv3l1nPdVqNdRqtd12pVLZrBdFY54vxN8HAHCxrMItLuwr1dx/Vp6C7eYctptz2G7OY9s5h+3mHHdpt4bU0WWhuE2bNpDL5XY9uOfOnbPr6bUIDQ11WF6hUCA4OLjOMo6O+dBDD+GLL77Ad999hw4dOlzJx3FLQX7m2Sfyi3mjHREREXk3l80+oVKpEBMTYzcmJSMjA/Hx8Q73iYuLsyufnp6O2NhY628CtZWpfkwhBGbNmoVPP/0U3377LaKiohrjI7mdIG3VUs9ERERE3sylwyfmzZuHpKQkxMbGIi4uDm+++Says7Mxffp0AOZxvH///TfWr18PAJg+fTpWrlyJefPm4f7770dmZiZSU1Ots0oAwOzZszFkyBAsXboUY8eOxeeff45t27bZDI+YOXMm/vvf/+Lzzz+Hv7+/tWe5VatW8PHxacYWcC3LPMX5JfrLDlshIiIi8mQuDcWTJk1Cfn4+nn32WeTk5KB3797YsmULOnbsCADIycmxmbM4KioKW7Zswdy5c/H6668jPDwcK1assE7HBgDx8fHYsGEDnnzySSxcuBBdunRBWloaBg4caC2zevVqAMCwYcNs6rN27VpMmTKl6T5wCxNcOXxCX2FCid4IP7XLh5gTERERuYTLU9CMGTMwY8YMh++tW7fObtvQoUNx8ODBOo85YcIETJgwodb3LTfneTutSgGNUoZygwkFxXqGYiIiIvJaLl/mmVwr2LqqHZd6JiIiIu/FUOzlAn3NNyjyZjsiIiLyZgzFXi7I0lPMadmIiIjIizEUe7ngyhkoCkoYiomIiMh7MRR7uSCGYiIiIiKGYm9Xfa5iIiIiIm/FUOzlLKH4AkMxEREReTGGYi/HnmIiIiIihmKvxxvtiIiIiBiKvR5vtCMiIiJiKPZ6lhXtinUV0FUYXVwbIiIiItdgKPZy/hoF5DIJAHChxODi2hARERG5BkOxl5PJJARqLTfb6VxcGyIiIiLXYCgm3mxHREREXo+hmHizHREREXk9hmJCkF/l8IlihmIiIiLyTgzFhKDKMcUXShmKiYiIyDsxFBNXtSMiIiKvx1BMCK4cPlHA4RNERETkpRiKiTfaERERkddjKKZqwyc4TzERERF5J4ZisobiC6Vc0Y6IiIi8E0MxVQvFehhNwsW1ISIiImp+DMVkXeZZCOAip2UjIiIiL8RQTFDKZWjlowTAm+2IiIjIOzEUEwAgmHMVExERkRdjKCYAQKBlXDFDMREREXkhhmICwFXtiIiIyLsxFBOAquETHFNMRERE3oihmABwVTsiIiLybgzFBIDDJ4iIiMi7MRQTgGoLeDAUExERkRdiKCYA7CkmIiIi78ZQTACAYF81AKCgROfimhARERE1P4ZiAgAE+VXdaCeEcHFtiIiIiJoXQzEBAIK05lBsMAoU6SpcXBsiIiKi5sVQTAAAH5UcPko5AN5sR0RERN6HoZiseLMdEREReSuGYrIKtowrLmYoJiIiIu/CUExWXNWOiIiIvBVDMVlZbrbj8AkiIiLyNgzFZGVd1a6UoZiIiIi8C0MxWVnmKs7nmGIiIiLyMgzFZBVsHVPMVe2IiIjIuzAUk1WQdaln9hQTERGRd2EoJqsgXyUAoIBjiomIiMjLMBSTlbWnmGOKiYiIyMswFJOVZfaJEr0R5Qaji2tDRERE1HwYiskqQKOAUi4B4LhiIiIi8i4MxWQlSRICtVzVjoiIiLwPQzHZ4FLPRERE5I0YiskGQzERERF5I4ZismEJxfkMxURERORFGIrJBle1IyIiIm/EUEw2uKodEREReSOGYrJhXdWOoZiIiIi8CEMx2WBPMREREXkjhmKywRvtiIiIyBsxFJONYD9OyUZERETeh6GYbFh6ii+WGlBhNLm4NkRERETNg6GYbLT2UVqfXywzuLAmRERERM2HoZhsKOQytNZyBgoiIiLyLgzFZMd6s10xQzERERF5B4ZislO1qh1DMREREXkHhmKyE8SlnomIiMjLMBSTnapQzBvtiIiIyDswFJMd9hQTERGRt2EoJjuWpZ65qh0RERF5C4ZissMb7YiIiMjbuDwUr1q1ClFRUdBoNIiJicGuXbvqLL9z507ExMRAo9Ggc+fOWLNmjV2ZjRs3Ijo6Gmq1GtHR0di0aZPN+9999x3GjBmD8PBwSJKEzz77rDE/ktsLYigmIiIiL+PSUJyWloY5c+bgiSeewKFDhzB48GAkJiYiOzvbYfmsrCyMHj0agwcPxqFDh/D444/j4YcfxsaNG61lMjMzMWnSJCQlJeHIkSNISkrCxIkT8f3331vLlJSUoG/fvli5cmWTf0Z3xFBMRERE3kbhypO/8sormDp1KqZNmwYASElJwddff43Vq1dj8eLFduXXrFmDyMhIpKSkAAB69uyJAwcOYNmyZRg/frz1GCNGjEBycjIAIDk5GTt37kRKSgo+/PBDAEBiYiISExOb4RO6J0sovlCqhxACkiS5uEZERERETctloViv1+PHH3/EggULbLYnJCRg7969DvfJzMxEQkKCzbaRI0ciNTUVBoMBSqUSmZmZmDt3rl0ZS5B2lk6ng05XNRtDYWEhAMBgMMBgaPqpyyznaI5zBajMIdhgFCgoKkOAj7LJz9mUmrPtPAnbzTlsN+ew3ZzHtnMO28057tZuDamny0JxXl4ejEYj2rVrZ7O9Xbt2yM3NdbhPbm6uw/IVFRXIy8tDWFhYrWVqO2Z9LV68GM8884zd9vT0dGi12is6dkNkZGQ0y3nUMjl0JgmfbslAW59mOWWTa6628zRsN+ew3ZzDdnMe2845bDfnuEu7lZaW1rusS4dPALD7av5yX9c7Kl9ze0OPWR/JycmYN2+e9XVhYSEiIiKQkJCAgICAKzp2fRgMBmRkZGDEiBFQKpu+5/bl33bhzIUy9BkQj/6RrZv8fE2pudvOU7DdnMN2cw7bzXlsO+ew3Zzjbu1m+Wa/PlwWitu0aQO5XG7Xg3vu3Dm7nl6L0NBQh+UVCgWCg4PrLFPbMetLrVZDrVbbbVcqlc16UTTX+YJ9VThzoQyFOpNbXPT10dx/Vp6C7eYctptz2G7OY9s5h+3mHHdpt4bU0WWzT6hUKsTExNh1v2dkZCA+Pt7hPnFxcXbl09PTERsba/3QtZWp7ZjkGFe1IyIiIm/i0uET8+bNQ1JSEmJjYxEXF4c333wT2dnZmD59OgDzkIW///4b69evBwBMnz4dK1euxLx583D//fcjMzMTqamp1lklAGD27NkYMmQIli5dirFjx+Lzzz/Htm3bsHv3bmuZ4uJi/PHHH9bXWVlZOHz4MIKCghAZGdlMn75l46p2RERE5E1cGoonTZqE/Px8PPvss8jJyUHv3r2xZcsWdOzYEQCQk5NjM2dxVFQUtmzZgrlz5+L1119HeHg4VqxYYZ2ODQDi4+OxYcMGPPnkk1i4cCG6dOmCtLQ0DBw40FrmwIEDuOGGG6yvLWOF77nnHqxbt66JP7V7CPar7CkuZigmIiIiz+fyG+1mzJiBGTNmOHzPUUAdOnQoDh48WOcxJ0yYgAkTJtT6/rBhw6w36JFjXMCDiIiIvInLl3mmlilIWxmKSxmKiYiIyPMxFJND7CkmIiIib8JQTA4FVY4pzueYYiIiIvICDMXkUDB7iomIiMiLMBSTQ5bhE2UGI8r0RhfXhoiIiKhpMRSTQ35qBZRy89LYvNmOiIiIPB1DMTkkSVLVzXYcV0xEREQejqGYalW1qh2XeiYiIiLPxlBMteLNdkREROQtGIqpVpyrmIiIiLwFQ7G7KMpFz7MfAUZDs52SoZiIiIi8BUOxOzAZoXhvDLr98yVkP77TbKdlKCYiIiJvwVDsDmRyGK+bZX666yWgJL9ZTmsJxfkMxUREROThGIrdhOj3f7jkEwmp/BKw48VmOSdvtCMiIiJvwVDsLmRy/Nx+svn5gXeAf442+SkDK0PxBYZiIiIi8nAMxW4k378nTD3GAMIEfJ0MCNGk5wvm8AkiIiLyEgzFbsY4fBEgVwN/7QCOf9Wk57KMKb5UZoDBaGrScxERERG5EkOxu2ndEYibaX7+9eNARdOtNtdaq4IkmZ9fKGVvMREREXkuhmJ3NHge4NcOuJAFfL+myU4jl0kI1PJmOyIiIvJ8DMXuSO0PDH/a/Hzny0DxuSY7VaBWCYChmIiIiDwbQ7G76nsnEH4NoC8Cvn2uyU4T7KsGwFBMREREno2h2F3JZMCoJebnB98Dco40yWm4qh0RERF5A4ZidxZ5HdB7AgABbG2aKdqC/CqnZStmKCYiIiLPxVDs7kY8Ayh8gFN7gKOfN/rhuaodEREReQOGYnfXqgMwaLb5efpCwFDWqIe3zj7BKdmIiIjIgzEUe4JBs4GA9sClbCBzZaMeOrhy+EQBh08QERGRB2Mo9gQqLXDTM+bnu14FCnMa7dC80Y6IiIi8AUOxp7h6AtDhWsBQAnzzTKMd1hKK8xmKiYiIyIMxFHsKSQISK6doO/IhcObHRjmsZZ7iC6V6mEyNP7sFERERUUvAUOxJ2scAfe8yP9+6oFGmaAv0Na9oZzQJFJVXXPHxiIiIiFoihmJPM/wpQOkLnPkB+PmTKz6cWiGHn1oBAMgv0V3x8YiIiIhaIoZiTxMQBgyeZ36e8RSgL7niQ/JmOyIiIvJ0DMWeKG4W0DoSKDoL7Hntig/Hm+2IiIjI0zEUeyKlBhjxnPn5nteAi6ev6HBc1Y6IiIg8HUOxp4oeC3QcBFSUA9uevqJDBTIUExERkYdjKPZUkgSMWgJAAn7ZCGTvc/pQ7CkmIiIiT8dQ7MnC+gD97zY//2o+YDI5dRjLmOKfz1zCpVJDY9WOiIiIqMVgKPZ0Ny4E1AFAzmHzoh5O6BbqDwD44WQBhry8HW999xfKDcZGrCQRERGRazEUezq/EGDIv83Pv3kG0BU1+BA3dG+LtVMGoFs7P1wqM+CFLccwfPlObDp0hqvcERERkUdgKPYGA6cDQZ2B4n+AXcudOsQNPdriq9lD8NL4PggN0ODvi2WYm3YEt/xnN777/XwjV5iIiIioeTEUewOFCkh4wfw883WgIMupw8hlEiYOiMD2R4fhsVHd4a9W4GhOIe5+5wf839vf45e/LzVipYmIiIiaD0Oxt+ieCHQeBhj1QMbCKzqUj0qOGcOuwneP3YCp10dBKZew+4883PKf3Ziz4RBOF5Q2Tp2JiIiImglDsbeQJGDkYkCSA8f+B2TtuuJDBvqqsPCWaHz7yDCM7RcOAPjs8FkMX74Tz395FBc4hRsRERG5CYZib9IuGoi9z/x8azJgapwZJCKCtHjtjmvwv1nXY9BVwdAbTXh7dxaGvLwdq3f8yZkqiIiIqMVjKPY2NzwOaFoD//wMHFzfqIe+ukMrvD91IN6971r0DAtAUXkFlm79DTcs24GPDpyGkTNVEBERUQvFUOxttEHAsGTz82+fA8ouNurhJUnC0G4h2PzQ9XhlYl+0b+2DnEvleOyTnzD6tV3Y/ts5CMFwTERERC0LQ7E3GjAVaNMdKM0Hvnu5SU4hk0m4vX8HfPPIUDw+ugcCNAoc/6cI967bjzvf2ocjpy82yXmJiIiInMFQ7I3kSmDki+bn368B8v5oslNplHI8MKQLvnvsBjwwpDNUChn2/VWAsa/vwaz/HsSp/JImOzcRERFRfTEUe6uuNwFdRwKmCiD9iSY/XWutCo+P7ontjw7D7f3bQ5KAL3/KwU2v7MSiL35FfrGuyetAREREVBuGYm828gVApgB+3wr88U2znLJ9ax+8MrEfNj80GEO6hcBgFFi39ySGvrwDK789gVJ9RbPUg4iIiKg6hmJv1qYrcO3/Mz//+nHA2HyBNDo8AOvvuxYfTBuI3u0DUKyrwLL03zHs5R348IdsVBhNzVYXIiIiIoZibzf0MUAbDJz/DTjwTrOfftBVbfDFzOvx2h390CHQB+eKdEj+9GeMem0XMo7+w5kqiIiIqFkwFHs7n9bADZVjire/AJQWNHsVZDIJY/u1xzePDMXCW6IRqFXij3PFuH/9AUx8IxMHsy80e52IiIjIuzAUE9D/HqBtL6D8IrBjscuqoVbIMfX6KOx87AbMGNYFaoUM+09ewO2r9mLimkw89+VRfPLjGRw9Wwh9BYdXEBERUeNRuLoC1ALIFcCoxcD6W4H9qealoNv2dFl1AjRKPDaqB5LiOuLVjN/xyY9n8MPJAvxwsqoXWymXcFVbf/QM80d0WACiwwLQMywAgb4ql9WbiIiI3BdDMZl1Hgr0uAX47UvzTXf/9ykgSS6tUlgrH7w0oS9mDLsKP5wswNGzhTiWU4ijOYUoKq/AsRzz60/xd7V9NNaAHB1u/hnur3ThpyAiIiJ3wFBMVRKeB06kA39+C/z+NdB9lKtrBADo1MYXndr4Wl8LIfD3xbLKkFyEozmXcCynCNkFpci5VI6cS+X45rdz1vJalRxtVXJ8bzyKXu1bIzosAN1D/aFV8fInIiIiM6YCqhIUBVw3A9iTYu4t7nIjoHDRcARjBZBzGMj6Dji1F5DJgcg4oNP1kML6okOgFh0CtUjoFWrdpajcgN9yi2x6lI/nFqFUb8RJvYSTP5wBcAaAuRM8qo2vuUe52vCLdgFqSC7uISciIqLmx1BMtoY8Chz+L1DwJ/DDm0D8rOY5r8kI5BwBTu4CsnYB2ZmAvti2zO9bzT+VvkDEtUDHQUDHeKB9DKDUwF+jxIBOQRjQKci6S4XRhN9zL2HD1l1Qt+uC4+dKcPRsIfKKdfjrfAn+Ol+CzT/lWMsH+aqs45QtQzC6hPhBKec9qURERJ6MoZhsqf2B4U8BX8wCdi4F+kwC/EIa/zwmI/DPL+YAfHKXuTdYV2hbRtMa6HQ90GkwIIzAyT1A9l6g7ALw13bzAwDkaqBDbFVIjrgWUJmHWyjkMnRt64eYNgKjR3aDUmkeX3yuqBzHcorMPcqVPct/ni9GQYkee/7Ix54/8q3VUMgkhLf2QWSQFhFBPogI0iKy8hERqEVrrZK9y0RERG6OoZjs9ZsM7H/L3HO7/XlgzGtXfkyTCTj3K3BytzkIn9pjngKuOnUrc6iNGmwOwu16A7JqPbRxM83HOX/MHKJP7jb/LDlnPt6pPeZyMgUQfk1lSB4EhMfaVaetvwZt/TUY2q0q8JcbjPj9n6rhF5bQXKSrQHZBKbILSh1+NH+1AhGVgdkSljtU/mzf2gcapfxKW4+IiIiaGEMx2ZPJgFFLgLWJwMH1wIBpQOjVDTuGEMC5Y+Ze4JO7zL28ZTUWBlH5m0Nwp+vNQTi0j3ns8OXq1q6X+XHt/ebz5P9hDsQnK4Nx4d/Amf3mx54UKCQZhmoiIVPuNZ+nYzygDbI7tEYpR58OrdGnQ+tqH0Mgt7AcpwvKrMH4TOXP7IJSnCvSoUhXgaOVY5hrkiSgnb+mspfZNjhHBGkR4qeGTMZeZiIiIldjKCbHOsYDvW4Hfv0U2JoM3PO/uqdoEwLI+918Y9zJ3eZHaZ5tGaUvEHldZU/wECCsr3mO5CshSUCbruZHzBRzPS6equxJNodk6UIWWpedBH5YY34AQNvoquEWHQcB/u1qObyEsFY+CGvlg2uj7IN0ucGIMxfMAbl6cD5d+SjRG5FbWI7cwnKbeZYt1AqZOSwH+lQLzlWh2U/Nv6JERETNgf/HpdqNeBY4vsXc03vsf0D0rVXvCQHk/wmc/K5yXPBu8zCG6hQ+5hDc6Xogaoh5SIO8iecMliQgsJP50e8uAIAh/xSOfLEa1wSVQn56H5B3HDh31PzY/5Z5v+CrqoZbdBoEtOpQr9NplHJc1dYfV7X1t3tPCIGCEj1OXyizCcrZBaU4faEUZy+WQ1dhwh/nivHHuWIHRwcCtUq08lFCq1LAVy2Hj0oBX5W82ms5fFUKaFVy+KrNP7WWMmrzT2sZtRwquYzjn4mIiBxgKKbatY4A4h8GvnsJSH8SCOkOZO+rHA6xGyjKsS2v0JhvcutUOSa4fYzrpnSrLiAcfwfFoe/o0ZArlUDxefMNeyf3mHuU//nFPAQj/w/g4LvmfVpHAh2vN/ckR8aZp6u73NCOGiRJQrCfGsF+avSLaG33vsFoQs7Fcpy+UGrXw5xdUIoLpQbro7EoZJJNSPZVKSpfy6FVydFBOo9OFSfRQf8X2pX/hfbFpfit5AeYgq4C2nSDIuQq+Pn5wV+jhL9GwVk5iIjIYzAUU92unwMcet88JOH1a23fk6uADtdWjQluHwsoNS6pZoP4hQDRY80PACgtAE5/X3nj3h7zDYYXs4GL/wWO/NdcRq6uHKbRzfzLQZtuQEgPILgLoFA7VQ2lXIbIYC0ig7UY5OD9onID/r5YhhJdBUp0RpTqq36W6o0o0RtRqqsw/6zcVr1Mic6IMoMRJboK6CpMAIAKk0BReQVEeSHaS6fRU5aNHlI2eshOo7t0Gv5SmX1Fft9jfWoSEs6INvhJhONPEY5sWTj+UUYgT9MRBp+21rBsfihtf6oVDt/njYhERNQSMBRT3VS+wMgXgE/uBWRK89RnlmnSIq4FlD6uruGV0wYB3RPNDwDQFVWG5Mqe5LOHAKPO3KP8zy+2+0py81CNkB5ASDegTfeq0Kz2u6Jq+WuU6BF6hcNNjBVAwV8w5vyMitxfIHJ/gfz8MSiLTjsuLimQ7xOFHE0XnFZ0woULBeiovIB2+myEV5yBv1SCSOk8InEew3CkcicAJUBhsQ/+EmH4S4TjT1N45fMwnBSh0KH2bwxUchn8rEFZAX+1OSz7aSqHhVT2ZluGhtj+rPFcrYCPUg45b14ksmUymm9CvnDS/lF8DlAHmP8t1AZXewTZ/vSp/KnyrfseEyI3xVBMl9f7dqDDAPM/jCrfy5d3d2p/4KqbzA/A/D+Ti6eA878D538z31B4/rj5oS8yL3RS8CdwfLPtcQI6mINySI+qHuaQHg5nvmgUJXmVwf3Xqsf534CKcsgB2PXHBrSvmsmjrfmnvE1XtJUr0RZAtMGALVu2IG70aPP8zkIAJeeBvBOoOP87Kv45DpF3ArKCE1AVnUYAytBP+gv98JfNyUyQkKdohzOyDjiJcPxhCsMxQyh+0bXFebSG3mhCQYkeBSX6RmsKjVIGrcockC1jsbXVnlvGWtcWsH1UcsgkCRLMw2AkCTDnbPNz63ag8nXldgkwVhjxdwlwPLcISqWyWvnq+9jvCwAymXm7TJIgkwFySYJcJkEmk6qeSxIUlduIbOiKgLwzjoPvxWzA1EhDseTqasE50DZI+9QI0pbnSi2DNLV4DMVUP60jXF0D15HJgaDO5kf3UVXbhTCPqz5/vDIo/2YOznnHzeGx8Iz58ee3tsfTtrEdgmHpYQ4Ir9//NCp05nP+82tVCD53FCj+x3F5pdY820a7Xua5n9tFm183NJxLEuDXFvBrC0WnQbb/eFTogIIsczvknwDyqh4y3SW0rchFW+Siv/VYADSAUPvD0LoLygI6o8gvChe1HXFO3RG58nAUVshRqjeiTG8eIlKmNw8FKTMYzcNHKp+X6MxlSg1GCGE+fLnBhHJD44XshlPgpZ8ym/ws8sqwbAnQMplUbVu1IF3tfUVlsJbLqkK2zXFkEuQyGeQSzD9lgEImq7FvjfckCQq5bWCXO9hW/ae8eh1kEmAy4ki+BMXRfyC/0llp6kMYIa8og6KiBPKKUsgryiAplJCpfCGp/SDT+EOuVEOlkEMpl0Ehl6Cq/KmUy6CUyaBUSFDIZFDKpea5gbWW3l55QRZGnTsB5aGiuveXKYHAjlU3Iwd2AgKjAL925sWTyi4Apfk1HgWVj8rXRp35UXTW/KgvhaYqIPvUCMzqAPM3ayrLw9f8UPtXPq/c1sD7OogayuWheNWqVXj55ZeRk5ODXr16ISUlBYMHD661/M6dOzFv3jz8+uuvCA8Px2OPPYbp06fblNm4cSMWLlyIP//8E126dMELL7yA22677YrOS2RHksxBNiAc6HKD7XulBZVh+XhVUD5/HLh02jxV3am8qsVGLFT+1YZgVAbm1h3N+1h7gI+ag6cwOqqQ+YZAS/i1BOHAKNtFUJqCQg207WF+VFetd9kcmP+oen7xFCRdEVT/HIbqn8NoBaBqzg8J8A+r/B9l5f8UlVrAxxdo5Vv1P02Vr3mqP5UvhMoXepkPyiUNyiQNyoQGJVCjWGhQbFKhpEKqHHdtHotdaqgK2qWGym16ozV0m4QAzP9BCAGTAAQEhIA1fAshKt83v2cS5m3lOh1UKhUAyaZMzWNW31fUOJfRJC7b7EaTgBHCPITFjoAaBmighwZ6+Eg6qCufayQ9fKCDT7X3LOUUkhF6oYAB5oceShggh14ooKu2TQ8FDEJu+xoKGISljMK6zQiZ+c/0suR45/cjDj+HFuXwlXTwgQ6+KIdWKjf/rNyuRbntdklnfV9r8755u490+V+aKoQMpdCgBBqUCA0KoEFp5XVVWrmtFBoUQwOdpEG55AOdzPzQy7TQy3xgkGthUPjAIPeFUaGFkKuhVMihkMugkleGaoUMCpn5GwMfUynaGM6ijSEHwYazCNJbHjkI1OdALirs6ikDYLmzoVQZiEua9iisfBT5dECRTwcUajugXBMCSaaAJEmQSYAMEmQXAVySABECyxUn1IBQCYjW1a51AMJkgsJUDrXuAjSGi1AbLkBtuAS14SI0+ovQGOwfPoaLkAsDUFFuDvSFf9fjOnDMINOY21Nubk+D3AcGhW/ltsqHQosKufmhV/iiorJMhVwLg8IXRksZhRYGKHE0R8KFH05DqTB/OyS3fjNk/oXN8tzyy6Dk4LlMQuV+lc9lts9lNY4pq/zWSG7ZXwa7X2arb6/+i6vlG6cWzWQ0d5RUlANGvfm5UW9+XaGv/MVKD3Qe5uqa2nFpKE5LS8OcOXOwatUqDBo0CG+88QYSExNx9OhRREZG2pXPysrC6NGjcf/99+P999/Hnj17MGPGDISEhGD8+PEAgMzMTEyaNAnPPfccbrvtNmzatAkTJ07E7t27MXDgQKfOS9Rg2iCgY5z5UZ2u2NyTahl+YRmKUfCXeSjG3z+aH5ejaV3Z69ur6hHS44rHMTe6ar3L6FTjdsIKnflz552w612G7lJlT1QDTgVzMFADaOWogFxlG7BVNcK1X1XAhkoLSJVBTpJV9uBLtj+t70tVn1WSocJowi+//ILeV/eBQqGoUa76T5ntvtW3GQ2AoRQmfSmEoQzC+rMMMJRBGEoBQxlQYX4tGUohVZSbn1eUQVZRBqmiHBIuH6ybgwkSKiQljJISFVCgQlKgQlLCAKX5eWV4Lq8wwVduhI8oh0aUQVP5Uw5Tk9XNCBnKJfMvU3JhNJ8XOgCAQjIhAKUIQGn9Mr2FqfLh6HxCQgk01lBtea6BHhHSOQRLdV/0eiHHGRGCbNEO2aItskVbnLb+DEFxubaWvzcXKx+NRQIQVPmoi4AWOgRJRQhEEYKkIrSu/BkoFSEIRfCVyuFn/QWnDL7QVf40/yKjkMyNqTSVQ2kqBxppFEiFkMEABZBz+bJNxQA5KiCHEXLz88pfNCsqt5vfr3wtZNbnRqnqp7Haa5OkgFGq/AkFTDLzT6NMAZNkfojK9xWSEUphgBIGqIQeCmGACgbrNtufepvXvSrK8c9Pj1fbVvV+ff++GhcWQC5vWb3/khDCZf9qDhw4EP3798fq1aut23r27Ilx48Zh8eLFduXnz5+PL774AseOHbNumz59Oo4cOYLMTPNXlZMmTUJhYSG++uora5lRo0YhMDAQH374oVPndaSwsBCtWrXCpUuXEBAQ0LAP7gRD5fjO0ZbxnVRvbtF2FXrzuOSaQzEunjLPmVxj7G+9h1pcAZe1m6V3+dIZwFAK6EsAfTGgr/68pPK9yud1vWey71nzOpLcHPYVGvPNsUqteaYYpbbytY95XnGlj/kraqOh8qGv8TBU9fpY3zdU9fxU36cpKDTVfpHxq+V5Xe9VPldqq54r1PZ/l0zGyuupxPa60hcD+mIIXTGMuhKYdEUwlZdA6Iog9MWArgQwmPeR9CWQDCWQG4ohM5RCbnQws0stypStcUnTHpc0HXBJ0x4X1e1xUR2OC+r2KFS2gRFy6zcNJpNAhdGIv7JOomPHjoAkg8n6LYSAyWT+hsLyLYZA1WuTENYyljHxkiX91xg7D1jet92G6mPqbcpVjbO3FKw+vr5yi/V59W9MAFi/RYEQUAgdVMZSqExlUBnLoDKVQGUsg9pk2VYKtakMKlMp1JXb1aaq96teVz5Eeb3/LOjKGIVk/SZJB5X5pzC/7vbkAchVTT9jVUPymst6ivV6PX788UcsWLDAZntCQgL27t3rcJ/MzEwkJCTYbBs5ciRSU1NhMBigVCqRmZmJuXPn2pVJSUlx+rwAoNPpoNPprK8LC81L+hoMBhgMjTePbG0s52iOc3ka92g7CQi8yvzodvPli1c0fdBzabupA4G2gY1zLKPeJuBIhmphpzJ0W7dZXutLAZgs/1cGRPXn1X462GYyGXH+3Dm0DWlj/h9+9XJ2+5vstwHmRW4UVaFVWMKswhJiNRCWEFs90Cp9HG9v6kVzahLCfFNXzaBc7bVU47VRV4qfDh/C1THXQaENgFD61hgiowVkTfC/rNr+Lsl9AB8fwKdNrbtKcHADqwMmACaTseqXPEMJoCu2vRZlSojAjkDrjlCo/REMILieH8FgMCAj4y+MGHFVy/3FvwUxVP5ZVJRcxO6d3+L6wYPN3+o440o6J6x/TyrMv7ybzL+MCqMBporKn0Y9TBUVEKYKCKMeosIAYayAqCwnjIaq/UwVQOV7MBpsjglTBSRT1XkkUwWEpIBJroJRpoKo/GmSqWCSqWGSW54rK5+bt1m+7TmRdRpRXXsCKg1MUmVZuRomWeVx5EqYZGqIyr+zjrpfu0AGUzPmp/pwWSjOy8uD0WhEu3a2y+u2a9cOubm5DvfJzc11WL6iogJ5eXkICwurtYzlmM6cFwAWL16MZ555xm57eno6tFpt7R+0kWVkZDTbuTwN2845nt1umspHjfhxpd/o2S9weGUEAH3lwyHLm5ca+cTNSQkEXouzf5nQ+F/1t3SWKQuzKx/O8ey/q01EHYL0H35zdS0uQ1H5qEevqqzy0di/G9UYEuTbvhXOlQIoNcA8nqWkwYf86mTjVO1ySktL613W5Tfa1RwwLoSocxC5o/I1t9fnmA09b3JyMubNm2d9XVhYiIiICCQkJDTb8ImMjAyMGDGCPQENxLZzDtvNOWw357DdnMe2cw7bzTnu1m6Wb/brw2WhuE2bNpDL5Xa9s+fOnbPrxbUIDQ11WF6hUCA4OLjOMpZjOnNeAFCr1VCr7VcuUyqVzXpRNPf5PAnbzjlsN+ew3ZzDdnMe2845bDfnuEu7NaSOTTxPU+1UKhViYmLsvu7JyMhAfHy8w33i4uLsyqenpyM2Ntb6oWsrYzmmM+clIiIiIs/m0uET8+bNQ1JSEmJjYxEXF4c333wT2dnZ1nmHk5OT8ffff2P9+vUAzDNNrFy5EvPmzcP999+PzMxMpKamWmeVAIDZs2djyJAhWLp0KcaOHYvPP/8c27Ztw+7du+t9XiIiIiLyLi4NxZMmTUJ+fj6effZZ5OTkoHfv3tiyZYt5WhkAOTk5yM6uuukgKioKW7Zswdy5c/H6668jPDwcK1assM5RDADx8fHYsGEDnnzySSxcuBBdunRBWlqadY7i+pyXiIiIiLyLy2+0mzFjBmbMmOHwvXXr1tltGzp0KA4ePFjnMSdMmIAJEyY4fV4iIiIi8i4uG1NMRERERNRSMBQTERERkddjKCYiIiIir8dQTERERERej6GYiIiIiLweQzEREREReT2GYiIiIiLyegzFREREROT1GIqJiIiIyOsxFBMRERGR12MoJiIiIiKvx1BMRERERF5P4eoKuCshBACgsLCwWc5nMBhQWlqKwsJCKJXKZjmnp2DbOYft5hy2m3PYbs5j2zmH7eYcd2s3S06z5La6MBQ7qaioCAAQERHh4poQERERUV2KiorQqlWrOstIoj7RmeyYTCacPXsW/v7+kCSpyc9XWFiIiIgInD59GgEBAU1+Pk/CtnMO2805bDfnsN2cx7ZzDtvNOe7WbkIIFBUVITw8HDJZ3aOG2VPsJJlMhg4dOjT7eQMCAtziImyJ2HbOYbs5h+3mHLab89h2zmG7Oced2u1yPcQWvNGOiIiIiLweQzEREREReT2GYjehVqvx9NNPQ61Wu7oqbodt5xy2m3PYbs5huzmPbecctptzPLndeKMdEREREXk99hQTERERkddjKCYiIiIir8dQTERERERej6GYiIiIiLweQ3ELsmrVKkRFRUGj0SAmJga7du2qs/zOnTsRExMDjUaDzp07Y82aNc1U05Zj8eLFGDBgAPz9/dG2bVuMGzcOx48fr3OfHTt2QJIku8dvv/3WTLV2vUWLFtl9/tDQ0Dr34fUGdOrUyeG1M3PmTIflvfla++677zBmzBiEh4dDkiR89tlnNu8LIbBo0SKEh4fDx8cHw4YNw6+//nrZ427cuBHR0dFQq9WIjo7Gpk2bmugTuEZd7WYwGDB//nxcffXV8PX1RXh4OO6++26cPXu2zmOuW7fO4XVYXl7exJ+m+VzuepsyZYrd57/uuusue1xvvt4AOLxuJEnCyy+/XOsx3fl6YyhuIdLS0jBnzhw88cQTOHToEAYPHozExERkZ2c7LJ+VlYXRo0dj8ODBOHToEB5//HE8/PDD2LhxYzPX3LV27tyJmTNnYt++fcjIyEBFRQUSEhJQUlJy2X2PHz+OnJwc66Nr167NUOOWo1evXjaf/+eff661LK83s/3799u0WUZGBgDgX//6V537eeO1VlJSgr59+2LlypUO33/ppZfwyiuvYOXKldi/fz9CQ0MxYsQIFBUV1XrMzMxMTJo0CUlJSThy5AiSkpIwceJEfP/99031MZpdXe1WWlqKgwcPYuHChTh48CA+/fRT/P7777j11lsve9yAgACbazAnJwcajaYpPoJLXO56A4BRo0bZfP4tW7bUeUxvv94A2F0z77zzDiRJwvjx4+s8rtteb4JahGuvvVZMnz7dZluPHj3EggULHJZ/7LHHRI8ePWy2/b//9//Edddd12R1dAfnzp0TAMTOnTtrLbN9+3YBQFy4cKH5KtbCPP3006Jv3771Ls/rzbHZs2eLLl26CJPJ5PB9XmtmAMSmTZusr00mkwgNDRVLliyxbisvLxetWrUSa9asqfU4EydOFKNGjbLZNnLkSHHHHXc0ep1bgprt5sgPP/wgAIhTp07VWmbt2rWiVatWjVu5FsxRu91zzz1i7NixDToOrzd7Y8eOFTfeeGOdZdz5emNPcQug1+vx448/IiEhwWZ7QkIC9u7d63CfzMxMu/IjR47EgQMHYDAYmqyuLd2lS5cAAEFBQZcte8011yAsLAzDhw/H9u3bm7pqLc6JEycQHh6OqKgo3HHHHfjrr79qLcvrzZ5er8f777+P++67D5Ik1VnW26+1mrKyspCbm2tzTanVagwdOrTWf/OA2q/DuvbxdJcuXYIkSWjdunWd5YqLi9GxY0d06NABt9xyCw4dOtQ8FWxBduzYgbZt26Jbt264//77ce7cuTrL83qz9c8//2Dz5s2YOnXqZcu66/XGUNwC5OXlwWg0ol27djbb27Vrh9zcXIf75ObmOixfUVGBvLy8JqtrSyaEwLx583D99dejd+/etZYLCwvDm2++iY0bN+LTTz9F9+7dMXz4cHz33XfNWFvXGjhwINavX4+vv/4ab731FnJzcxEfH4/8/HyH5Xm92fvss89w8eJFTJkypdYyvNYcs/y71pB/8yz7NXQfT1ZeXo4FCxbgrrvuQkBAQK3levTogXXr1uGLL77Ahx9+CI1Gg0GDBuHEiRPNWFvXSkxMxAcffIBvv/0Wy5cvx/79+3HjjTdCp9PVug+vN1vvvvsu/P39cfvtt9dZzp2vN4WrK0BVavY2CSHq7IFyVN7Rdm8xa9Ys/PTTT9i9e3ed5bp3747u3btbX8fFxeH06dNYtmwZhgwZ0tTVbBESExOtz6+++mrExcWhS5cuePfddzFv3jyH+/B6s5WamorExESEh4fXWobXWt0a+m+es/t4IoPBgDvuuAMmkwmrVq2qs+x1111nc1PZoEGD0L9/f/znP//BihUrmrqqLcKkSZOsz3v37o3Y2Fh07NgRmzdvrjPk8Xqr8s4772Dy5MmXHRvsztcbe4pbgDZt2kAul9v99nnu3Dm731ItQkNDHZZXKBQIDg5usrq2VA899BC++OILbN++HR06dGjw/tddd51b/BbbVHx9fXH11VfX2ga83mydOnUK27Ztw7Rp0xq8r7dfawCsM5005N88y34N3ccTGQwGTJw4EVlZWcjIyKizl9gRmUyGAQMGePV1GBYWho4dO9bZBrzequzatQvHjx936t88d7reGIpbAJVKhZiYGOud7BYZGRmIj493uE9cXJxd+fT0dMTGxkKpVDZZXVsaIQRmzZqFTz/9FN9++y2ioqKcOs6hQ4cQFhbWyLVzHzqdDseOHau1DXi92Vq7di3atm2Lm2++ucH7evu1BgBRUVEIDQ21uab0ej127txZ6795QO3XYV37eBpLID5x4gS2bdvm1C+lQggcPnzYq6/D/Px8nD59us424PVWJTU1FTExMejbt2+D93Wr681Vd/iRrQ0bNgilUilSU1PF0aNHxZw5c4Svr684efKkEEKIBQsWiKSkJGv5v/76S2i1WjF37lxx9OhRkZqaKpRKpfjkk09c9RFc4sEHHxStWrUSO3bsEDk5OdZHaWmptUzNtnv11VfFpk2bxO+//y5++eUXsWDBAgFAbNy40RUfwSUeeeQRsWPHDvHXX3+Jffv2iVtuuUX4+/vzeqsHo9EoIiMjxfz58+3e47VWpaioSBw6dEgcOnRIABCvvPKKOHTokHWWhCVLlohWrVqJTz/9VPz888/izjvvFGFhYaKwsNB6jKSkJJsZePbs2SPkcrlYsmSJOHbsmFiyZIlQKBRi3759zf75mkpd7WYwGMStt94qOnToIA4fPmzzb55Op7Meo2a7LVq0SGzdulX8+eef4tChQ+Lee+8VCoVCfP/99674iE2irnYrKioSjzzyiNi7d6/IysoS27dvF3FxcaJ9+/a83i7z91QIIS5duiS0Wq1YvXq1w2N40vXGUNyCvP7666Jjx45CpVKJ/v3720wrds8994ihQ4falN+xY4e45pprhEqlEp06dar1gvVkABw+1q5day1Ts+2WLl0qunTpIjQajQgMDBTXX3+92Lx5c/NX3oUmTZokwsLChFKpFOHh4eL2228Xv/76q/V9Xm+1+/rrrwUAcfz4cbv3eK1VsUxHV/Nxzz33CCHM07I9/fTTIjQ0VKjVajFkyBDx888/2xxj6NCh1vIWH3/8sejevbtQKpWiR48eHvcLRl3tlpWVVeu/edu3b7ceo2a7zZkzR0RGRgqVSiVCQkJEQkKC2Lt3b/N/uCZUV7uVlpaKhIQEERISIpRKpYiMjBT33HOPyM7OtjkGrzf7v6dCCPHGG28IHx8fcfHiRYfH8KTrTRKi8m4ZIiIiIiIvxTHFREREROT1GIqJiIiIyOsxFBMRERGR12MoJiIiIiKvx1BMRERERF6PoZiIiIiIvB5DMRERERF5PYZiIiIiIvJ6DMVERNRgkiThs88+c3U1iIgaDUMxEZGbmTJlCiRJsnuMGjXK1VUjInJbCldXgIiIGm7UqFFYu3atzTa1Wu2i2hARuT/2FBMRuSG1Wo3Q0FCbR2BgIADz0IbVq1cjMTERPj4+iIqKwscff2yz/88//4wbb7wRPj4+CA4OxgMPPIDi4mKbMu+88w569eoFtVqNsLAwzJo1y+b9vLw83HbbbdBqtejatSu++OIL63sXLlzA5MmTERISAh8fH3Tt2tUuxBMRtSQMxUREHmjhwoUYP348jhw5gv/7v//DnXfeiWPHjgEASktLMWrUKAQGBmL//v34+OOPsW3bNpvQu3r1asycORMPPPAAfv75Z3zxxRe46qqrbM7xzDPPYOLEifjpp58wevRoTJ48GQUFBdbzHz16FF999RWOHTuG1atXo02bNs3XAEREDSQJIYSrK0FERPU3ZcoUvP/++9BoNDbb58+fj4ULF0KSJEyfPh2rV6+2vnfdddehf//+WLVqFd566y3Mnz8fp0+fhq+vLwBgy5YtGDNmDM6ePYt27dqhffv2uPfee/H88887rIMkSXjyySfx3HPPAQBKSkrg7++PLVu2YNSoUbj11lvRpk0bvPPOO03UCkREjYtjiomI3NANN9xgE3oBICgoyPo8Li7O5r24uDgcPnwYAHDs2DH07dvXGogBYNCgQTCZTDh+/DgkScLZs2cxfPjwOuvQp08f63NfX1/4+/vj3LlzAIAHH3wQ48ePx8GDB5GQkIBx48YhPj7eqc9KRNQcGIqJiNyQr6+v3XCGy5EkCQAghLA+d1TGx8enXsdTKpV2+5pMJgBAYmIiTp06hc2bN2Pbtm0YPnw4Zs6ciWXLljWozkREzYVjiomIPNC+ffvsXvfo0QMAEB0djcOHD6OkpMT6/p49eyCTydCtWzf4+/ujU6dO+Oabb66oDiEhIdahHikpKXjzzTev6HhERE2JPcVERG5Ip9MhNzfXZptCobDezPbxxx8jNjYW119/PT744AP88MMPSE1NBQBMnjwZTz/9NO655x4sWrQI58+fx0MPPYSkpCS0a9cOALBo0SJMnz4dbdu2RWJiIoqKirBnzx489NBD9arfU089hZiYGPTq1Qs6nQ5ffvklevbs2YgtQETUuBiKiYjc0NatWxEWFmazrXv37vjtt98AmGeG2LBhA2bMmIHQ0FB88MEHiI6OBgBotVp8/fXXmD17NgYMGACtVovx48fjlVdesR7rnnvuQXl5OV599VU8+uijaNOmDSZMmFDv+qlUKiQnJ+PkyZPw8fHB4MGDsWHDhkb45ERETYOzTxAReRhJkrBp0yaMGzfO1VUhInIbHFNMRERERF6PoZiIiIiIvB7HFBMReRiOiiMiajj2FBMRERGR12MoJiIiIiKvx1BMRERERF6PoZiIiIiIvB5DMRERERF5PYZiIiIiIvJ6DMVERERE5PUYiomIiIjI6/1/6C/C+B3JDh4AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_loss_curves(best_rnn_model.history, title=\"SimpleRNN Loss Curves\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afb5eb9c",
      "metadata": {
        "id": "afb5eb9c"
      },
      "source": [
        "### GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ff7e4f",
      "metadata": {
        "id": "e9ff7e4f",
        "outputId": "77f1febf-f20c-4571-979d-51b895f82072"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_loss_curves(best_gru_model.history, title=\"GRU Loss Curves\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cdb0522",
      "metadata": {
        "id": "6cdb0522"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01771c8c",
      "metadata": {
        "id": "01771c8c",
        "outputId": "86f4b91e-0d20-4fd5-e56b-8313dfda4b1a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_loss_curves(best_lstm_model.history, title=\"LSTM Loss Curves\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a578027b",
      "metadata": {
        "id": "a578027b"
      },
      "source": [
        "## Evaluation on Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd095db",
      "metadata": {
        "id": "bdd095db",
        "outputId": "38ed0de2-8b90-4573-84f1-fe673a8b5d73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "SimpleRNN - MDA: 0.49874037504196167, Accuracy: 78.97027648689472, RMSE: 0.17631248469962654, MAE: 0.1309425422157699\n",
            "GRU - MDA: 0.4987523853778839, Accuracy: 78.6791513863567, RMSE: 0.17815879820837147, MAE: 0.13232051152694854\n",
            "LSTM - MDA: 0.4987844228744507, Accuracy: 79.07572547557479, RMSE: 0.17587505619811986, MAE: 0.13077742925789487\n"
          ]
        }
      ],
      "source": [
        "# Use this function to evaluate the models\n",
        "rnn_acc, rnn_rmse, rnn_mda, rnn_mae = evaluate_model(best_rnn_model, X_test, y_test)\n",
        "gru_acc, gru_rmse, gru_mda, gru_mae = evaluate_model(best_gru_model, X_test, y_test)\n",
        "lstm_acc, lstm_rmse, lstm_mda, lstm_mae = evaluate_model(best_lstm_model, X_test, y_test)\n",
        "\n",
        "# Print out evaluation results\n",
        "print(f\"SimpleRNN - MDA: {rnn_mda}, Accuracy: {rnn_acc}, RMSE: {rnn_rmse}, MAE: {rnn_mae}\")\n",
        "print(f\"GRU - MDA: {gru_mda}, Accuracy: {gru_acc}, RMSE: {gru_rmse}, MAE: {gru_mae}\")\n",
        "print(f\"LSTM - MDA: {lstm_mda}, Accuracy: {lstm_acc}, RMSE: {lstm_rmse}, MAE: {lstm_mae}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "376692c6",
      "metadata": {
        "id": "376692c6"
      },
      "source": [
        "## Plotting Predictions for Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "510bc8c1",
      "metadata": {
        "id": "510bc8c1"
      },
      "outputs": [],
      "source": [
        "# Function to plot predictions against actual values\n",
        "def plot_predictions(model, X, y, title=\"Predicted vs Actual Stock Prices\"):\n",
        "    predictions = model.predict(X)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(y, label='Actual', color='blue')\n",
        "    plt.plot(predictions, label='Predicted', color='orange')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('Stock Prices (Scaled)')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d938d7a9",
      "metadata": {
        "id": "d938d7a9"
      },
      "source": [
        "### Predictions for Vanilla RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d9e23d8",
      "metadata": {
        "id": "2d9e23d8",
        "outputId": "a9740ca4-4705-413c-cde6-8eb15914bc30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot predictions for SimpleRNN\n",
        "plot_predictions(best_rnn_model, X_test, y_test, title=\"SimpleRNN Predictions vs Actual\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba1afb1b",
      "metadata": {
        "id": "ba1afb1b"
      },
      "source": [
        "### Predictions for GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09ea751f",
      "metadata": {
        "id": "09ea751f",
        "outputId": "b614fa75-a5d4-48d2-8baf-7fbf12ecbea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot predictions for GRU\n",
        "plot_predictions(best_gru_model, X_test, y_test, title=\"GRU Predictions vs Actual\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "430a6a5f",
      "metadata": {
        "id": "430a6a5f"
      },
      "source": [
        "### Predictions for LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa66a45a",
      "metadata": {
        "id": "fa66a45a",
        "outputId": "89953e26-f162-4bdd-ddf7-f4d3f0f10aaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot predictions for LSTM\n",
        "plot_predictions(best_lstm_model, X_test, y_test, title=\"LSTM Predictions vs Actual\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b54cc16d",
      "metadata": {
        "id": "b54cc16d"
      },
      "source": [
        "Orange Line (Predicted): Represents the model's predictions for the test data.\n",
        "\n",
        "Blue Line (Actual): Represents the true stock prices (scaled) for the test data.\n",
        "\n",
        "Interpretation:\n",
        "If the orange line closely follows the blue line, the model captures the trend well.\n",
        "Deviations highlight areas where the model struggles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a63decda",
      "metadata": {
        "id": "a63decda",
        "outputId": "3930faa9-7b39-4be0-80d5-c10f59302191"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plotting combined predictions\n",
        "def plot_combined_predictions(models, model_names, X_test, y_test):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(y_test, label='Actual', color='blue')\n",
        "    for model, name in zip(models, model_names):\n",
        "        predictions = model.predict(X_test)\n",
        "        plt.plot(predictions, label=f'{name} Predictions')\n",
        "    plt.title('Combined Predictions vs Actual')\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('Stock Prices (Scaled)')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "# Plotting predictions for all models\n",
        "plot_combined_predictions(\n",
        "    [best_rnn_model, best_gru_model, best_lstm_model],\n",
        "    ['SimpleRNN', 'GRU', 'LSTM'],\n",
        "    X_test, y_test\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa8969b0",
      "metadata": {
        "id": "aa8969b0"
      },
      "source": [
        "## Comparison of Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34bdc628",
      "metadata": {
        "id": "34bdc628"
      },
      "outputs": [],
      "source": [
        "models = ['SimpleRNN', 'GRU', 'LSTM']\n",
        "mda_values = [rnn_mda, gru_mda, lstm_mda]\n",
        "acc_values = [rnn_acc, gru_acc, lstm_acc]\n",
        "rmse_values = [rnn_rmse, gru_rmse, lstm_rmse]\n",
        "mae_values = [rnn_mae, gru_mae, lstm_mae]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd767d89",
      "metadata": {
        "id": "fd767d89",
        "outputId": "4104a44b-d86b-47fd-baa9-d39f09f03e60"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plotting the Bar Chart for MDA\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(models, mda_values, color=['skyblue', 'lightgreen', 'salmon'])\n",
        "plt.title('Test MDA Comparison Across Models')\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('MDA')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d65e57b8",
      "metadata": {
        "id": "d65e57b8",
        "outputId": "66a6fefe-778f-41dd-edaf-94d450269498"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plotting the Bar Chart for Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(models, acc_values, color=['skyblue', 'lightgreen', 'salmon'])\n",
        "plt.title('Test Accuracy Comparison Across Models')\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c223a2e",
      "metadata": {
        "id": "7c223a2e",
        "outputId": "29d8f863-0c21-4ac5-8594-e83c6ac24275"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2AAAAIhCAYAAAAo4dnZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABP90lEQVR4nO3deVxV1f7/8feROVFUUMACxDGcERLBKC3DHErNHJMyh5vZvYpk5VCJ1s1KM7McbgYOlaKlTTevijmkSeVEI6U3JdQghS7gkIC6f3/45fw6HUBQ3Mfk9Xw89iP3Omvv9dkHDvFm7bOOxTAMQwAAAACAK66GowsAAAAAgOqCAAYAAAAAJiGAAQAAAIBJCGAAAAAAYBICGAAAAACYhAAGAAAAACYhgAEAAACASQhgAAAAAGASAhgAAAAAmIQABuCaY7FYKrRt3br1ssc6ffq0EhISKnyujIwMmxpq1KihunXr6vbbb9fGjRvt+ickJFj7HTx40O7xU6dOqXbt2rJYLBo+fLjNY4cPH9bYsWPVvHlzeXh4qF69emrTpo1Gjx6tw4cP241R1paRkXHR6zp//rzefPNNdevWTT4+PnJxcVGDBg3Uu3dvffTRRzp//nyFnp+/MovFooSEBEeXcVH33HOPLBaL/v73vzu6lCum5Hv3z6+JEjNmzKjU93dFDR8+XI0aNbqkY7t06aIuXbpUWS0Arl7Oji4AAKpaamqqzf4zzzyjLVu2aPPmzTbtLVu2vOyxTp8+renTp0tSpX55+sc//qGhQ4fq3Llz+uGHHzR9+nT17NlTmzdv1i233GLX39PTU0uWLNEzzzxj0/7OO++ouLhYLi4uNu1HjhxRhw4dVKdOHT366KNq0aKF8vPz9f3332v16tU6ePCgAgICbI5Zv369vLy87Mb29/cv91rOnDmjvn37auPGjRo8eLAWLlwoPz8/HT9+XOvXr9eAAQO0atUq9enTp6JPz19SamqqbrjhBkeXUa5jx47p3//+tyTp7bff1uzZs+Xu7u7gqq6MWrVq6Z133tGrr76qWrVqWdsNw9DSpUtVu3ZtFRQUOLBCANUVAQzANadTp042+/Xr11eNGjXs2h0pMDDQWk/nzp3VrFkz3XrrrUpMTCw1gA0aNEjLli3T9OnTVaPG/795ITExUf369dOHH35o03/x4sXKycnRl19+qeDgYGt73759NWXKlFJnpMLCwuTj41Ppa4mPj9eGDRu0bNky3X///TaP3XPPPXrsscf0+++/V/q8fwWGYejMmTPy8PC4qr6/yrJ8+XIVFxerV69e+vjjj7V27VoNHTq0Ss59+vRpXXfddVVyrqrQp08frVmzRsnJyRo9erS1ffPmzTp06JBGjx6txYsXO7BCANUVtyACqJaKior07LPP6sYbb5Sbm5vq16+vBx98UMePH7fpt3nzZnXp0kXe3t7y8PBQYGCg+vfvr9OnTysjI0P169eXJE2fPv2itz2VJzw8XJL066+/lvr4iBEjdPjwYaWkpFjb9u/frx07dmjEiBF2/XNzc1WjRg01aNCg1PP9McRdjuzsbL3xxhvq3r27Xfgq0axZM7Vt29a6n5mZqWHDhqlBgwZyc3NTSEiIXnrpJZtQWHKr5qxZs/TCCy+oUaNG8vDwUJcuXbR//34VFxdr0qRJatiwoby8vNSvXz8dO3bMZtxGjRqpd+/eeu+999S2bVu5u7urcePGmjdvnk2/M2fO6NFHH1X79u3l5eWlevXqKTIyUh988IHdtZTcurdo0SKFhITIzc1Ny5Ytsz72x1sQT58+rYkTJyo4OFju7u6qV6+ewsPDtXLlSptzfvjhh4qMjNR1112nWrVq6Y477rCbxS25TfS7777TkCFD5OXlJV9fX40YMUL5+fnlfIVsJSUlydfXV8uWLZOHh4eSkpJK7ffFF1/orrvukre3t9zd3dWkSRPFxcXZ1bN3717de++9qlu3rpo0aWJ9PidPnqzg4GC5urrq+uuv1yOPPKK8vDybMcp7bZVYuHCh2rVrJ09PT9WqVUs33nijpkyZUqFrLfm++PM1JiUlqXPnzmrevHmZz1G7du2sX7N+/fopPT3drt/SpUvVokUL6/fw8uXLSz1fRX/WlOZyrh/A1YsZMADVzvnz59WnTx9t375djz/+uKKiovTzzz9r2rRp6tKli3bv3i0PDw9lZGSoV69eio6OVlJSkurUqaOjR49q/fr1Kioqkr+/v9avX68777xTI0eO1KhRoyTJGsoq49ChQ5JU5i+FzZo1s9bRvXt3SRd+UWzUqJFuv/12u/6RkZGaP3++7rnnHsXHxysyMlK1a9cut4Zz587p7NmzNm0Wi0VOTk5lHrNlyxYVFxerb9++5Z67xPHjxxUVFaWioiI988wzatSokf79739r4sSJ+umnn7RgwQKb/vPnz1fbtm01f/585eXl6dFHH9Vdd92liIgIubi4KCkpST///LMmTpyoUaNG2c0EpqWlKS4uTgkJCfLz89Pbb7+t8ePHq6ioSBMnTpQkFRYW6rffftPEiRN1/fXXq6ioSJs2bdI999yjJUuW2AXL999/X9u3b9fTTz8tPz+/MkNufHy83nzzTT377LMKDQ3VqVOn9O233yo3N9faZ8WKFbrvvvsUExOjlStXqrCwUC+++KK6dOmiTz75RDfffLPNOfv3769BgwZp5MiR+uabbzR58mRJKjNI/dHOnTuVnp6uxx57TN7e3urfv7/efvttHTp0yGaWdMOGDbrrrrsUEhKiOXPmKDAwUBkZGaW+R/Gee+7R4MGDNWbMGJ06dUqGYahv37765JNPNHnyZEVHR+vrr7/WtGnTlJqaqtTUVLm5uV30tXXdddcpOTlZY8eO1T/+8Q/Nnj1bNWrU0H//+199//33F73WEiNHjtTtt9+u9PR0hYSEKC8vT2vXrtWCBQtsvg4lZs6cqSlTpmjIkCGaOXOmcnNzlZCQoMjISO3atUvNmjWTdCF8Pfjgg+rTp49eeukl5efnKyEhQYWFhTZ/3Kjoz5rSVMX1A7hKGQBwjXvggQeMmjVrWvdXrlxpSDLWrFlj02/Xrl2GJGPBggWGYRjGu+++a0gy0tLSyjz38ePHDUnGtGnTKlTLoUOHDEnGCy+8YBQXFxtnzpwx0tLSjMjISMPf3984dOiQTf9p06YZkozjx48bS5YsMdzc3Izc3Fzj7Nmzhr+/v5GQkGAYhmHUrFnTeOCBB6zHnT9/3njooYeMGjVqGJIMi8VihISEGBMmTChzjNK2Jk2alHs9zz//vCHJWL9+fYWuf9KkSYYk44svvrBpf/jhhw2LxWL8+OOPNs9Tu3btjHPnzln7zZ0715Bk3H333TbHx8XFGZKM/Px8a1tQUJBhsVjsvn533HGHUbt2bePUqVOl1nj27FmjuLjYGDlypBEaGmrzmCTDy8vL+O233+yO+/P3QevWrY2+ffuW+VycO3fOaNiwodGmTRubazxx4oTRoEEDIyoqytpW8jV68cUXbc4xduxYw93d3Th//nyZ45QYMWKEIclIT083DMMwtmzZYkgynnrqKZt+TZo0MZo0aWL8/vvvZZ6rpJ6nn37apn39+vWl1rlq1SpDkvH6668bhlGx19bf//53o06dOhe9rtJIMh555BHj/PnzRnBwsDFx4kTDMAxj/vz5hqenp3HixAlj1qxZhiTr6+F///uf4eHhYfTs2dPmXJmZmYabm5sxdOhQwzD+/9etQ4cONs97RkaG4eLiYgQFBVnbKvqzxjAM49ZbbzVuvfXWKrl+AFc3bkEEUO38+9//Vp06dXTXXXfp7Nmz1q19+/by8/OzrmjYvn17ubq66m9/+5uWLVtW6iqEl+qJJ56Qi4uL3N3d1b59e3377bf66KOPyl1BbcCAAXJ1ddXbb7+tdevWKTs7u8zbHS0WixYtWqSDBw9qwYIFevDBB1VcXKyXX35ZrVq10rZt2+yO2bRpk3bt2mWzvf/++1Vzwf9n8+bNatmypTp27GjTPnz4cBmGYbdQSs+ePW1mFEJCQiRJvXr1sulX0p6ZmWnT3qpVK7Vr186mbejQoSooKNDevXutbe+88446d+4sT09POTs7y8XFRYmJiaXeenbbbbepbt26F73Wjh076j//+Y8mTZqkrVu32r0P7scff9Qvv/yi2NhYm2v09PRU//799fnnn9vcjidJd999t81+27ZtdebMGbvbL//s5MmTWr16taKionTjjTdKkm699VY1adJES5cutd7+uX//fv30008aOXJkhRbn6N+/v81+ydfvz9+XAwYMUM2aNfXJJ59Iqthrq2PHjsrLy9OQIUP0wQcfKCcn56L1/FnJLcFvvvmmzp49q8TERA0cOFCenp52fVNTU/X777/b1R4QEKDbbrvNWnvJ123o0KGyWCzWfkFBQYqKirI5tqI/a0pTFdcP4OpEAANQ7fz666/Ky8uTq6urXFxcbLbs7GzrLzpNmjTRpk2b1KBBAz3yyCNq0qSJmjRpoldeeeWyaxg/frx27dqlHTt2aPbs2SouLlafPn1KvS2qRM2aNTVo0CAlJSUpMTFR3bp1U1BQULnjBAUF6eGHH1ZiYqIOHDigVatW6cyZM3rsscfs+rZr107h4eE2W+vWrcs9f2BgoKT/fwvlxeTm5pa6qmLDhg2tj/9RvXr1bPZdXV3LbT9z5oxNu5+fn91YJW0lY61du1YDBw7U9ddfr7feekupqanatWuXRowYYXc+6eKrQpaYN2+ennjiCb3//vvq2rWr6tWrp759++rAgQM245f1fJw/f17/+9//bNq9vb1t9t3c3CTpooucrFq1SidPntTAgQOVl5envLw85efna+DAgTbvLSx5X1JFV3P8c+25ublydna2uw3XYrHIz8/Pes0VeW3FxsZabzHt37+/GjRooIiICJv3QVZEyfutnnvuOe3du1cjR44std/Fvh4lj5f8t7zvrRIV/VlTmqq6fgBXH94DBqDa8fHxkbe3t9avX1/q439csjo6OlrR0dE6d+6cdu/erVdffVVxcXHy9fXV4MGDL7mGG264wbrwRufOneXn56dhw4Zp2rRpeu2118o8bsSIEXrjjTf09ddf6+233670uAMHDtTMmTP17bffXnLtf9S1a1e5uLjo/fff15gxYy7a39vbW1lZWXbtv/zyiyRd0iqM5cnOzi6zrSTMvPXWWwoODtaqVatsZjQKCwtLPecf+5SnZs2amj59uqZPn65ff/3VOht211136YcffrCOX9bzUfIZcVUhMTFRkhQXF2ezmMYfH+/evbs1OB05cqRC5/3zc+Ht7a2zZ8/q+PHjNiHMMAxlZ2frpptusrZV5LX14IMP6sEHH9SpU6f06aefatq0aerdu7f2799/0T8+lAgICFC3bt00ffp0tWjRwm6W6o+1S2V/PUq+N0v6lfe9VaIyP2tKUxXXD+DqwwwYgGqnd+/eys3N1blz5+xmfMLDw9WiRQu7Y5ycnBQREaH58+dLkvX2tYrOQFzMfffdpy5dumjx4sX6+eefy+wXGRmpESNGqF+/furXr1+Z/Ur7JVK6cCva4cOHrTNOl8vPz0+jRo3Shg0bylwF7qefftLXX38tSbr99tv1/fff29z+J11YHt1isahr165VUleJ7777Tl999ZVN24oVK1SrVi116NBB0oUQ4erqahMmsrOzS10F8VL5+vpq+PDhGjJkiH788UedPn1aLVq00PXXX68VK1bIMAxr31OnTmnNmjXWlREvV3p6ulJTU9W/f39t2bLFbrv99tv1wQcfKDc3V82bN1eTJk2UlJRUZgAtT8mCMG+99ZZN+5o1a3Tq1KlSF4wp67X1RzVr1lSPHj00depUFRUV6bvvvqtUXSWLtzz11FNl9omMjJSHh4dd7UeOHNHmzZuttbdo0UL+/v5auXKlzdft559/1s6dO22OvZSfNaW53OsHcHVhBgxAtTN48GC9/fbb6tmzp8aPH6+OHTvKxcVFR44c0ZYtW9SnTx/169dPixYt0ubNm9WrVy8FBgbqzJkz1tXmunXrJunCX7CDgoL0wQcf6Pbbb1e9evXk4+NT7nu5yvLCCy8oIiJCzzzzjN54440y+5XMZpTnn//8pz777DMNGjRI7du3l4eHhw4dOqTXXntNubm5mjVrlt0xe/bsKfWDmFu2bFnuCopz5szRwYMHNXz4cG3YsEH9+vWTr6+vcnJylJKSoiVLlig5OVlt27bVhAkTtHz5cvXq1UszZsxQUFCQPv74Yy1YsEAPP/xwmatAXqqGDRvq7rvvVkJCgvz9/fXWW28pJSVFL7zwgjXc9O7dW2vXrtXYsWN177336vDhw3rmmWfk7+9vvV3wUkRERKh3795q27at6tatq/T0dL355ps2werFF1/Ufffdp969e+uhhx5SYWGhZs2apby8PD3//PNV8hyUfL88/vjjdu+9k6QTJ07ok08+0VtvvaXx48dr/vz5uuuuu9SpUydNmDBBgYGByszM1IYNGy4663rHHXeoe/fueuKJJ1RQUKDOnTtbV0EMDQ1VbGysJFXotTV69Gh5eHioc+fO8vf3V3Z2tmbOnCkvLy+bmbSKiImJUUxMTLl96tSpo6eeekpTpkzR/fffryFDhig3N1fTp0+Xu7u7pk2bJunCRzg888wzGjVqlPr166fRo0crLy/PutLmH1X0Z01pqvL6AVxlHLsGCABceX9eBdEwDKO4uNiYPXu20a5dO8Pd3d3w9PQ0brzxRuOhhx4yDhw4YBiGYaSmphr9+vUzgoKCDDc3N8Pb29u49dZbjQ8//NDmXJs2bTJCQ0MNNzc3Q5LNaoR/VrK636xZs0p9fMCAAYazs7Px3//+1zAM21UQy/PnVRA///xz45FHHjHatWtn1KtXz3BycjLq169v3Hnnnca6detsji1vFURJRkpKSrljG8aFlQOXLVtm3HbbbUa9evUMZ2dno379+kaPHj2MFStW2Kzy9/PPPxtDhw41vL29DRcXF6NFixbGrFmzbPqU9TyVrNz3zjvv2LQvWbLEkGTs2rXL2hYUFGT06tXLePfdd41WrVoZrq6uRqNGjYw5c+bY1f/8888bjRo1Mtzc3IyQkBBj8eLF1uflj/R/q+uVRn9aBXHSpElGeHi4UbduXcPNzc1o3LixMWHCBCMnJ8fmuPfff9+IiIgw3N3djZo1axq333678dlnn9n0Kev7oOS6/7yyZYmioiKjQYMGRvv27Ut93DAufO1uuOEGo02bNta21NRUo0ePHoaXl5fh5uZmNGnSxJgwYcJF6zEMw/j999+NJ554wggKCjJcXFwMf39/4+GHHzb+97//2Zz/Yq+tZcuWGV27djV8fX0NV1dXo2HDhsbAgQONr7/+usxrKVHe16nEn1dBLPHGG28Ybdu2NVxdXQ0vLy+jT58+xnfffWd3/BtvvGE0a9bMcHV1NZo3b24kJSUZDzzwgM0qiIZRsZ81hmG/CuLlXD+Aq5vFMP4wfw4AwDWiUaNGat26tf797387uhQAAKx4DxgAAAAAmIQABgAAAAAm4RZEAAAAADAJM2AAAAAAYBICGAAAAACYhAAGAAAAACbhg5gv0fnz5/XLL7+oVq1aslgsji4HAAAAgIMYhqETJ06oYcOGqlGj/DkuAtgl+uWXXxQQEODoMgAAAABcJQ4fPqwbbrih3D4EsEtUq1YtSRee5Nq1azu4GgAAAACOUlBQoICAAGtGKA8B7BKV3HZYu3ZtAhgAAACACr01iUU4AAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATOLs6AIAAJXzyv9ecXQJwFVtfN3xji4BAMrEDBgAAAAAmIQABgAAAAAmIYABAAAAgEkc/h6wBQsWaNasWcrKylKrVq00d+5cRUdHl9o3KytLjz76qPbs2aMDBw5o3Lhxmjt3rk2fLl26aNu2bXbH9uzZUx9//LEkKSEhQdOnT7d53NfXV9nZ2VVzUQ7w/L4cR5cAXPUmhfo4ugQAAFDNOTSArVq1SnFxcVqwYIE6d+6sf/3rX+rRo4e+//57BQYG2vUvLCxU/fr1NXXqVL388sulnnPt2rUqKiqy7ufm5qpdu3YaMGCATb9WrVpp06ZN1n0nJ6cquioAAICqUTz9UUeXAFzVXKa95OgSKs2hAWzOnDkaOXKkRo0aJUmaO3euNmzYoIULF2rmzJl2/Rs1aqRXXrmw+ldSUlKp56xXr57NfnJysq677jq7AObs7Cw/P7+quAwAAAAAqBCHvQesqKhIe/bsUUxMjE17TEyMdu7cWWXjJCYmavDgwapZs6ZN+4EDB9SwYUMFBwdr8ODBOnjwYLnnKSwsVEFBgc0GAAAAAJXhsACWk5Ojc+fOydfX16a9Kt+L9eWXX+rbb7+1zrCViIiI0PLly7VhwwYtXrxY2dnZioqKUm5ubpnnmjlzpry8vKxbQEBAldQIAAAAoPpw+CqIFovFZt8wDLu2S5WYmKjWrVurY8eONu09evRQ//791aZNG3Xr1s26OMeyZcvKPNfkyZOVn59v3Q4fPlwlNQIAAACoPhz2HjAfHx85OTnZzXYdO3bMblbsUpw+fVrJycmaMWPGRfvWrFlTbdq00YEDB8rs4+bmJjc3t8uuCwAAAED15bAZMFdXV4WFhSklJcWmPSUlRVFRUZd9/tWrV6uwsFDDhg27aN/CwkKlp6fL39//sscFAAAAgLI4dBXE+Ph4xcbGKjw8XJGRkXr99deVmZmpMWPGSLpw29/Ro0e1fPly6zFpaWmSpJMnT+r48eNKS0uTq6urWrZsaXPuxMRE9e3bV97e3nbjTpw4UXfddZcCAwN17NgxPfvssyooKNADDzxw5S4WAAAAQLXn0AA2aNAg5ebmasaMGcrKylLr1q21bt06BQUFSbrwwcuZmZk2x4SGhlr/vWfPHq1YsUJBQUHKyMiwtu/fv187duzQxo0bSx33yJEjGjJkiHJyclS/fn116tRJn3/+uXVcAAAAALgSHBrAJGns2LEaO3ZsqY8tXbrUrs0wjIues3nz5uX2S05OrnB9AAAAAFBVHL4KIgAAAABUFwQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADCJwwPYggULFBwcLHd3d4WFhWn79u1l9s3KytLQoUPVokUL1ahRQ3FxcXZ9li5dKovFYredOXPmkscFAAAAgKrg0AC2atUqxcXFaerUqdq3b5+io6PVo0cPZWZmltq/sLBQ9evX19SpU9WuXbsyz1u7dm1lZWXZbO7u7pc8LgAAAABUBYcGsDlz5mjkyJEaNWqUQkJCNHfuXAUEBGjhwoWl9m/UqJFeeeUV3X///fLy8irzvBaLRX5+fjbb5YwLAAAAAFXBYQGsqKhIe/bsUUxMjE17TEyMdu7ceVnnPnnypIKCgnTDDTeod+/e2rdv32WPW1hYqIKCApsNAAAAACrDYQEsJydH586dk6+vr027r6+vsrOzL/m8N954o5YuXaoPP/xQK1eulLu7uzp37qwDBw5c1rgzZ86Ul5eXdQsICLjkGgEAAABUTw5fhMNisdjsG4Zh11YZnTp10rBhw9SuXTtFR0dr9erVat68uV599dXLGnfy5MnKz8+3bocPH77kGgEAAABUT86OGtjHx0dOTk52s07Hjh2zm526HDVq1NBNN91knQG71HHd3Nzk5uZWZXUBAAAAqH4cNgPm6uqqsLAwpaSk2LSnpKQoKiqqysYxDENpaWny9/c3dVwAAAAA+DOHzYBJUnx8vGJjYxUeHq7IyEi9/vrryszM1JgxYyRduO3v6NGjWr58ufWYtLQ0SRcW2jh+/LjS0tLk6uqqli1bSpKmT5+uTp06qVmzZiooKNC8efOUlpam+fPnV3hcAAAAALgSHBrABg0apNzcXM2YMUNZWVlq3bq11q1bp6CgIEkXPnj5z5/NFRoaav33nj17tGLFCgUFBSkjI0OSlJeXp7/97W/Kzs6Wl5eXQkND9emnn6pjx44VHhcAAAAArgSLYRiGo4v4KyooKJCXl5fy8/NVu3ZtR5ej5/flOLoE4Ko3KdTH0SVUiVf+94qjSwCuauPrjnd0CVWmePqjji4BuKq5THvJ0SVIqlw2cPgqiAAAAABQXRDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMInDA9iCBQsUHBwsd3d3hYWFafv27WX2zcrK0tChQ9WiRQvVqFFDcXFxdn0WL16s6Oho1a1bV3Xr1lW3bt305Zdf2vRJSEiQxWKx2fz8/Kr60gAAAADAhkMD2KpVqxQXF6epU6dq3759io6OVo8ePZSZmVlq/8LCQtWvX19Tp05Vu3btSu2zdetWDRkyRFu2bFFqaqoCAwMVExOjo0eP2vRr1aqVsrKyrNs333xT5dcHAAAAAH/k0AA2Z84cjRw5UqNGjVJISIjmzp2rgIAALVy4sNT+jRo10iuvvKL7779fXl5epfZ5++23NXbsWLVv31433nijFi9erPPnz+uTTz6x6efs7Cw/Pz/rVr9+/Sq/PgAAAAD4I4cFsKKiIu3Zs0cxMTE27TExMdq5c2eVjXP69GkVFxerXr16Nu0HDhxQw4YNFRwcrMGDB+vgwYPlnqewsFAFBQU2GwAAAABUhsMCWE5Ojs6dOydfX1+bdl9fX2VnZ1fZOJMmTdL111+vbt26WdsiIiK0fPlybdiwQYsXL1Z2draioqKUm5tb5nlmzpwpLy8v6xYQEFBlNQIAAACoHhy+CIfFYrHZNwzDru1Svfjii1q5cqXWrl0rd3d3a3uPHj3Uv39/tWnTRt26ddPHH38sSVq2bFmZ55o8ebLy8/Ot2+HDh6ukRgAAAADVh7OjBvbx8ZGTk5PdbNexY8fsZsUuxezZs/Xcc89p06ZNatu2bbl9a9asqTZt2ujAgQNl9nFzc5Obm9tl1wUAAACg+nLYDJirq6vCwsKUkpJi056SkqKoqKjLOvesWbP0zDPPaP369QoPD79o/8LCQqWnp8vf3/+yxgUAAACA8jhsBkyS4uPjFRsbq/DwcEVGRur1119XZmamxowZI+nCbX9Hjx7V8uXLrcekpaVJkk6ePKnjx48rLS1Nrq6uatmypaQLtx0+9dRTWrFihRo1amSdYfP09JSnp6ckaeLEibrrrrsUGBioY8eO6dlnn1VBQYEeeOABE68eAAAAQHXj0AA2aNAg5ebmasaMGcrKylLr1q21bt06BQUFSbrwwct//kyw0NBQ67/37NmjFStWKCgoSBkZGZIufLBzUVGR7r33Xpvjpk2bpoSEBEnSkSNHNGTIEOXk5Kh+/frq1KmTPv/8c+u4AAAAAHAlODSASdLYsWM1duzYUh9bunSpXZthGOWerySIlSc5ObkipQEAAABAlXL4KogAAAAAUF0QwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADCJwwPYggULFBwcLHd3d4WFhWn79u1l9s3KytLQoUPVokUL1ahRQ3FxcaX2W7NmjVq2bCk3Nze1bNlS77333mWNCwAAAABVwaEBbNWqVYqLi9PUqVO1b98+RUdHq0ePHsrMzCy1f2FhoerXr6+pU6eqXbt2pfZJTU3VoEGDFBsbq6+++kqxsbEaOHCgvvjii0seFwAAAACqgsUwDMNRg0dERKhDhw5auHChtS0kJER9+/bVzJkzyz22S5cuat++vebOnWvTPmjQIBUUFOg///mPte3OO+9U3bp1tXLlysset0RBQYG8vLyUn5+v2rVrV+iYK+n5fTmOLgG46k0K9XF0CVXilf+94ugSgKva+LrjHV1ClSme/qijSwCuai7TXnJ0CZIqlw0cNgNWVFSkPXv2KCYmxqY9JiZGO3fuvOTzpqam2p2ze/fu1nNe6riFhYUqKCiw2QAAAACgMioVwL788kudO3fOuv/nybPCwkKtXr26QufKycnRuXPn5Ovra9Pu6+ur7OzsypRlIzs7u9xzXuq4M2fOlJeXl3ULCAi45BoBAAAAVE+VCmCRkZHKzc217nt5eengwYPW/by8PA0ZMqRSBVgsFpt9wzDs2iqrIues7LiTJ09Wfn6+dTt8+PBl1QgAAACg+nGuTOc/z3iV9vaxir6lzMfHR05OTnazTseOHbObnaoMPz+/cs95qeO6ubnJzc3tkusCAAAAgCp/D1hFZ69cXV0VFhamlJQUm/aUlBRFRUVd8viRkZF259y4caP1nFdqXAAAAAC4mErNgFW1+Ph4xcbGKjw8XJGRkXr99deVmZmpMWPGSLpw29/Ro0e1fPly6zFpaWmSpJMnT+r48eNKS0uTq6urWrZsKUkaP368brnlFr3wwgvq06ePPvjgA23atEk7duyo8LgAAAAAcCVUOoB9//331tv3DMPQDz/8oJMnT0q6sMBFZQwaNEi5ubmaMWOGsrKy1Lp1a61bt05BQUGSLnzw8p8/mys0NNT67z179mjFihUKCgpSRkaGJCkqKkrJycl68skn9dRTT6lJkyZatWqVIiIiKjwuAAAAAFwJlfocsBo1ashisZT6Pq+SdovFYrNS4rWKzwED/nr4HDCgeuBzwIDq46/4OWCVmgE7dOjQZRUGAAAAANVZpQIYt+gBAAAAwKWr1CqIv/32m44cOWLT9t133+nBBx/UwIEDtWLFiiotDgAAAACuJZUKYI888ojmzJlj3T927Jiio6O1a9cuFRYWavjw4XrzzTervEgAAAAAuBZUKoB9/vnnuvvuu637y5cvV7169ZSWlqYPPvhAzz33nObPn1/lRQIAAADAtaBSASw7O1vBwcHW/c2bN6tfv35ydr7wVrK7775bBw4cqNoKAQAAAOAaUakAVrt2beXl5Vn3v/zyS3Xq1Mm6b7FYVFhYWGXFAQAAAMC1pFIBrGPHjpo3b57Onz+vd999VydOnNBtt91mfXz//v0KCAio8iIBAAAA4FpQqWXon3nmGXXr1k1vvfWWzp49qylTpqhu3brWx5OTk3XrrbdWeZEAAAAAcC2oVABr37690tPTtXPnTvn5+SkiIsLm8cGDB6tly5ZVWiAAAAAAXCsqFcAkqX79+urTp0+pj/Xq1euyCwIAAACAa1WlAtjy5csr1O/++++/pGIAAAAA4FpWqQA2fPhweXp6ytnZWYZhlNrHYrEQwAAAAACgFJUKYCEhIfr11181bNgwjRgxQm3btr1SdQEAAADANadSy9B/9913+vjjj/X777/rlltuUXh4uBYuXKiCgoIrVR8AAAAAXDMqFcAkKSIiQv/617+UlZWlcePGafXq1fL399d9993HhzADAAAAQDkqHcBKeHh46P7779f06dPVsWNHJScn6/Tp01VZGwAAAABcUy4pgB09elTPPfecmjVrpsGDB+umm27Sd999Z/OhzAAAAAAAW5VahGP16tVasmSJtm3bpu7du+ull15Sr1695OTkdKXqAwAAAIBrRqUC2ODBgxUYGKgJEybI19dXGRkZmj9/vl2/cePGVVmBAAAAAHCtqFQACwwMlMVi0YoVK8rsY7FYCGAAAAAAUIpKBbCMjIyL9jl69Oil1gIAAAAA17RLXgXxz7KzszVu3Dg1bdq0qk4JAAAAANeUSgWwvLw83Xfffapfv74aNmyoefPm6fz583r66afVuHFjpaamKikp6UrVCgAAAAB/aZW6BXHKlCn69NNP9cADD2j9+vWaMGGC1q9frzNnzug///mPbr311itVJwAAAAD85VUqgH388cdasmSJunXrprFjx6pp06Zq3ry55s6de4XKAwAAAIBrR6VuQfzll1/UsmVLSVLjxo3l7u6uUaNGXZHCAAAAAOBaU6kAdv78ebm4uFj3nZycVLNmzSovCgAAAACuRZW6BdEwDA0fPlxubm6SpDNnzmjMmDF2IWzt2rVVVyEAAAAAXCMqFcAeeOABm/1hw4ZVaTEAAAAAcC2rVABbsmTJlaoDAAAAAK55VfZBzAAAAACA8hHAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATOLwALZgwQIFBwfL3d1dYWFh2r59e7n9t23bprCwMLm7u6tx48ZatGiRzeNdunSRxWKx23r16mXtk5CQYPe4n5/fFbk+AAAAACjh0AC2atUqxcXFaerUqdq3b5+io6PVo0cPZWZmltr/0KFD6tmzp6Kjo7Vv3z5NmTJF48aN05o1a6x91q5dq6ysLOv27bffysnJSQMGDLA5V6tWrWz6ffPNN1f0WgEAAADA2ZGDz5kzRyNHjtSoUaMkSXPnztWGDRu0cOFCzZw5067/okWLFBgYqLlz50qSQkJCtHv3bs2ePVv9+/eXJNWrV8/mmOTkZF133XV2AczZ2blSs16FhYUqLCy07hcUFFT4WAAAAACQHDgDVlRUpD179igmJsamPSYmRjt37iz1mNTUVLv+3bt31+7du1VcXFzqMYmJiRo8eLBq1qxp037gwAE1bNhQwcHBGjx4sA4ePFhuvTNnzpSXl5d1CwgIuNglAgAAAIANhwWwnJwcnTt3Tr6+vjbtvr6+ys7OLvWY7OzsUvufPXtWOTk5dv2//PJLffvtt9YZthIRERFavny5NmzYoMWLFys7O1tRUVHKzc0ts97JkycrPz/fuh0+fLiilwoAAAAAkhx8C6IkWSwWm33DMOzaLta/tHbpwuxX69at1bFjR5v2Hj16WP/dpk0bRUZGqkmTJlq2bJni4+NLHdfNzU1ubm7lXwwAAAAAlMNhM2A+Pj5ycnKym+06duyY3SxXCT8/v1L7Ozs7y9vb26b99OnTSk5Otpv9Kk3NmjXVpk0bHThwoJJXAQAAAAAV57AA5urqqrCwMKWkpNi0p6SkKCoqqtRjIiMj7fpv3LhR4eHhcnFxsWlfvXq1CgsLNWzYsIvWUlhYqPT0dPn7+1fyKgAAAACg4hy6DH18fLzeeOMNJSUlKT09XRMmTFBmZqbGjBkj6cL7ru6//35r/zFjxujnn39WfHy80tPTlZSUpMTERE2cONHu3ImJierbt6/dzJgkTZw4Udu2bdOhQ4f0xRdf6N5771VBQYEeeOCBK3exAAAAAKo9h74HbNCgQcrNzdWMGTOUlZWl1q1ba926dQoKCpIkZWVl2XwmWHBwsNatW6cJEyZo/vz5atiwoebNm2ddgr7E/v37tWPHDm3cuLHUcY8cOaIhQ4YoJydH9evXV6dOnfT5559bxwUAAACAK8Hhi3CMHTtWY8eOLfWxpUuX2rXdeuut2rt3b7nnbN68uXVxjtIkJydXqkYAAAAAqAoOvQURAAAAAKoTAhgAAAAAmIQABgAAAAAmIYABAAAAgEkIYAAAAABgEgIYAAAAAJiEAAYAAAAAJiGAAQAAAIBJCGAAAAAAYBICGAAAAACYhAAGAAAAACYhgAEAAACASQhgAAAAAGASAhgAAAAAmIQABgAAAAAmIYABAAAAgEkIYAAAAABgEgIYAAAAAJiEAAYAAAAAJiGAAQAAAIBJCGAAAAAAYBICGAAAAACYhAAGAAAAACYhgAEAAACASQhgAAAAAGASAhgAAAAAmIQABgAAAAAmIYABAAAAgEkIYAAAAABgEgIYAAAAAJiEAAYAAAAAJiGAAQAAAIBJCGAAAAAAYBICGAAAAACYhAAGAAAAACYhgAEAAACASQhgAAAAAGASAhgAAAAAmIQABgAAAAAmIYABAAAAgEkIYAAAAABgEgIYAAAAAJiEAAYAAAAAJiGAAQAAAIBJCGAAAAAAYBICGAAAAACYhAAGAAAAACYhgAEAAACASQhgAAAAAGASAhgAAAAAmMThAWzBggUKDg6Wu7u7wsLCtH379nL7b9u2TWFhYXJ3d1fjxo21aNEim8eXLl0qi8Vit505c+ayxgUAAACAy+XQALZq1SrFxcVp6tSp2rdvn6Kjo9WjRw9lZmaW2v/QoUPq2bOnoqOjtW/fPk2ZMkXjxo3TmjVrbPrVrl1bWVlZNpu7u/sljwsAAAAAVcGhAWzOnDkaOXKkRo0apZCQEM2dO1cBAQFauHBhqf0XLVqkwMBAzZ07VyEhIRo1apRGjBih2bNn2/SzWCzy8/Oz2S5nXAAAAACoCg4LYEVFRdqzZ49iYmJs2mNiYrRz585Sj0lNTbXr3717d+3evVvFxcXWtpMnTyooKEg33HCDevfurX379l3WuJJUWFiogoICmw0AAAAAKsNhASwnJ0fnzp2Tr6+vTbuvr6+ys7NLPSY7O7vU/mfPnlVOTo4k6cYbb9TSpUv14YcfauXKlXJ3d1fnzp114MCBSx5XkmbOnCkvLy/rFhAQUOlrBgAAAFC9OXwRDovFYrNvGIZd28X6/7G9U6dOGjZsmNq1a6fo6GitXr1azZs316uvvnpZ406ePFn5+fnW7fDhwxe/OAAAAAD4A2dHDezj4yMnJye7Wadjx47ZzU6V8PPzK7W/s7OzvL29Sz2mRo0auummm6wzYJcyriS5ubnJzc3totcFAAAAAGVx2AyYq6urwsLClJKSYtOekpKiqKioUo+JjIy0679x40aFh4fLxcWl1GMMw1BaWpr8/f0veVwAAAAAqAoOmwGTpPj4eMXGxio8PFyRkZF6/fXXlZmZqTFjxki6cNvf0aNHtXz5cknSmDFj9Nprryk+Pl6jR49WamqqEhMTtXLlSus5p0+frk6dOqlZs2YqKCjQvHnzlJaWpvnz51d4XAAAAAC4EhwawAYNGqTc3FzNmDFDWVlZat26tdatW6egoCBJUlZWls1ncwUHB2vdunWaMGGC5s+fr4YNG2revHnq37+/tU9eXp7+9re/KTs7W15eXgoNDdWnn36qjh07VnhcAAAAALgSLEbJKhaolIKCAnl5eSk/P1+1a9d2dDl6fl+Oo0sArnqTQn0cXUKVeOV/rzi6BOCqNr7ueEeXUGWKpz/q6BKAq5rLtJccXYKkymUDh6+CCAAAAADVBQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkzg8gC1YsEDBwcFyd3dXWFiYtm/fXm7/bdu2KSwsTO7u7mrcuLEWLVpk8/jixYsVHR2tunXrqm7duurWrZu+/PJLmz4JCQmyWCw2m5+fX5VfGwAAAAD8kUMD2KpVqxQXF6epU6dq3759io6OVo8ePZSZmVlq/0OHDqlnz56Kjo7Wvn37NGXKFI0bN05r1qyx9tm6dauGDBmiLVu2KDU1VYGBgYqJidHRo0dtztWqVStlZWVZt2+++eaKXisAAAAAODty8Dlz5mjkyJEaNWqUJGnu3LnasGGDFi5cqJkzZ9r1X7RokQIDAzV37lxJUkhIiHbv3q3Zs2erf//+kqS3337b5pjFixfr3Xff1SeffKL777/f2u7s7MysFwAAAABTOWwGrKioSHv27FFMTIxNe0xMjHbu3FnqMampqXb9u3fvrt27d6u4uLjUY06fPq3i4mLVq1fPpv3AgQNq2LChgoODNXjwYB08eLDcegsLC1VQUGCzAQAAAEBlOCyA5eTk6Ny5c/L19bVp9/X1VXZ2dqnHZGdnl9r/7NmzysnJKfWYSZMm6frrr1e3bt2sbREREVq+fLk2bNigxYsXKzs7W1FRUcrNzS2z3pkzZ8rLy8u6BQQEVPRSAQAAAEDSVbAIh8Visdk3DMOu7WL9S2uXpBdffFErV67U2rVr5e7ubm3v0aOH+vfvrzZt2qhbt276+OOPJUnLli0rc9zJkycrPz/fuh0+fPjiFwcAAAAAf+Cw94D5+PjIycnJbrbr2LFjdrNcJfz8/Ert7+zsLG9vb5v22bNn67nnntOmTZvUtm3bcmupWbOm2rRpowMHDpTZx83NTW5ubuWeBwAAAADK47AZMFdXV4WFhSklJcWmPSUlRVFRUaUeExkZadd/48aNCg8Pl4uLi7Vt1qxZeuaZZ7R+/XqFh4dftJbCwkKlp6fL39//Eq4EAAAAACrGobcgxsfH64033lBSUpLS09M1YcIEZWZmasyYMZIu3Pb3x5ULx4wZo59//lnx8fFKT09XUlKSEhMTNXHiRGufF198UU8++aSSkpLUqFEjZWdnKzs7WydPnrT2mThxorZt26ZDhw7piy++0L333quCggI98MAD5l08AAAAgGrHocvQDxo0SLm5uZoxY4aysrLUunVrrVu3TkFBQZKkrKwsm88ECw4O1rp16zRhwgTNnz9fDRs21Lx586xL0EsXPti5qKhI9957r81Y06ZNU0JCgiTpyJEjGjJkiHJyclS/fn116tRJn3/+uXVcAAAAALgSHBrAJGns2LEaO3ZsqY8tXbrUru3WW2/V3r17yzxfRkbGRcdMTk6uaHkAAAAAUGUcvgoiAAAAAFQXBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAAAAADAJAQwAAAAATEIAAwAAAACTEMAAAAAAwCQEMAAAAAAwCQEMAAAAAExCAAMAAAAAkxDAAAAAAMAkBDAAAAAAMAkBDAAAAABM4vAAtmDBAgUHB8vd3V1hYWHavn17uf23bdumsLAwubu7q3Hjxlq0aJFdnzVr1qhly5Zyc3NTy5Yt9d577132uAAAAABwuRwawFatWqW4uDhNnTpV+/btU3R0tHr06KHMzMxS+x86dEg9e/ZUdHS09u3bpylTpmjcuHFas2aNtU9qaqoGDRqk2NhYffXVV4qNjdXAgQP1xRdfXPK4AAAAAFAVLIZhGI4aPCIiQh06dNDChQutbSEhIerbt69mzpxp1/+JJ57Qhx9+qPT0dGvbmDFj9NVXXyk1NVWSNGjQIBUUFOg///mPtc+dd96punXrauXKlZc0bmkKCgrk5eWl/Px81a5du3IXfgU8vy/H0SUAV71JoT6OLqFKvPK/VxxdAnBVG193vKNLqDLF0x91dAnAVc1l2kuOLkFS5bKBs0k12SkqKtKePXs0adIkm/aYmBjt3Lmz1GNSU1MVExNj09a9e3clJiaquLhYLi4uSk1N1YQJE+z6zJ0795LHlaTCwkIVFhZa9/Pz8yVdeLKvBmdOnnB0CcBVr6DA1dElVIkzBWccXQJwVStwujr+31wVis8UXrwTUI25XCW/i5dkgorMbTksgOXk5OjcuXPy9fW1aff19VV2dnapx2RnZ5fa/+zZs8rJyZG/v3+ZfUrOeSnjStLMmTM1ffp0u/aAgICyLxLAVcX+FQzgWjRJky7eCcC14fn5jq7AxokTJ+Tl5VVuH4cFsBIWi8Vm3zAMu7aL9f9ze0XOWdlxJ0+erPj4eOv++fPn9dtvv8nb27vc41A9FRQUKCAgQIcPH74qblEFcOXwegeqB17rKI9hGDpx4oQaNmx40b4OC2A+Pj5ycnKym3U6duyY3exUCT8/v1L7Ozs7y9vbu9w+Jee8lHElyc3NTW5ubjZtderUKfsCAUm1a9fmhzRQTfB6B6oHXusoy8Vmvko4bBVEV1dXhYWFKSUlxaY9JSVFUVFRpR4TGRlp13/jxo0KDw+Xi4tLuX1Kznkp4wIAAABAVXDoLYjx8fGKjY1VeHi4IiMj9frrryszM1NjxoyRdOG2v6NHj2r58uWSLqx4+Nprryk+Pl6jR49WamqqEhMTrasbStL48eN1yy236IUXXlCfPn30wQcfaNOmTdqxY0eFxwUAAACAK8GhAWzQoEHKzc3VjBkzlJWVpdatW2vdunUKCgqSJGVlZdl8NldwcLDWrVunCRMmaP78+WrYsKHmzZun/v37W/tERUUpOTlZTz75pJ566ik1adJEq1atUkRERIXHBS6Xm5ubpk2bZnfbKoBrD693oHrgtY6q4tDPAQMAAACA6sRh7wEDAAAAgOqGAAYAAAAAJiGAAQAAAIBJCGCoFiwWi95///0rPk6XLl0UFxd3xccBAADAXxMBDNeEY8eO6aGHHlJgYKDc3Nzk5+en7t27KzU1VdKFFTV79Ojh4Crtbd26VRaLxbp5e3vrtttu02effWbTLyEhQRaLxe6jEtLS0mSxWJSRkSFJysjIkMViUYMGDXTixAmbvu3bt1dCQsKVvBzgmpCdna3x48eradOmcnd3l6+vr26++WYtWrRIp0+fliQ1atTI+rr18PDQjTfeqFmzZumP61qVvL7z8vLsxuD1CJhv+PDh6tu3b6mP7du3T71791aDBg3k7u6uRo0aadCgQcrJybH+P7i8LSMjw9rvzjvvtDv/iy++KIvFoi5dulzZi8RfAgEM14T+/fvrq6++0rJly7R//359+OGH6tKli3777TdJkp+f31W9bOyPP/6orKwsbd26VfXr11evXr107Ngxmz7u7u5KTEzU/v37L3q+EydOaPbs2VeqXOCadfDgQYWGhmrjxo167rnntG/fPm3atEkTJkzQRx99pE2bNln7lnyUSXp6uiZOnKgpU6bo9ddfd2D1AC7FsWPH1K1bN/n4+GjDhg1KT09XUlKS/P39dfr0aU2cOFFZWVnW7YYbbrC+/ku2gIAASZK/v7+2bNmiI0eO2IyxZMkSBQYGOuLycBUigOEvLy8vTzt27NALL7ygrl27KigoSB07dtTkyZPVq1cvSba3IJbMEq1evVrR0dHy8PDQTTfdpP3792vXrl0KDw+Xp6en7rzzTh0/ftw6TslfzqZPn64GDRqodu3aeuihh1RUVFRmbUVFRXr88cd1/fXXq2bNmoqIiNDWrVvt+jVo0EB+fn5q06aNnnzySeXn5+uLL76w6dOiRQt17dpVTz755EWfk3/84x+aM2eOXYgDUL6xY8fK2dlZu3fv1sCBAxUSEqI2bdqof//++vjjj3XXXXdZ+9aqVUt+fn5q1KiRRo0apbZt22rjxo0OrB7Apdi5c6cKCgr0xhtvKDQ0VMHBwbrttts0d+5cBQYGytPTU35+ftbNycnJ+vr/Y5t04f/nMTExWrZsmc35c3JyrL+TAAQw/OV5enrK09NT77//vgoLCyt83LRp0/Tkk09q7969cnZ21pAhQ/T444/rlVde0fbt2/XTTz/p6aeftjnmk08+UXp6urZs2aKVK1fqvffe0/Tp08sc48EHH9Rnn32m5ORkff311xowYIDuvPNOHThwoNT+p0+f1pIlSyRJLi4udo8///zzWrNmjXbt2lXutQ0ZMkRNmzbVjBkzLvY0APg/ubm52rhxox555BHVrFmz1D4Wi8WuzTAMbd26Venp6aW+bgFc3fz8/HT27Fm99957qoqPxx0xYoSWLl1q3U9KStJ9990nV1fXyz43rg0EMPzlOTs7a+nSpVq2bJnq1Kmjzp07a8qUKfr666/LPW7ixInq3r27QkJCNH78eO3du1dPPfWUOnfurNDQUI0cOVJbtmyxOcbV1VVJSUlq1aqVevXqpRkzZmjevHk6f/683fl/+uknrVy5Uu+8846io6PVpEkTTZw4UTfffLM1ZJW44YYbrEHy5ZdfVlhYmG6//Xa7c3bo0EEDBw7UpEmTyr02i8Wi559/Xq+//rp++umncvsCuOC///2vDMNQixYtbNp9fHysr88nnnjC2v7EE0/I09NTbm5u6tq1qwzD0Lhx48wuG8Bl6tSpk6ZMmaKhQ4fKx8dHPXr00KxZs/Trr79e0vl69+6tgoICffrppzp16pRWr16tESNGVHHV+CsjgOGa0L9/f/3yyy/68MMP1b17d23dulUdOnSw+QvUn7Vt29b6b19fX0lSmzZtbNr+fAtfu3btdN1111n3IyMjdfLkSR0+fNju/Hv37pVhGGrevLn1lzdPT09t27bNLhRt375de/fu1cqVKxUUFKSlS5eW+Zf0Z599Vtu3b7/orU7du3fXzTffrKeeeqrcfgBs/XmW68svv1RaWppatWplM8v+2GOPKS0tTdu2bVPXrl01depURUVFmV0ugCrwz3/+U9nZ2Vq0aJFatmypRYsW6cYbb9Q333xT6XO5uLho2LBhWrJkid555x01b97c5ncOwNnRBQBVxd3dXXfccYfuuOMOPf300xo1apSmTZum4cOHl9r/jwGn5BeuP7eVNrNVmtJuSzp//rycnJy0Z88e673hJTw9PW32g4ODVadOHTVv3lxnzpxRv3799O2335a6cEiTJk00evRoTZo0SYmJieXW9fzzzysyMlKPPfZYha4DqM6aNm0qi8WiH374waa9cePGkiQPDw+bdh8fHzVt2lRNmzbVmjVr1LRpU3Xq1EndunWTJNWuXVuSlJ+frzp16tgcm5eXJy8vryt0JQAuhbe3twYMGKABAwZo5syZCg0N1ezZs23ez1VRI0aMUEREhL799ltmv2CHGTBcs1q2bKlTp05V6Tm/+uor/f7779b9zz//XJ6enrrhhhvs+oaGhurcuXM6duyY9Ze0ks3Pz6/MMWJjY3X+/HktWLCgzD5PP/209u/fr+Tk5HLr7dixo+65556L3rII4MIvX3fccYdee+21Sv/sqFu3rv7xj39o4sSJ1veQNGvWTDVq1LB7z2ZWVpaOHj1qd6sjgKuHq6urmjRpcsm/R7Rq1UqtWrXSt99+q6FDh1ZxdfirYwYMf3m5ubkaMGCARowYobZt26pWrVravXu3XnzxRfXp06dKxyoqKtLIkSP15JNP6ueff9a0adP097//XTVq2P8to3nz5rrvvvt0//3366WXXlJoaKhycnK0efNmtWnTRj179ix1jBo1aiguLk7PPvusHnroIZtbHkv4+voqPj5es2bNumjN//znP9WqVSs5O/NyBy5mwYIF6ty5s8LDw5WQkKC2bdtaQ9QPP/ygsLCwMo995JFH9MILL2jNmjW69957VatWLT300EN69NFH5ezsrHbt2umXX37R1KlTFRISopiYGBOvDIB0YUY6LS3Npu3rr7/Wxo0bNXjwYDVv3lyGYeijjz7SunXr7N6zXRmbN29WcXGx3Qw4wG9k+Mvz9PRURESEXn75Zf30008qLi5WQECARo8erSlTplTpWLfffruaNWumW265RYWFhRo8eHC5H6a6ZMkSPfvss3r00Ud19OhReXt7KzIysszwVWLEiBGaNm2aXnvtNT3++OOl9nnssce0cOFCnTlzptxzNW/eXCNGjODziYAKaNKkifbt26fnnntOkydP1pEjR+Tm5qaWLVtq4sSJGjt2bJnH1q9fX7GxsUpISNA999yjGjVq6OWXX5a/v7+mTJmijIwMNWjQQF27dlVycjJ/FAEcYOvWrQoNDbVpi42N1XXXXadHH31Uhw8flpubm5o1a6Y33nhDsbGxlzxWWaupAhajKtbbBKqB4cOHKy8vz/p5YgAAAEBl8R4wAAAAADAJAQwAAAAATMItiAAAAABgEmbAAAAAAMAkBDAAAAAAMAkBDAAAAABMQgADAAAAAJMQwAAAAADAJAQwAACqwNatW2WxWJSXl1fhYxo1aqS5c+desZoAAFcfAhgAoFoYPny4LBaLxowZY/fY2LFjZbFYNHz4cPMLAwBUKwQwAEC1ERAQoOTkZP3+++/WtjNnzmjlypUKDAx0YGUAgOqCAAYAqDY6dOigwMBArV271tq2du1aBQQEKDQ01NpWWFiocePGqUGDBnJ3d9fNN9+sXbt22Zxr3bp1at68uTw8PNS1a1dlZGTYjbdz507dcsst8vDwUEBAgMaNG6dTp06VWV9CQoICAwPl5uamhg0baty4cZd/0QCAqwoBDABQrTz44INasmSJdT8pKUkjRoyw6fP4449rzZo1WrZsmfbu3aumTZuqe/fu+u233yRJhw8f1j333KOePXsqLS1No0aN0qRJk2zO8c0336h79+6655579PXXX2vVqlXasWOH/v73v5da17vvvquXX35Z//rXv3TgwAG9//77atOmTRVfPQDA0QhgAIBqJTY2Vjt27FBGRoZ+/vlnffbZZxo2bJj18VOnTmnhwoWaNWuWevTooZYtW2rx4sXy8PBQYmKiJGnhwoVq3LixXn75ZbVo0UL33Xef3fvHZs2apaFDhyouLk7NmjVTVFSU5s2bp+XLl+vMmTN2dWVmZsrPz0/dunVTYGCgOnbsqNGjR1/R5wIAYD4CGACgWvHx8VGvXr20bNkyLVmyRL169ZKPj4/18Z9++knFxcXq3Lmztc3FxUUdO3ZUenq6JCk9PV2dOnWSxWKx9omMjLQZZ8+ePVq6dKk8PT2tW/fu3XX+/HkdOnTIrq4BAwbo999/V+PGjTV69Gi99957Onv2bFVfPgDAwZwdXQAAAGYbMWKE9VbA+fPn2zxmGIYk2YSrkvaStpI+5Tl//rweeuihUt/HVdqCHwEBAfrxxx+VkpKiTZs2aezYsZo1a5a2bdsmFxeXil0YAOCqxwwYAKDaufPOO1VUVKSioiJ1797d5rGmTZvK1dVVO3bssLYVFxdr9+7dCgkJkSS1bNlSn3/+uc1xf97v0KGDvvvuOzVt2tRuc3V1LbUuDw8P3X333Zo3b562bt2q1NRUffPNN1VxyQCAqwQzYACAasfJycl6O6GTk5PNYzVr1tTDDz+sxx57TPXq1VNgYKBefPFFnT59WiNHjpQkjRkzRi+99JLi4+P10EMPWW83/KMnnnhCnTp10iOPPKLRo0erZs2aSk9PV0pKil599VW7mpYuXapz584pIiJC1113nd588015eHgoKCjoyjwJAACHYAYMAFAt1a5dW7Vr1y71seeff179+/dXbGysOnTooP/+97/asGGD6tatK+nCLYRr1qzRRx99pHbt2mnRokV67rnnbM7Rtm1bbdu2TQcOHFB0dLRCQ0P11FNPyd/fv9Qx69Spo8WLF6tz585q27atPvnkE3300Ufy9vau2gsHADiUxajIjewAAAAAgMvGDBgAAAAAmIQABgAAAAAmIYABAAAAgEkIYAAAAABgEgIYAAAAAJiEAAYAAAAAJiGAAQAAAIBJCGAAAAAAYBICGAAAAACYhAAGAAAAACYhgAEAAACASf4f2DbrwTtD7G0AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plotting the Bar Chart for RMSE\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(models, rmse_values, color=['skyblue', 'lightgreen', 'salmon'])\n",
        "plt.title('Test RMSE Comparison Across Models')\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('RMSE')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be32a0ba",
      "metadata": {
        "id": "be32a0ba",
        "outputId": "5151ec5f-2b9c-48e9-a9ea-7c1eb8b53b81"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plotting the Bar Chart for MAE\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(models, mae_values, color=['skyblue', 'lightgreen', 'salmon'])\n",
        "plt.title('Test MAE Comparison Across Models')\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('MAE')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "639ae644",
      "metadata": {
        "id": "639ae644",
        "outputId": "136a46e4-09bf-4171-98d5-117dff09ba5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Evaluation Comparison:\n",
            "       Model   Accuracy      RMSE       MDA       MAE\n",
            "0  SimpleRNN  78.970276  0.176312  0.498740  0.130943\n",
            "1        GRU  78.679151  0.178159  0.498752  0.132321\n",
            "2       LSTM  79.075725  0.175875  0.498784  0.130777\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MDA</th>\n",
              "      <th>MAE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SimpleRNN</td>\n",
              "      <td>78.970276</td>\n",
              "      <td>0.176312</td>\n",
              "      <td>0.498740</td>\n",
              "      <td>0.130943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GRU</td>\n",
              "      <td>78.679151</td>\n",
              "      <td>0.178159</td>\n",
              "      <td>0.498752</td>\n",
              "      <td>0.132321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LSTM</td>\n",
              "      <td>79.075725</td>\n",
              "      <td>0.175875</td>\n",
              "      <td>0.498784</td>\n",
              "      <td>0.130777</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Model   Accuracy      RMSE       MDA       MAE\n",
              "0  SimpleRNN  78.970276  0.176312  0.498740  0.130943\n",
              "1        GRU  78.679151  0.178159  0.498752  0.132321\n",
              "2       LSTM  79.075725  0.175875  0.498784  0.130777"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Convert tensors to numpy values before creating the DataFrame\n",
        "rmse_values = [rnn_rmse.numpy(), gru_rmse.numpy(), lstm_rmse.numpy()]  # Convert RMSE tensors\n",
        "mda_values = [rnn_mda.numpy(), gru_mda.numpy(), lstm_mda.numpy()]  # Convert MDA tensors\n",
        "acc_values = [rnn_acc, gru_acc, lstm_acc]  # Already numeric\n",
        "mae_values = [rnn_mae, gru_mae, lstm_mae]  # Already numeric\n",
        "\n",
        "# Creating a DataFrame to show a comparison table of metrics\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Model': models,\n",
        "    'Accuracy': acc_values,\n",
        "    'RMSE': rmse_values,\n",
        "    'MDA': mda_values,\n",
        "    'MAE': mae_values\n",
        "})\n",
        "\n",
        "# Displaying the table of metrics\n",
        "print(\"Model Evaluation Comparison:\")\n",
        "print(metrics_df)\n",
        "\n",
        "# Optionally display the DataFrame for Jupyter Notebook environments\n",
        "import IPython.display as display\n",
        "display.display(metrics_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75e7d955",
      "metadata": {
        "id": "75e7d955"
      },
      "source": [
        "### Evaluation of Results\n",
        "\n",
        "From the `Model Evaluation Comparison` table:\n",
        "\n",
        "| Model      | Accuracy  | RMSE    | MDA     | MAE     |\n",
        "|------------|-----------|---------|---------|---------|\n",
        "| SimpleRNN  | 78.970276 | 0.176312 | 0.498740 | 0.130943 |\n",
        "| GRU        | 78.679151 | 0.178159 | 0.498752 | 0.132321 |\n",
        "| LSTM       | 79.075725 | 0.175875 | 0.498784 | 0.130777 |\n",
        "\n",
        "### Best Model: **LSTM**\n",
        "\n",
        "#### Reasoning:\n",
        "\n",
        "1. **Accuracy**:\n",
        "   - LSTM achieves the highest accuracy (79.08%), which indicates that it performs better than both GRU (78.68%) and SimpleRNN (78.97%) in predicting the direction of stock price changes. This metric is crucial for trading strategies where predicting the trend correctly is vital.\n",
        "\n",
        "2. **RMSE (Root Mean Squared Error)**:\n",
        "   - LSTM has the lowest RMSE (0.175875), showing it is the most accurate in terms of minimizing large prediction errors compared to GRU (0.178159) and SimpleRNN (0.176312). This is critical in financial contexts, as large errors can result in significant financial losses.\n",
        "\n",
        "3. **MDA (Mean Directional Accuracy)**:\n",
        "   - LSTM slightly outperforms the other models in MDA (0.498784), which measures how well the model predicts the direction (up or down). While the difference is minor, LSTM's higher MDA indicates a better ability to capture stock price trends compared to SimpleRNN (0.498740) and GRU (0.498752).\n",
        "\n",
        "4. **MAE (Mean Absolute Error)**:\n",
        "   - LSTM achieves the lowest MAE (0.130777), meaning it has the smallest average prediction error in absolute terms compared to GRU (0.132321) and SimpleRNN (0.130943). This shows LSTM's consistent accuracy in making small, precise predictions.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "- **LSTM performs the best** because it achieves the **highest accuracy**, **lowest RMSE**, **lowest MAE**, and slightly better **MDA** compared to the other models. These metrics together show that LSTM is the most robust model for both accurate stock price predictions and trend predictions, making it the most suitable for the given task of predicting future stock prices.\n",
        "\n",
        "- **Reason for Better Performance of LSTM**:\n",
        "   - LSTMs are designed to handle long-term dependencies better, which is critical in time-series data like stock prices. They retain information over longer sequences, making them well-suited for this task compared to GRU and SimpleRNN. This aligns with the task requirement of predicting future stock prices based on past sequences."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}